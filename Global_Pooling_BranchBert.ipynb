{"cells":[{"cell_type":"markdown","metadata":{"id":"Zu8bU7EbaEf4"},"source":["# Preparation"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25613,"status":"ok","timestamp":1649052960945,"user":{"displayName":"刘耘果","userId":"11745284739417236882"},"user_tz":-480},"id":"gtIFGEWQhMjy","outputId":"7e5773be-985c-4567-fb9a-8a9b78c44d8d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17100,"status":"ok","timestamp":1649052978039,"user":{"displayName":"刘耘果","userId":"11745284739417236882"},"user_tz":-480},"id":"omeDVC8Hzy6G","outputId":"850feb1d-8a73-4f00-af4c-5547c604af8b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting opencc-python-reimplemented\n","  Downloading opencc-python-reimplemented-0.1.6.tar.gz (484 kB)\n","\u001b[?25l\r\u001b[K     |▊                               | 10 kB 22.1 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20 kB 20.3 MB/s eta 0:00:01\r\u001b[K     |██                              | 30 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 40 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 51 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████                            | 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 81 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████                          | 92 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 184 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 194 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 204 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 215 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 225 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 235 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 245 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 256 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 266 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 276 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 286 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 296 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 307 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 317 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 327 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 337 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 348 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 358 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 368 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 378 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 389 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 399 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 409 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 419 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 430 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 440 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 450 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 460 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 471 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 481 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 484 kB 5.1 MB/s \n","\u001b[?25hBuilding wheels for collected packages: opencc-python-reimplemented\n","  Building wheel for opencc-python-reimplemented (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for opencc-python-reimplemented: filename=opencc_python_reimplemented-0.1.6-py2.py3-none-any.whl size=486152 sha256=2c744613c8170fc0a99b43ab36fc77f0ac7d1d43f52446c809de06f6880de4ef\n","  Stored in directory: /root/.cache/pip/wheels/4e/e2/60/d062d260be08788bb389521544a8fc173de9a9a78d6a593344\n","Successfully built opencc-python-reimplemented\n","Installing collected packages: opencc-python-reimplemented\n","Successfully installed opencc-python-reimplemented-0.1.6\n","Collecting pycantonese\n","  Downloading pycantonese-3.4.0-py3-none-any.whl (3.9 MB)\n","\u001b[K     |████████████████████████████████| 3.9 MB 5.1 MB/s \n","\u001b[?25hCollecting wordseg==0.0.2\n","  Downloading wordseg-0.0.2-py3-none-any.whl (9.5 kB)\n","Requirement already satisfied: importlib-metadata>=1.0 in /usr/local/lib/python3.7/dist-packages (from pycantonese) (4.11.3)\n","Collecting pylangacq<0.17.0,>=0.16.0\n","  Downloading pylangacq-0.16.2-py3-none-any.whl (64 kB)\n","\u001b[K     |████████████████████████████████| 64 kB 3.1 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.0->pycantonese) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.0->pycantonese) (3.7.0)\n","Requirement already satisfied: python-dateutil<=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from pylangacq<0.17.0,>=0.16.0->pycantonese) (2.8.2)\n","Requirement already satisfied: tabulate[widechars]<=0.9.0,>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from pylangacq<0.17.0,>=0.16.0->pycantonese) (0.8.9)\n","Requirement already satisfied: requests<=3.0.0,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from pylangacq<0.17.0,>=0.16.0->pycantonese) (2.23.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<=3.0.0,>=2.0.0->pylangacq<0.17.0,>=0.16.0->pycantonese) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<=3.0.0,>=2.18.0->pylangacq<0.17.0,>=0.16.0->pycantonese) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<=3.0.0,>=2.18.0->pylangacq<0.17.0,>=0.16.0->pycantonese) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<=3.0.0,>=2.18.0->pylangacq<0.17.0,>=0.16.0->pycantonese) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<=3.0.0,>=2.18.0->pylangacq<0.17.0,>=0.16.0->pycantonese) (1.24.3)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from tabulate[widechars]<=0.9.0,>=0.8.9->pylangacq<0.17.0,>=0.16.0->pycantonese) (0.2.5)\n","Installing collected packages: wordseg, pylangacq, pycantonese\n","Successfully installed pycantonese-3.4.0 pylangacq-0.16.2 wordseg-0.0.2\n","Collecting transformers\n","  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 54.8 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting tokenizers!=0.11.3,>=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 46.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 62.8 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.17.0\n"]}],"source":["!pip install opencc-python-reimplemented\n","!pip install pycantonese\n","!pip install transformers\n","# !pip uninstall xlrd\n","# !pip install xlrd==1.2.0"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":6459,"status":"ok","timestamp":1649052984493,"user":{"displayName":"刘耘果","userId":"11745284739417236882"},"user_tz":-480},"id":"_xQkGSnyhSYr"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import re,string\n","import math\n","import opencc\n","from sklearn import preprocessing\n","import torch\n","import torch.nn as nn\n","import pkg_resources\n","import warnings\n","pkg_resources.get_distribution(\"xlrd\").version\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1649052984494,"user":{"displayName":"刘耘果","userId":"11745284739417236882"},"user_tz":-480},"id":"QmuJRLtrO5jc"},"outputs":[],"source":["def simplify_punctuation_and_whitespace(sentence):\n","    sent = _replace_urls(sentence)\n","    sent = _simplify_punctuation(sent)\n","    sent = _normalize_whitespace(sent)\n","    return sent\n","\n","def _replace_urls(text):\n","    url_regex = r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n","    text = text.replace('�','.')\n","    text = re.sub(r'^[\\x00-\\x7F]+|[\\x00-\\x7F]+$', '', text)#special case\n","    text = re.sub(url_regex, \"<URL>\", text)\n","    return text\n","\n","def _simplify_punctuation(text):\n","    \"\"\"\n","    This function simplifies doubled or more complex punctuation. The exception is '...'.\n","    \"\"\"\n","    corrected = str(text)\n","    corrected = re.sub(r'([!?,;？❓！.])\\1+', r'\\1', corrected)\n","    corrected = re.sub(r'\\.{2,}', r'...', corrected)\n","    return corrected\n","\n","def _normalize_whitespace(text):\n","    \"\"\"\n","    This function normalizes whitespaces, removing duplicates.\n","    \"\"\"\n","    corrected = str(text)\n","    corrected = re.sub(r\"//t\",r\"\\t\", corrected)\n","    corrected = re.sub(r\"( )\\1+\",r\"\\1\", corrected)\n","    corrected = re.sub(r\"(\\n)\\1+\",r\"\\1\", corrected)\n","    corrected = re.sub(r\"(\\r)\\1+\",r\"\\1\", corrected)\n","    corrected = re.sub(r\"(\\t)\\1+\",r\"\\1\", corrected)\n","    return corrected.strip(\" \")"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8457,"status":"ok","timestamp":1649052992947,"user":{"displayName":"刘耘果","userId":"11745284739417236882"},"user_tz":-480},"id":"zOg1LkYw9pFj","outputId":"5bd75e7b-5b69-4218-f8d5-7a5b83424d59"},"outputs":[{"output_type":"stream","name":"stdout","text":["5880\n","5880\n"]}],"source":["data=pd.read_excel(r'/content/drive/MyDrive/DATA/dataset_feature.xlsx')\n","df=pd.DataFrame(data)\n","print(len(df))\n","\n","cc = opencc.OpenCC('s2hk')\n","for i in range(len(df)):\n","  text=cc.convert(df['text'].iloc[i])\n","  text_new=simplify_punctuation_and_whitespace(text)\n","  if len(text_new)==0:\n","    df.drop([i],inplace = True)\n","  else:\n","    df['text'].iloc[i]=text_new\n","print(len(df))"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1424,"status":"ok","timestamp":1649052994366,"user":{"displayName":"刘耘果","userId":"11745284739417236882"},"user_tz":-480},"id":"fKP53b1NRJvI","outputId":"c89ed870-1166-4733-cacc-a5c76725fb5e"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 5380 entries, 0 to 5444\n","Data columns (total 8 columns):\n"," #   Column       Non-Null Count  Dtype  \n","---  ------       --------------  -----  \n"," 0   label        5380 non-null   float64\n"," 1   info_id      5380 non-null   float64\n"," 2   cmt_id       5380 non-null   object \n"," 3   parent       5380 non-null   object \n"," 4   last_sep     5380 non-null   int64  \n"," 5   text         5380 non-null   object \n"," 6   branch_text  5380 non-null   object \n"," 7   depth        5380 non-null   int64  \n","dtypes: float64(2), int64(2), object(4)\n","memory usage: 378.3+ KB\n"]}],"source":["mask=(df['is_post']==0)\n","df2=df.loc[mask] # comment df \n","mask=(df['is_post']==1)\n","df_info=df.loc[mask] # comment df info_df\n","\n","\n","df_info['branch_text']=df_info['text']\n","df_info['depth']=1\n","columns=['label', 'is_post', 'info_id', 'cmt_id', 'parent', 'text','branch_text','depth']\n","df_info=df_info[columns]\n","df_info['label']=df_info['label']-1\n","\n","df2['last_sep']=0\n","df2['depth']=1\n","df2['branch_text']=''\n","columns=['label', 'info_id', 'cmt_id', 'parent','last_sep', 'text','branch_text','depth']\n","df2=df2[columns]\n","df2['label']=df2['label']-1\n","\n","for i in range(len(df2)):\n","  if math.isnan(df2['parent'].iloc[i]):\n","    df2['parent'].iloc[i]=df2['info_id'].iloc[i]\n","  else:\n","    df2['parent'].iloc[i] = str(int(df2['parent'].iloc[i]))\n","df2.info()"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":12102,"status":"ok","timestamp":1649053006466,"user":{"displayName":"刘耘果","userId":"11745284739417236882"},"user_tz":-480},"id":"Z0qa0cOHSM9J"},"outputs":[],"source":["def build_branch(dataframe, i, text_list):\n","  if dataframe['parent'].iloc[i] == dataframe['info_id'].iloc[i]:\n","    '''双亲节点为Post'''\n","    info_id = dataframe['info_id'].iloc[i]\n","    df_parent = df_info.loc[df_info['info_id']==info_id]\n","    if len(df_parent) == 1:\n","      text_list.append(dataframe['text'].iloc[i])\n","      text_list.append(df_parent['text'].iloc[0])\n","    else:\n","      print(\"INFO EMPTY: \", info_id)\n","  else:\n","    '''双亲节点为Replies'''\n","    text_list.append(dataframe['text'].iloc[i])\n","    parent_id = str(dataframe['parent'].iloc[i])\n","    df_parent = df2.loc[df2['cmt_id'] == parent_id]#从完整df中截取\n","    if len(df_parent) == 1:\n","      '''递归找双亲结点'''\n","      build_branch(df_parent, 0, text_list)\n","    else:\n","      emptys.append(parent_id)\n","  return text_list\n","\n","Sep = '[SEP]'\n","lenths=[]#branch 超长的长度\n","emptys=[]\n","'''遍历df2'''\n","for i in range(len(df2)):\n","  text=[]\n","  text = build_branch(df2, i, text)\n","  text=list(reversed(text))\n","  Str = Sep.join(text)\n","  df2['branch_text'].iloc[i]=Str\n","  df2['depth'].iloc[i]=len(text)\n","df2=df2[~df2['parent'].isin(emptys)]\n"]},{"cell_type":"markdown","metadata":{"id":"20xHeJLjaS3N"},"source":["# Branch Model"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":904,"status":"ok","timestamp":1649053007358,"user":{"displayName":"刘耘果","userId":"11745284739417236882"},"user_tz":-480},"id":"_AMhXeyd97T2"},"outputs":[],"source":["# coding:utf-8\n","import pycantonese\n","import nltk\n","import numpy\n","import jieba\n","import codecs\n","import os\n","class SummaryTxt:\n","    def __init__(self):\n","        # 单词数量\n","        self.N = 100\n","        # 单词间的距离\n","        self.CLUSTER_THRESHOLD = 5\n","        # 返回的top n句子\n","        self.TOP_SENTENCES = 5\n","        #加载停用词\n","        self.stopwords = {}.fromkeys(pycantonese.stop_words())\n","    def _split_sentences(self,texts):\n","        '''\n","        把texts拆分成单个句子，保存在列表里面，以（.!?。！？）这些标点作为拆分的意见，\n","        :param texts: 文本信息\n","        :return:\n","        '''\n","        splitstr = '.!?。！？'.encode('utf8').decode('utf8')\n","        start = 0\n","        index = 0  # 每个字符的位置\n","        sentences = []\n","        for text in texts:\n","            if text in splitstr:  # 检查标点符号下一个字符是否还是标点\n","                sentences.append(texts[start:index + 1])  # 当前标点符号位置\n","                start = index + 1  # start标记到下一句的开头\n","            index += 1\n","        if start < len(texts):\n","            sentences.append(texts[start:])  # 这是为了处理文本末尾没有标\n","        return sentences\n","\n","    def _score_sentences(self,sentences, topn_words):\n","        '''\n","        利用前N个关键字给句子打分\n","        :param sentences: 句子列表\n","        :param topn_words: 关键字列表\n","        :return:\n","        '''\n","        scores = []\n","        sentence_idx = -1\n","        for s in [pycantonese.segment(s) for s in sentences]:\n","            sentence_idx += 1\n","            word_idx = []\n","            for w in topn_words:\n","                try:\n","                    word_idx.append(s.index(w))  # 关键词出现在该句子中的索引位置\n","                except ValueError:  # w不在句子中\n","                    pass\n","            word_idx.sort()\n","            if len(word_idx) == 0:\n","                continue\n","            # 对于两个连续的单词，利用单词位置索引，通过距离阀值计算族\n","            clusters = []\n","            cluster = [word_idx[0]]\n","            i = 1\n","            while i < len(word_idx):\n","                if word_idx[i] - word_idx[i - 1] < self.CLUSTER_THRESHOLD:\n","                    cluster.append(word_idx[i])\n","                else:\n","                    clusters.append(cluster[:])\n","                    cluster = [word_idx[i]]\n","                i += 1\n","            clusters.append(cluster)\n","            # 对每个族打分，每个族类的最大分数是对句子的打分\n","            max_cluster_score = 0\n","            for c in clusters:\n","                significant_words_in_cluster = len(c)\n","                total_words_in_cluster = c[-1] - c[0] + 1\n","                score = 1.0 * significant_words_in_cluster * significant_words_in_cluster / total_words_in_cluster\n","                if score > max_cluster_score:\n","                    max_cluster_score = score\n","            scores.append((sentence_idx, max_cluster_score))\n","        return scores\n","\n","    def summaryScoredtxt(self,text):\n","        # 将文章分成句子\n","        sentences = self._split_sentences(text)\n","        # 生成分词\n","        words = [w for sentence in sentences for w in pycantonese.segment(sentence) if w not in self.stopwords if\n","                 len(w) > 1 and w != '\\t']\n","        # 统计词频\n","        wordfre = nltk.FreqDist(words)\n","        # 获取词频最高的前N个词\n","        topn_words = [w[0] for w in sorted(wordfre.items(), key=lambda d: d[1], reverse=True)][:self.N]\n","        # 根据最高的n个关键词，给句子打分\n","        scored_sentences = self._score_sentences(sentences, topn_words)\n","        # 利用均值和标准差过滤非重要句子\n","        avg = numpy.mean([s[1] for s in scored_sentences])  # 均值\n","        std = numpy.std([s[1] for s in scored_sentences])  # 标准差\n","        summarySentences = []\n","        for (sent_idx, score) in scored_sentences:\n","            if score > (avg + 0.5 * std):\n","                summarySentences.append(sentences[sent_idx])\n","        return summarySentences\n","\n","    def summaryTopNtxt(self,text):\n","        # 将文章分成句子\n","        sentences = self._split_sentences(text)\n","        # 根据句子列表生成分词列表\n","        words = [w for sentence in sentences for w in pycantonese.segment(sentence) if w not in self.stopwrods if\n","                 len(w) > 1 and w != '\\t']\n","        # 统计词频\n","        wordfre = nltk.FreqDist(words)\n","        # 获取词频最高的前N个词\n","        topn_words = [w[0] for w in sorted(wordfre.items(), key=lambda d: d[1], reverse=True)][:self.N]\n","        # 根据最高的n个关键词，给句子打分\n","        scored_sentences = self._score_sentences(sentences, topn_words)\n","        top_n_scored = sorted(scored_sentences, key=lambda s: s[1])[-self.TOP_SENTENCES:]\n","        top_n_scored = sorted(top_n_scored, key=lambda s: s[0])\n","        summarySentences = []\n","        for (idx, score) in top_n_scored:\n","            summarySentences.append(sentences[idx])\n","        return sentences"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":339,"status":"ok","timestamp":1649053008033,"user":{"displayName":"刘耘果","userId":"11745284739417236882"},"user_tz":-480},"id":"WVln7AGMKas8"},"outputs":[],"source":["import numpy as np\n","import torch\n","from tqdm import tqdm\n","import time\n","from datetime import timedelta\n","import pandas as pd\n","import numpy as np\n","from sklearn import preprocessing\n","from transformers import BertTokenizer\n","PAD, CLS = '[PAD]', '[CLS]'  # padding符号, bert中综合信息符号\n","\n","def build_dataset(config, mode= 'branch'):\n","    def load_dataset(df, pad_size=config.pad_size):\n","        contents = []\n","        summarizer = SummaryTxt()\n","        tokenizer=BertTokenizer.from_pretrained(config.bert_path)\n","        tokenizer.add_special_tokens({\"additional_special_tokens\": ['[PAD]','[CLS]']})\n","        if mode =='branch':\n","          text_col='branch_text'\n","        else:\n","          text_col='text'\n","        #建立数据集\n","        print('mode: ', mode)\n","        for i in range(len(df)):\n","            content=df[text_col].iloc[i]\n","            label=df['label'].iloc[i]\n","            token = tokenizer.tokenize(content)\n","            seq_len = len(token)\n","            if mode =='branch':\n","              if seq_len > pad_size:\n","                #对Post摘要\n","                first_index=content.find(Sep)\n","                post=content[:first_index]\n","                post=summarizer.summaryScoredtxt(post)\n","                post = '。'.join(post)\n","                content=post + content[first_index:]\n","                #摘要后重新分词\n","                token = tokenizer.tokenize(content)\n","                seq_len = len(token)\n","                if seq_len >pad_size:\n","                  #任然越界，则前截断\n","                  token = token[-pad_size:]\n","                  seq_len = len(token)\n","              pos = [i for i, x in enumerate(token) if x ==Sep]    \n","              if len(pos)>0:#存在SEP,为COMMENT\n","                last_sep=pos[-1]#最后一个\n","              else:\n","                last_sep=0#句首    \n","              mask = []\n","              token_ids = tokenizer.convert_tokens_to_ids(token)\n","              if pad_size:\n","                  if len(token) < pad_size:\n","                      mask = [1] * len(token_ids) + [0] * (pad_size - len(token))\n","                      token_ids += ([0] * (pad_size - len(token)))\n","                  else:\n","                      mask = [1] * pad_size\n","                      token_ids = token_ids[:pad_size]\n","                      seq_len = pad_size\n","              contents.append((token_ids, int(label), seq_len, mask ,last_sep))\n","            else:\n","              mask = []\n","              token = [CLS] + token\n","              token_ids = tokenizer.convert_tokens_to_ids(token)\n","              pad_size = config.cmt_max_len\n","              if pad_size:\n","                  if len(token) < pad_size:\n","                      mask = [1] * len(token_ids) + [0] * (pad_size - len(token))\n","                      token_ids += ([0] * (pad_size - len(token)))\n","                  else:\n","                      mask = [1] * pad_size\n","                      token_ids = token_ids[:pad_size]\n","                      seq_len = pad_size\n","              contents.append((token_ids, int(label), seq_len, mask))\n","        return contents\n","    train = load_dataset(config.train_df, config.pad_size)\n","    test = load_dataset(config.test_df, config.pad_size)\n","    return train, test\n","\n","class DatasetIterater(object):\n","    def __init__(self, batches, batch_size, device, mode):\n","        self.batch_size = batch_size\n","        self.batches = batches\n","        self.n_batches = len(batches) // batch_size\n","        self.residue = False  # 记录batch数量是否为整数\n","        if len(batches) % self.n_batches != 0:\n","            self.residue = True\n","        self.index = 0\n","        self.device = device   \n","        self.mode = mode   \n","          \n","    def _to_tensor(self, datas):\n","        x = torch.LongTensor([_[0] for _ in datas]).to(self.device)\n","        y = torch.LongTensor([_[1] for _ in datas]).to(self.device)\n","\n","        # pad前的长度(超过pad_size的设为pad_size)\n","        seq_len = torch.LongTensor([_[2] for _ in datas]).to(self.device)\n","        mask = torch.LongTensor([_[3] for _ in datas]).to(self.device)\n","        if self.mode==\"branch\":\n","          last_sep = torch.LongTensor([_[4] for _ in datas]).to(self.device)\n","          return (x, seq_len, mask, last_sep), y\n","        else:\n","          return (x, seq_len, mask), y\n","\n","    def __next__(self):\n","        if self.residue and self.index == self.n_batches:\n","            batches = self.batches[self.index * self.batch_size: len(self.batches)]\n","            self.index += 1\n","            batches = self._to_tensor(batches)\n","            return batches\n","        elif self.index >= self.n_batches:\n","            self.index = 0\n","            raise StopIteration\n","        else:\n","            batches = self.batches[self.index * self.batch_size: (self.index + 1) * self.batch_size]\n","            self.index += 1\n","            batches = self._to_tensor(batches)\n","            return batches\n","\n","    def __iter__(self):\n","        return self\n","\n","    def __len__(self):\n","        if self.residue:\n","            return self.n_batches + 1\n","        else:\n","            return self.n_batches\n","\n","def build_iterator(dataset, config, mode='branch'):\n","    iter = DatasetIterater(dataset, config.batch_size, config.device, mode)\n","    return iter\n","\n","\n","def get_time_dif(start_time):\n","    \"\"\"获取已使用时间\"\"\"\n","    end_time = time.time()\n","    time_dif = end_time - start_time\n","    return timedelta(seconds=int(round(time_dif)))"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1649053008033,"user":{"displayName":"刘耘果","userId":"11745284739417236882"},"user_tz":-480},"id":"UHxRUoTWMM4k"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","from sklearn import metrics\n","import time\n","\n","def train(config, model, train_iter, dev_iter, test_iter):\n","    start_time = time.time()\n","    model.train()\n","    bert_params = list(map(id, model.bert.parameters()))\n","    other_params = filter(lambda p: id(p) not in bert_params, model.parameters())\n","  \n","    optimizer = torch.optim.AdamW([\n","             {'params': other_params, \"lr\": 1e-4},\n","             {'params': model.bert.parameters(), 'lr':config.learning_rate}])\n","    total_batch = 0  # 记录进行到多少batch\n","    dev_best_loss = float('inf')\n","    last_improve = 0  # 记录上次验证集loss下降的batch数\n","    flag = False  # 记录是否很久没有效果提升\n","    model.train()\n","    train_score=[]\n","    test_score=[]\n","    for epoch in range(config.num_epochs):\n","        print('Epoch [{}/{}]'.format(epoch + 1, config.num_epochs))\n","        for i, (trains, labels) in enumerate(train_iter):\n","            outputs = model(trains)\n","            model.zero_grad()\n","            loss = F.cross_entropy(outputs,labels)\n","            loss.backward()\n","            optimizer.step()\n","            if total_batch % 50 == 0:\n","                # 每多少轮输出在训练集和验证集上的效果\n","                true = labels.data.cpu()\n","                predic = torch.max(outputs.data, 1)[1].cpu()\n","                train_acc = metrics.accuracy_score(true, predic)\n","                dev_acc, dev_loss = evaluate(config, model, dev_iter)\n","                if dev_loss < dev_best_loss:\n","                    dev_best_loss = dev_loss\n","                    torch.save(model.state_dict(), config.save_path)\n","                    improve = '*'\n","                    last_improve = total_batch\n","                else:\n","                    improve = ''\n","                time_dif = get_time_dif(start_time)\n","                msg = 'Iter: {0:>6},  Train Loss: {1:>5.2},  Train Acc: {2:>6.2%},  Val Loss: {3:>5.2},  Val Acc: {4:>6.2%},  Time: {5} {6}'\n","                print(msg.format(total_batch, loss.item(), train_acc, dev_loss, dev_acc, time_dif, improve))\n","                train_score.append(loss.item())\n","                test_score.append(dev_loss)\n","                model.train()\n","            total_batch += 1\n","            if total_batch - last_improve > config.require_improvement:\n","              # 验证集loss超过1000batch没下降，结束训练\n","              print(\"No optimization for a long time, auto-stopping...\")\n","              flag = True\n","              break\n","        if flag:\n","          break\n","    return None\n","\n","def evaluate(config, model, data_iter, test=False):\n","    model.eval()\n","    loss_total = 0\n","    predict_all = np.array([], dtype=int)\n","    labels_all = np.array([], dtype=int)\n","    with torch.no_grad():\n","        for texts, labels in data_iter:\n","            outputs = model(texts)\n","            loss = F.cross_entropy(outputs, labels)\n","            loss_total += loss\n","            labels = labels.data.cpu().numpy()\n","            predic = torch.max(outputs.data, 1)[1].cpu().numpy()\n","            labels_all = np.append(labels_all, labels)\n","            predict_all = np.append(predict_all, predic)\n","    acc = metrics.accuracy_score(labels_all, predict_all)\n","    if test:\n","        f1 = metrics.f1_score(labels_all, predict_all,average='macro')\n","        return predict_all, f1\n","    return acc, loss_total / len(data_iter)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1649053008034,"user":{"displayName":"刘耘果","userId":"11745284739417236882"},"user_tz":-480},"id":"JCX5EWGShRNs"},"outputs":[],"source":["def get_depth_f1(df,depth):\n","  if depth < 3:\n","    df_depth=df.loc[df['depth']==depth]\n","  else:\n","    df_depth=df.loc[df['depth']>=depth]\n","  f1 = metrics.f1_score(df_depth['label'].to_list(),df_depth['predicted'].to_list(),average='macro')\n","  return f1\n","\n","def test(config, model, test_iter, df_test):\n","# test\n","  model.load_state_dict(torch.load(config.save_path))\n","  start_time = time.time()\n","  predict_all,  f1 = evaluate(config, model, test_iter, test=True)\n","  df_test['predicted'] = list(predict_all)\n","  return df_test, f1"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1649053008034,"user":{"displayName":"刘耘果","userId":"11745284739417236882"},"user_tz":-480},"id":"q-J0mHD-HtXo"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from transformers import BertModel, BertTokenizer\n","\n","\n","class BERT_CNN_Config(object):\n","    \"\"\"配置参数\"\"\"\n","    def __init__(self, dataset, df_train, df_test):\n","        self.model_name = 'Branch_Bert_CNN'\n","        self.train_df = df_train  # 训练集\n","        self.dev_df = df_test  # 验证集\n","        self.test_df = df_test  # 测试集\n","        self.class_list = [0, 1, 2]  # 类别名单\n","        self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'        # 模型训练结果\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 设备\n","\n","        self.require_improvement = 500                               # 若超过1000batch效果还没提升，则提前结束训练\n","        self.num_classes = len(self.class_list)                         # 类别数\n","        self.num_epochs = 100                                             # epoch数\n","        self.batch_size = 16                                   # mini-batch大小\n","        self.pad_size = 500\n","        self.cmt_max_len = 200                                              # 每句话处理成的长度(短填长切)\n","        self.learning_rate = 1e-5                                       # 学习率\n","        self.bert_path = '/content/drive/MyDrive/Prediction/pretrained_BERT'\n","        self.tokenizer = BertTokenizer.from_pretrained(self.bert_path)\n","        self.hidden_size = 768\n","        self.filter_sizes = (2, 3, 4) # 卷积核尺寸\n","        self.num_filters = 64# 卷积核数量\n","        # droptout\n","        self.dropout = 0.5\n","\n","class Global_Pooling_Config(object):\n","    \"\"\"配置参数\"\"\"\n","    def __init__(self, dataset, df_train, df_test):\n","        self.model_name = 'Bert_Global_pooling'\n","        self.train_df = df_train  # 训练集\n","        self.dev_df = df_test  # 验证集\n","        self.test_df = df_test  # 测试集\n","        self.class_list = [0, 1, 2]  # 类别名单\n","        self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'        # 模型训练结果\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 设备\n","        self.require_improvement = 500                               # 若超过1000batch效果还没提升，则提前结束训练\n","        self.num_classes = len(self.class_list)                         # 类别数\n","        self.num_epochs = 100                                             # epoch数\n","        self.batch_size = 16                                   # mini-batch大小\n","        self.pad_size = 500\n","        self.cmt_max_len = 200                                              # 每句话处理成的长度(短填长切)\n","        self.learning_rate = 1e-5                                       # 学习率\n","        self.bert_path = '/content/drive/MyDrive/Prediction/pretrained_BERT'\n","        self.tokenizer = BertTokenizer.from_pretrained(self.bert_path)\n","        self.hidden_size = 768\n","        # droptout\n","        self.dropout = 0.5"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":326,"status":"ok","timestamp":1649053008356,"user":{"displayName":"刘耘果","userId":"11745284739417236882"},"user_tz":-480},"id":"fsbWRk50YWMl"},"outputs":[],"source":["class BranchBert_CNN(nn.Module):\n","    def __init__(self, config, mode='branch'):\n","        super(BranchBert_CNN, self).__init__()\n","        self.bert = BertModel.from_pretrained(config.bert_path)\n","        self.mode = mode\n","        for param in self.bert.parameters():\n","            param.requires_grad = True\n","        self.convs = nn.ModuleList(\n","            [nn.Conv2d(in_channels=1, out_channels=config.num_filters, kernel_size=(k, config.hidden_size)) for k in config.filter_sizes]\n","        )\n","        self.droptout = nn.Dropout(config.dropout)\n","        self.fc = nn.Linear(config.num_filters * len(config.filter_sizes), config.num_classes)\n","    def conv_and_pool(self, x, conv):\n","        x = conv(x)\n","        x = F.relu(x)\n","        x = x.squeeze(3)\n","        size = x.size(2)\n","        x = F.max_pool1d(x, size)\n","        x = x.squeeze(2)\n","        return x\n","    def forward(self, x):\n","        context = x[0]  # 输入的句子\n","        seq_len = x[1]\n","        mask = x[2]  # 对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]\n","        cut_idx = x[3]\n","        output = self.bert(context, attention_mask=mask)\n","        last_hidden_state = output.last_hidden_state #(batch_size,max_lenth, hidden)\n","        ts_list=[]\n","        for i in range(cut_idx.size()[0]):  \n","          temp_tensor=last_hidden_state[i,cut_idx[i]:seq_len[i],:].unsqueeze(0)        \n","          diff= int(config.cmt_max_len - (seq_len[i] - (cut_idx[i])))\n","          if diff>0:\n","            zero_pad = torch.zeros(1, diff , config.hidden_size).to(config.device)\n","            a = torch.cat([temp_tensor,zero_pad],dim=1)\n","          else:\n","            a=last_hidden_state[i,cut_idx[i]:cut_idx[i]+config.cmt_max_len,:].unsqueeze(0) \n","          ts_list.append(a)\n","        last_hidden_state= torch.cat(ts_list,dim=0)\n","        out = last_hidden_state.unsqueeze(1) # ([8, 1, 510, 768])\n","        out = torch.cat([self.conv_and_pool(out, conv)for conv in self.convs], 1)\n","        out = self.droptout(out)\n","        out = self.fc(out)\n","        return out\n","\n","class Branch_Averaging_Model(nn.Module):\n","    def __init__(self, config, mode='branch'):\n","        super(Branch_Averaging_Model, self).__init__()\n","        self.config = config\n","        self.mode = mode\n","        self.bert = BertModel.from_pretrained(config.bert_path)\n","        for param in self.bert.parameters():\n","            param.requires_grad = True\n","        self.droptout = nn.Dropout(config.dropout)\n","        self.fc = nn.Linear(config.hidden_size, config.num_classes)\n","        self.global_pooling=nn.AdaptiveAvgPool1d(1)\n","\n","    def forward(self, x):\n","        context = x[0]  # 输入的句子\n","        seq_len = x[1]\n","        mask = x[2]  # 对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]\n","        cut_idx = x[3]\n","        output = self.bert(context, attention_mask=mask)\n","        last_hidden_state = output.last_hidden_state #(batch_size,max_lenth, hidden)\n","        ts_list=[]\n","        for i in range(cut_idx.size()[0]):\n","          #截取目标文本embedding \n","          #print(cut_idx[i],seq_len[i])\n","          temp_tensor=last_hidden_state[i,cut_idx[i]:seq_len[i],:].unsqueeze(0)  #(batch_size, *real_lenth, hidden_size)\n","          #当前长度，与能接受最长评论长度的差\n","          diff= int(self.config.cmt_max_len - (seq_len[i] - (cut_idx[i])))\n","          if diff>0: #need zero padding\n","            zero_pad = torch.zeros(1, diff , self.config.hidden_size).to(self.config.device)\n","            a = torch.cat([temp_tensor,zero_pad],dim=1)\n","          else:# cut\n","            a=last_hidden_state[i,cut_idx[i]:cut_idx[i]+self.config.cmt_max_len,:].unsqueeze(0) \n","          ts_list.append(a)\n","        last_hidden_state= torch.cat(ts_list,dim=0)# ([batch_size, seq_len, hidden_size])\n","        input = last_hidden_state.permute(0, 2, 1)\n","        output = self.global_pooling(input)\n","        output=output.permute(0, 2, 1)\n","        output=output.squeeze(1)# (batch_size,hidden_size)\n","        out = self.droptout(output)\n","        out = self.fc(out)\n","        return out"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mBRgXYuEsKzz","outputId":"1061ef00-845f-485a-e896-2ec80ea55bd2","executionInfo":{"status":"ok","timestamp":1649076513754,"user_tz":-480,"elapsed":23505399,"user":{"displayName":"刘耘果","userId":"11745284739417236882"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading data...\n","mode:  branch\n","mode:  branch\n","Time usage: 0:00:20\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/drive/MyDrive/Prediction/pretrained_BERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/100]\n","Iter:      0,  Train Loss:   1.3,  Train Acc:  0.00%,  Val Loss:   1.2,  Val Acc: 31.83%,  Time: 0:00:33 *\n","Iter:     50,  Train Loss:   1.1,  Train Acc: 31.25%,  Val Loss:  0.94,  Val Acc: 54.13%,  Time: 0:06:02 *\n","Iter:    100,  Train Loss:  0.94,  Train Acc: 50.00%,  Val Loss:   0.9,  Val Acc: 57.45%,  Time: 0:11:31 *\n","Iter:    150,  Train Loss:  0.69,  Train Acc: 56.25%,  Val Loss:  0.82,  Val Acc: 59.40%,  Time: 0:16:59 *\n","Iter:    200,  Train Loss:  0.73,  Train Acc: 75.00%,  Val Loss:  0.75,  Val Acc: 66.55%,  Time: 0:22:28 *\n","Iter:    250,  Train Loss:  0.58,  Train Acc: 75.00%,  Val Loss:  0.74,  Val Acc: 65.62%,  Time: 0:27:57 *\n","Epoch [2/100]\n","Iter:    300,  Train Loss:   0.7,  Train Acc: 62.50%,  Val Loss:  0.71,  Val Acc: 68.68%,  Time: 0:33:24 *\n","Iter:    350,  Train Loss:  0.82,  Train Acc: 68.75%,  Val Loss:  0.67,  Val Acc: 70.55%,  Time: 0:38:54 *\n","Iter:    400,  Train Loss:  0.67,  Train Acc: 75.00%,  Val Loss:  0.72,  Val Acc: 70.55%,  Time: 0:44:21 \n","Iter:    450,  Train Loss:  0.39,  Train Acc: 75.00%,  Val Loss:  0.66,  Val Acc: 72.09%,  Time: 0:49:50 *\n","Iter:    500,  Train Loss:   0.8,  Train Acc: 62.50%,  Val Loss:  0.63,  Val Acc: 72.43%,  Time: 0:55:18 *\n","Iter:    550,  Train Loss:  0.85,  Train Acc: 62.50%,  Val Loss:  0.71,  Val Acc: 72.26%,  Time: 1:00:46 \n","Epoch [3/100]\n","Iter:    600,  Train Loss:  0.46,  Train Acc: 87.50%,  Val Loss:  0.62,  Val Acc: 73.70%,  Time: 1:06:13 *\n","Iter:    650,  Train Loss:  0.58,  Train Acc: 81.25%,  Val Loss:  0.66,  Val Acc: 72.51%,  Time: 1:11:41 \n","Iter:    700,  Train Loss:  0.53,  Train Acc: 75.00%,  Val Loss:  0.65,  Val Acc: 74.38%,  Time: 1:17:08 \n","Iter:    750,  Train Loss:  0.32,  Train Acc: 81.25%,  Val Loss:  0.65,  Val Acc: 74.38%,  Time: 1:22:35 \n","Iter:    800,  Train Loss:  0.54,  Train Acc: 75.00%,  Val Loss:  0.68,  Val Acc: 70.30%,  Time: 1:28:03 \n","Iter:    850,  Train Loss:  0.56,  Train Acc: 75.00%,  Val Loss:  0.68,  Val Acc: 72.51%,  Time: 1:33:30 \n","Epoch [4/100]\n","Iter:    900,  Train Loss:  0.42,  Train Acc: 81.25%,  Val Loss:  0.69,  Val Acc: 72.09%,  Time: 1:38:56 \n","Iter:    950,  Train Loss:   0.2,  Train Acc: 93.75%,  Val Loss:  0.69,  Val Acc: 71.91%,  Time: 1:44:23 \n","Iter:   1000,  Train Loss:   0.3,  Train Acc: 93.75%,  Val Loss:  0.75,  Val Acc: 70.13%,  Time: 1:49:51 \n","Iter:   1050,  Train Loss:  0.41,  Train Acc: 81.25%,  Val Loss:  0.72,  Val Acc: 72.85%,  Time: 1:55:18 \n","Iter:   1100,  Train Loss:  0.33,  Train Acc: 87.50%,  Val Loss:  0.75,  Val Acc: 69.11%,  Time: 2:00:45 \n","No optimization for a long time, auto-stopping...\n","0.7130761057505789 0.6407827623272994 0.7171220388691433 0.6383521530580354\n","Loading data...\n","mode:  branch\n","mode:  branch\n","Time usage: 0:00:17\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/drive/MyDrive/Prediction/pretrained_BERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/100]\n","Iter:      0,  Train Loss:   1.3,  Train Acc: 25.00%,  Val Loss:   1.2,  Val Acc: 30.98%,  Time: 0:00:29 *\n","Iter:     50,  Train Loss:   1.0,  Train Acc: 37.50%,  Val Loss:  0.93,  Val Acc: 55.57%,  Time: 0:05:58 *\n","Iter:    100,  Train Loss:   1.1,  Train Acc: 43.75%,  Val Loss:  0.87,  Val Acc: 58.55%,  Time: 0:11:26 *\n","Iter:    150,  Train Loss:  0.82,  Train Acc: 56.25%,  Val Loss:  0.81,  Val Acc: 64.60%,  Time: 0:16:55 *\n","Iter:    200,  Train Loss:  0.93,  Train Acc: 50.00%,  Val Loss:  0.75,  Val Acc: 65.53%,  Time: 0:22:24 *\n","Iter:    250,  Train Loss:  0.94,  Train Acc: 50.00%,  Val Loss:  0.69,  Val Acc: 71.15%,  Time: 0:27:52 *\n","Epoch [2/100]\n","Iter:    300,  Train Loss:  0.53,  Train Acc: 62.50%,  Val Loss:  0.78,  Val Acc: 66.98%,  Time: 0:33:18 \n","Iter:    350,  Train Loss:   1.1,  Train Acc: 62.50%,  Val Loss:  0.67,  Val Acc: 71.15%,  Time: 0:38:48 *\n","Iter:    400,  Train Loss:   0.6,  Train Acc: 75.00%,  Val Loss:  0.78,  Val Acc: 65.36%,  Time: 0:44:16 \n","Iter:    450,  Train Loss:  0.37,  Train Acc: 87.50%,  Val Loss:  0.64,  Val Acc: 73.02%,  Time: 0:49:44 *\n","Iter:    500,  Train Loss:  0.67,  Train Acc: 75.00%,  Val Loss:  0.65,  Val Acc: 71.06%,  Time: 0:55:11 \n","Iter:    550,  Train Loss:  0.89,  Train Acc: 62.50%,  Val Loss:  0.65,  Val Acc: 72.43%,  Time: 1:00:39 \n","Epoch [3/100]\n","Iter:    600,  Train Loss:   0.4,  Train Acc: 93.75%,  Val Loss:  0.64,  Val Acc: 73.62%,  Time: 1:06:06 *\n","Iter:    650,  Train Loss:  0.45,  Train Acc: 75.00%,  Val Loss:  0.65,  Val Acc: 73.28%,  Time: 1:11:33 \n","Iter:    700,  Train Loss:  0.29,  Train Acc: 93.75%,  Val Loss:  0.61,  Val Acc: 73.11%,  Time: 1:17:02 *\n","Iter:    750,  Train Loss:  0.28,  Train Acc: 87.50%,  Val Loss:  0.68,  Val Acc: 72.43%,  Time: 1:22:29 \n","Iter:    800,  Train Loss:  0.17,  Train Acc: 100.00%,  Val Loss:  0.66,  Val Acc: 73.36%,  Time: 1:27:57 \n","Iter:    850,  Train Loss:  0.61,  Train Acc: 75.00%,  Val Loss:  0.67,  Val Acc: 72.68%,  Time: 1:33:24 \n","Epoch [4/100]\n","Iter:    900,  Train Loss:  0.33,  Train Acc: 93.75%,  Val Loss:  0.66,  Val Acc: 72.68%,  Time: 1:38:50 \n","Iter:    950,  Train Loss:  0.24,  Train Acc: 93.75%,  Val Loss:  0.64,  Val Acc: 73.87%,  Time: 1:44:18 \n","Iter:   1000,  Train Loss:  0.28,  Train Acc: 93.75%,  Val Loss:  0.65,  Val Acc: 73.62%,  Time: 1:49:45 \n","Iter:   1050,  Train Loss:  0.33,  Train Acc: 87.50%,  Val Loss:  0.68,  Val Acc: 74.13%,  Time: 1:55:13 \n","Iter:   1100,  Train Loss:  0.14,  Train Acc: 100.00%,  Val Loss:  0.72,  Val Acc: 73.87%,  Time: 2:00:40 \n","Iter:   1150,  Train Loss:  0.59,  Train Acc: 87.50%,  Val Loss:  0.86,  Val Acc: 69.53%,  Time: 2:06:07 \n","Epoch [5/100]\n","Iter:   1200,  Train Loss:  0.34,  Train Acc: 93.75%,  Val Loss:   0.8,  Val Acc: 71.23%,  Time: 2:11:33 \n","No optimization for a long time, auto-stopping...\n","0.711950250337499 0.6011751152073733 0.7144409295126072 0.6732277526395173\n","Loading data...\n","mode:  branch\n","mode:  branch\n","Time usage: 0:00:17\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/drive/MyDrive/Prediction/pretrained_BERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/100]\n","Iter:      0,  Train Loss:   1.3,  Train Acc: 31.25%,  Val Loss:   1.2,  Val Acc: 20.77%,  Time: 0:00:29 *\n","Iter:     50,  Train Loss:  0.88,  Train Acc: 62.50%,  Val Loss:  0.94,  Val Acc: 53.28%,  Time: 0:05:58 *\n","Iter:    100,  Train Loss:  0.89,  Train Acc: 62.50%,  Val Loss:  0.88,  Val Acc: 59.23%,  Time: 0:11:27 *\n","Iter:    150,  Train Loss:  0.89,  Train Acc: 56.25%,  Val Loss:  0.79,  Val Acc: 64.26%,  Time: 0:16:55 *\n","Iter:    200,  Train Loss:  0.87,  Train Acc: 68.75%,  Val Loss:  0.77,  Val Acc: 66.98%,  Time: 0:22:24 *\n","Iter:    250,  Train Loss:  0.67,  Train Acc: 68.75%,  Val Loss:  0.72,  Val Acc: 68.09%,  Time: 0:27:53 *\n","Epoch [2/100]\n","Iter:    300,  Train Loss:   1.0,  Train Acc: 56.25%,  Val Loss:  0.71,  Val Acc: 69.53%,  Time: 0:33:20 *\n","Iter:    350,  Train Loss:  0.68,  Train Acc: 62.50%,  Val Loss:  0.69,  Val Acc: 70.72%,  Time: 0:38:49 *\n","Iter:    400,  Train Loss:  0.76,  Train Acc: 62.50%,  Val Loss:  0.65,  Val Acc: 72.26%,  Time: 0:44:17 *\n","Iter:    450,  Train Loss:  0.69,  Train Acc: 68.75%,  Val Loss:   0.6,  Val Acc: 74.89%,  Time: 0:49:46 *\n","Iter:    500,  Train Loss:   0.4,  Train Acc: 81.25%,  Val Loss:  0.61,  Val Acc: 75.06%,  Time: 0:55:13 \n","Iter:    550,  Train Loss:  0.43,  Train Acc: 87.50%,  Val Loss:   0.6,  Val Acc: 75.49%,  Time: 1:00:42 *\n","Epoch [3/100]\n","Iter:    600,  Train Loss:  0.66,  Train Acc: 75.00%,  Val Loss:  0.69,  Val Acc: 70.98%,  Time: 1:06:08 \n","Iter:    650,  Train Loss:   1.2,  Train Acc: 56.25%,  Val Loss:  0.58,  Val Acc: 76.60%,  Time: 1:11:36 *\n","Iter:    700,  Train Loss:  0.37,  Train Acc: 81.25%,  Val Loss:  0.58,  Val Acc: 75.23%,  Time: 1:17:05 *\n","Iter:    750,  Train Loss:  0.52,  Train Acc: 75.00%,  Val Loss:  0.57,  Val Acc: 76.77%,  Time: 1:22:33 *\n","Iter:    800,  Train Loss:  0.23,  Train Acc: 93.75%,  Val Loss:  0.63,  Val Acc: 73.70%,  Time: 1:28:01 \n","Iter:    850,  Train Loss:  0.43,  Train Acc: 87.50%,  Val Loss:  0.67,  Val Acc: 72.77%,  Time: 1:33:28 \n","Epoch [4/100]\n","Iter:    900,  Train Loss:   0.5,  Train Acc: 81.25%,  Val Loss:  0.81,  Val Acc: 70.81%,  Time: 1:38:54 \n","Iter:    950,  Train Loss:  0.54,  Train Acc: 75.00%,  Val Loss:   0.7,  Val Acc: 72.68%,  Time: 1:44:22 \n","Iter:   1000,  Train Loss:  0.24,  Train Acc: 87.50%,  Val Loss:   0.6,  Val Acc: 76.09%,  Time: 1:49:49 \n","Iter:   1050,  Train Loss:  0.22,  Train Acc: 93.75%,  Val Loss:  0.68,  Val Acc: 75.66%,  Time: 1:55:17 \n","Iter:   1100,  Train Loss: 0.079,  Train Acc: 100.00%,  Val Loss:  0.79,  Val Acc: 70.72%,  Time: 2:00:44 \n","Iter:   1150,  Train Loss:  0.17,  Train Acc: 93.75%,  Val Loss:  0.71,  Val Acc: 74.38%,  Time: 2:06:11 \n","Epoch [5/100]\n","Iter:   1200,  Train Loss:  0.18,  Train Acc: 93.75%,  Val Loss:  0.83,  Val Acc: 73.36%,  Time: 2:11:38 \n","Iter:   1250,  Train Loss:  0.26,  Train Acc: 75.00%,  Val Loss:  0.76,  Val Acc: 72.17%,  Time: 2:17:05 \n","No optimization for a long time, auto-stopping...\n","0.7462885609936949 0.6294322161719969 0.7708455882352941 0.597557084233101\n","f: 0.7237716390272576+0.019508349974684168\n","f1: 0.6237966979022231+0.02039634102531287\n","f2: 0.7341361855390148+0.03181952657698765\n","f3: 0.6363789966435512+0.03787390294048746\n","\n","[0.7130761057505789, 0.711950250337499, 0.7462885609936949]\n"]}],"source":["'''Branch-BERT'''\n","f=list()\n","f1=list()\n","f2=list()\n","f3=list()\n","\n","for i in range(3):\n","    dataset='/content/drive/MyDrive/Prediction'\n","    df = pd.concat([df2,df_info])\n","    df = df.sample(frac=1, random_state=i)   \n","    cut_idx1 = int(round(0.8 * df.shape[0]))\n","    df_train,df_test = df[:cut_idx1], df[cut_idx1:]\n","    config = BERT_CNN_Config(dataset,df_train,df_test)\n","    np.random.seed(1)\n","    torch.manual_seed(1)\n","    torch.cuda.manual_seed_all(1)\n","    torch.backends.cudnn.deterministic = True  # 保证每次结果一样\n","    start_time = time.time()\n","    print(\"Loading data...\")\n","    train_data, test_data = build_dataset(config)\n","    train_iter = build_iterator(train_data, config)\n","    dev_iter = build_iterator(test_data, config)\n","    test_iter = build_iterator(test_data, config)\n","    time_dif = get_time_dif(start_time)\n","    print(\"Time usage:\", time_dif)\n","    model = BranchBert_CNN(config).to(config.device)\n","    train(config, model, train_iter, dev_iter, test_iter)\n","    df_test,f_score = test(config, model, test_iter, df_test)\n","    print(f_score, get_depth_f1(df_test,1), get_depth_f1(df_test,2), get_depth_f1(df_test,3))\n","    f.append(f_score)\n","    f1.append(get_depth_f1(df_test,1))\n","    f2.append(get_depth_f1(df_test,2))\n","    f3.append(get_depth_f1(df_test,3))\n","\n","\n","print(\"f: \"+str(np.mean(f))+\"+\"+str(np.std(f, ddof = 1)))\n","print(\"f1: \"+str(np.mean(f1))+\"+\"+str(np.std(f1, ddof = 1)))\n","print(\"f2: \"+str(np.mean(f2))+\"+\"+str(np.std(f2, ddof = 1)))\n","print(\"f3: \"+str(np.mean(f3))+\"+\"+str(np.std(f3, ddof = 1)))\n","\n","print()\n","print(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AqYQOyf9MgdD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648984889820,"user_tz":-480,"elapsed":10839312,"user":{"displayName":"刘耘果","userId":"11745284739417236882"}},"outputId":"e4a43f3f-24b6-4823-9fd6-06719ee2c723"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading data...\n","mode:  branch\n","mode:  branch\n","Time usage: 0:00:18\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/drive/MyDrive/Prediction/pretrained_BERT were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/100]\n","Iter:      0,  Train Loss:   1.1,  Train Acc:  6.25%,  Val Loss:   1.1,  Val Acc: 21.53%,  Time: 0:00:29 *\n","Iter:     50,  Train Loss:   1.1,  Train Acc: 43.75%,  Val Loss:   1.1,  Val Acc: 49.53%,  Time: 0:01:36 *\n","Iter:    100,  Train Loss:  0.99,  Train Acc: 56.25%,  Val Loss:   1.1,  Val Acc: 48.68%,  Time: 0:02:42 *\n","Iter:    150,  Train Loss:  0.82,  Train Acc: 68.75%,  Val Loss:   1.0,  Val Acc: 50.89%,  Time: 0:03:48 *\n","Iter:    200,  Train Loss:  0.88,  Train Acc: 62.50%,  Val Loss:   1.0,  Val Acc: 51.83%,  Time: 0:04:53 *\n","Iter:    250,  Train Loss:   0.9,  Train Acc: 68.75%,  Val Loss:   1.0,  Val Acc: 52.17%,  Time: 0:05:58 \n","Epoch [2/100]\n","Iter:    300,  Train Loss:   0.9,  Train Acc: 68.75%,  Val Loss:   1.0,  Val Acc: 60.68%,  Time: 0:07:03 *\n","Iter:    350,  Train Loss:  0.95,  Train Acc: 75.00%,  Val Loss:  0.97,  Val Acc: 59.23%,  Time: 0:08:09 *\n","Iter:    400,  Train Loss:   1.0,  Train Acc: 50.00%,  Val Loss:  0.96,  Val Acc: 60.43%,  Time: 0:09:15 *\n","Iter:    450,  Train Loss:  0.87,  Train Acc: 75.00%,  Val Loss:  0.93,  Val Acc: 67.57%,  Time: 0:10:20 *\n","Iter:    500,  Train Loss:   1.0,  Train Acc: 50.00%,  Val Loss:  0.92,  Val Acc: 65.28%,  Time: 0:11:26 *\n","Iter:    550,  Train Loss:   1.1,  Train Acc: 56.25%,  Val Loss:  0.91,  Val Acc: 67.40%,  Time: 0:12:31 *\n","Epoch [3/100]\n","Iter:    600,  Train Loss:  0.93,  Train Acc: 81.25%,  Val Loss:   0.9,  Val Acc: 70.04%,  Time: 0:13:37 *\n","Iter:    650,  Train Loss:  0.77,  Train Acc: 50.00%,  Val Loss:   0.9,  Val Acc: 67.83%,  Time: 0:14:41 \n","Iter:    700,  Train Loss:  0.75,  Train Acc: 87.50%,  Val Loss:  0.85,  Val Acc: 70.89%,  Time: 0:15:47 *\n","Iter:    750,  Train Loss:  0.63,  Train Acc: 81.25%,  Val Loss:  0.87,  Val Acc: 71.23%,  Time: 0:16:51 \n","Iter:    800,  Train Loss:  0.84,  Train Acc: 68.75%,  Val Loss:  0.86,  Val Acc: 69.45%,  Time: 0:17:56 \n","Iter:    850,  Train Loss:  0.72,  Train Acc: 75.00%,  Val Loss:  0.87,  Val Acc: 70.21%,  Time: 0:19:00 \n","Epoch [4/100]\n","Iter:    900,  Train Loss:  0.63,  Train Acc: 87.50%,  Val Loss:  0.89,  Val Acc: 71.66%,  Time: 0:20:04 \n","Iter:    950,  Train Loss:  0.65,  Train Acc: 81.25%,  Val Loss:  0.84,  Val Acc: 71.40%,  Time: 0:21:10 *\n","Iter:   1000,  Train Loss:  0.59,  Train Acc: 81.25%,  Val Loss:  0.87,  Val Acc: 70.47%,  Time: 0:22:15 \n","Iter:   1050,  Train Loss:  0.55,  Train Acc: 68.75%,  Val Loss:  0.88,  Val Acc: 70.30%,  Time: 0:23:19 \n","Iter:   1100,  Train Loss:  0.64,  Train Acc: 81.25%,  Val Loss:  0.84,  Val Acc: 70.55%,  Time: 0:24:25 *\n","Iter:   1150,  Train Loss:  0.42,  Train Acc: 81.25%,  Val Loss:  0.88,  Val Acc: 70.55%,  Time: 0:25:29 \n","Epoch [5/100]\n","Iter:   1200,  Train Loss:  0.46,  Train Acc: 93.75%,  Val Loss:   0.9,  Val Acc: 71.23%,  Time: 0:26:34 \n","Iter:   1250,  Train Loss:  0.48,  Train Acc: 93.75%,  Val Loss:   0.9,  Val Acc: 70.89%,  Time: 0:27:38 \n","Iter:   1300,  Train Loss:  0.71,  Train Acc: 68.75%,  Val Loss:  0.85,  Val Acc: 70.81%,  Time: 0:28:43 \n","Iter:   1350,  Train Loss:  0.77,  Train Acc: 62.50%,  Val Loss:  0.88,  Val Acc: 72.00%,  Time: 0:29:47 \n","Iter:   1400,  Train Loss:  0.56,  Train Acc: 87.50%,  Val Loss:  0.87,  Val Acc: 70.81%,  Time: 0:30:52 \n","Iter:   1450,  Train Loss:  0.44,  Train Acc: 87.50%,  Val Loss:  0.98,  Val Acc: 70.13%,  Time: 0:31:56 \n","Epoch [6/100]\n","Iter:   1500,  Train Loss:  0.47,  Train Acc: 93.75%,  Val Loss:  0.93,  Val Acc: 72.94%,  Time: 0:33:00 \n","Iter:   1550,  Train Loss:  0.54,  Train Acc: 87.50%,  Val Loss:  0.94,  Val Acc: 70.13%,  Time: 0:34:05 \n","Iter:   1600,  Train Loss:  0.75,  Train Acc: 68.75%,  Val Loss:  0.87,  Val Acc: 70.98%,  Time: 0:35:09 \n","No optimization for a long time, auto-stopping...\n","0.6835456938658305 0.601796200916564 0.6931856354257476 0.5709876543209877\n","Loading data...\n","mode:  branch\n","mode:  branch\n","Time usage: 0:00:14\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/drive/MyDrive/Prediction/pretrained_BERT were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/100]\n","Iter:      0,  Train Loss:   1.1,  Train Acc: 12.50%,  Val Loss:   1.1,  Val Acc: 18.64%,  Time: 0:00:24 *\n","Iter:     50,  Train Loss:   1.1,  Train Acc: 50.00%,  Val Loss:   1.1,  Val Acc: 51.15%,  Time: 0:01:29 *\n","Iter:    100,  Train Loss:   1.2,  Train Acc: 31.25%,  Val Loss:   1.0,  Val Acc: 51.83%,  Time: 0:02:35 *\n","Iter:    150,  Train Loss:  0.96,  Train Acc: 68.75%,  Val Loss:   1.0,  Val Acc: 53.62%,  Time: 0:03:41 *\n","Iter:    200,  Train Loss:  0.91,  Train Acc: 50.00%,  Val Loss:   1.0,  Val Acc: 53.96%,  Time: 0:04:46 \n","Iter:    250,  Train Loss:   1.2,  Train Acc: 43.75%,  Val Loss:  0.98,  Val Acc: 58.47%,  Time: 0:05:51 *\n","Epoch [2/100]\n","Iter:    300,  Train Loss:  0.95,  Train Acc: 56.25%,  Val Loss:  0.95,  Val Acc: 57.79%,  Time: 0:06:56 *\n","Iter:    350,  Train Loss:   1.1,  Train Acc: 56.25%,  Val Loss:  0.94,  Val Acc: 64.17%,  Time: 0:08:02 *\n","Iter:    400,  Train Loss:   1.0,  Train Acc: 62.50%,  Val Loss:  0.96,  Val Acc: 61.11%,  Time: 0:09:06 \n","Iter:    450,  Train Loss:  0.88,  Train Acc: 75.00%,  Val Loss:  0.92,  Val Acc: 65.53%,  Time: 0:10:11 *\n","Iter:    500,  Train Loss:   1.0,  Train Acc: 62.50%,  Val Loss:  0.92,  Val Acc: 65.87%,  Time: 0:11:16 \n","Iter:    550,  Train Loss:  0.98,  Train Acc: 56.25%,  Val Loss:  0.88,  Val Acc: 65.96%,  Time: 0:12:21 *\n","Epoch [3/100]\n","Iter:    600,  Train Loss:  0.68,  Train Acc: 68.75%,  Val Loss:  0.87,  Val Acc: 68.26%,  Time: 0:13:26 *\n","Iter:    650,  Train Loss:  0.85,  Train Acc: 62.50%,  Val Loss:  0.87,  Val Acc: 68.60%,  Time: 0:14:31 \n","Iter:    700,  Train Loss:  0.74,  Train Acc: 87.50%,  Val Loss:  0.87,  Val Acc: 66.98%,  Time: 0:15:36 *\n","Iter:    750,  Train Loss:  0.63,  Train Acc: 81.25%,  Val Loss:  0.86,  Val Acc: 70.13%,  Time: 0:16:42 *\n","Iter:    800,  Train Loss:   0.6,  Train Acc: 93.75%,  Val Loss:  0.89,  Val Acc: 70.13%,  Time: 0:17:46 \n","Iter:    850,  Train Loss:  0.75,  Train Acc: 75.00%,  Val Loss:  0.82,  Val Acc: 70.89%,  Time: 0:18:51 *\n","Epoch [4/100]\n","Iter:    900,  Train Loss:  0.58,  Train Acc: 93.75%,  Val Loss:  0.82,  Val Acc: 72.34%,  Time: 0:19:57 *\n","Iter:    950,  Train Loss:  0.62,  Train Acc: 81.25%,  Val Loss:  0.82,  Val Acc: 71.06%,  Time: 0:21:01 \n","Iter:   1000,  Train Loss:   0.6,  Train Acc: 81.25%,  Val Loss:  0.81,  Val Acc: 71.32%,  Time: 0:22:06 *\n","Iter:   1050,  Train Loss:  0.69,  Train Acc: 81.25%,  Val Loss:  0.88,  Val Acc: 69.87%,  Time: 0:23:11 \n","Iter:   1100,  Train Loss:   0.8,  Train Acc: 75.00%,  Val Loss:  0.83,  Val Acc: 70.04%,  Time: 0:24:15 \n","Iter:   1150,  Train Loss:  0.58,  Train Acc: 81.25%,  Val Loss:  0.82,  Val Acc: 70.64%,  Time: 0:25:19 \n","Epoch [5/100]\n","Iter:   1200,  Train Loss:  0.47,  Train Acc: 81.25%,  Val Loss:  0.82,  Val Acc: 72.26%,  Time: 0:26:23 \n","Iter:   1250,  Train Loss:  0.51,  Train Acc: 87.50%,  Val Loss:  0.89,  Val Acc: 71.15%,  Time: 0:27:28 \n","Iter:   1300,  Train Loss:  0.77,  Train Acc: 81.25%,  Val Loss:  0.82,  Val Acc: 71.32%,  Time: 0:28:32 \n","Iter:   1350,  Train Loss:  0.71,  Train Acc: 62.50%,  Val Loss:  0.88,  Val Acc: 70.04%,  Time: 0:29:36 \n","Iter:   1400,  Train Loss:  0.51,  Train Acc: 93.75%,  Val Loss:  0.94,  Val Acc: 70.64%,  Time: 0:30:41 \n","Iter:   1450,  Train Loss:  0.62,  Train Acc: 75.00%,  Val Loss:  0.99,  Val Acc: 64.51%,  Time: 0:31:45 \n","Epoch [6/100]\n","Iter:   1500,  Train Loss:  0.55,  Train Acc: 81.25%,  Val Loss:  0.83,  Val Acc: 72.43%,  Time: 0:32:49 \n","No optimization for a long time, auto-stopping...\n","0.6896444842282863 0.5738264099966227 0.6880466029615424 0.6310755148741419\n","Loading data...\n","mode:  branch\n","mode:  branch\n","Time usage: 0:00:14\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/drive/MyDrive/Prediction/pretrained_BERT were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/100]\n","Iter:      0,  Train Loss:   1.1,  Train Acc: 18.75%,  Val Loss:   1.1,  Val Acc: 28.51%,  Time: 0:00:24 *\n","Iter:     50,  Train Loss:  0.99,  Train Acc: 68.75%,  Val Loss:   1.1,  Val Acc: 51.15%,  Time: 0:01:29 *\n","Iter:    100,  Train Loss:   1.2,  Train Acc: 43.75%,  Val Loss:   1.0,  Val Acc: 50.81%,  Time: 0:02:35 *\n","Iter:    150,  Train Loss:   1.1,  Train Acc: 31.25%,  Val Loss:   1.0,  Val Acc: 52.09%,  Time: 0:03:40 *\n","Iter:    200,  Train Loss:   1.0,  Train Acc: 50.00%,  Val Loss:   1.0,  Val Acc: 53.28%,  Time: 0:04:45 *\n","Iter:    250,  Train Loss:  0.95,  Train Acc: 68.75%,  Val Loss:  0.98,  Val Acc: 54.30%,  Time: 0:05:51 *\n","Epoch [2/100]\n","Iter:    300,  Train Loss:   1.0,  Train Acc: 18.75%,  Val Loss:  0.96,  Val Acc: 56.09%,  Time: 0:06:56 *\n","Iter:    350,  Train Loss:  0.98,  Train Acc: 56.25%,  Val Loss:  0.95,  Val Acc: 58.55%,  Time: 0:08:02 *\n","Iter:    400,  Train Loss:  0.93,  Train Acc: 62.50%,  Val Loss:  0.93,  Val Acc: 60.60%,  Time: 0:09:07 *\n","Iter:    450,  Train Loss:   1.1,  Train Acc: 68.75%,  Val Loss:  0.92,  Val Acc: 61.11%,  Time: 0:10:13 *\n","Iter:    500,  Train Loss:  0.81,  Train Acc: 75.00%,  Val Loss:   0.9,  Val Acc: 63.23%,  Time: 0:11:18 *\n","Iter:    550,  Train Loss:  0.87,  Train Acc: 81.25%,  Val Loss:  0.89,  Val Acc: 66.81%,  Time: 0:12:23 *\n","Epoch [3/100]\n","Iter:    600,  Train Loss:  0.64,  Train Acc: 81.25%,  Val Loss:  0.87,  Val Acc: 68.26%,  Time: 0:13:29 *\n","Iter:    650,  Train Loss:   1.1,  Train Acc: 62.50%,  Val Loss:  0.88,  Val Acc: 64.17%,  Time: 0:14:33 \n","Iter:    700,  Train Loss:  0.71,  Train Acc: 87.50%,  Val Loss:  0.82,  Val Acc: 72.00%,  Time: 0:15:39 *\n","Iter:    750,  Train Loss:  0.79,  Train Acc: 62.50%,  Val Loss:  0.83,  Val Acc: 73.28%,  Time: 0:16:43 \n","Iter:    800,  Train Loss:  0.57,  Train Acc: 81.25%,  Val Loss:   1.0,  Val Acc: 68.09%,  Time: 0:17:47 \n","Iter:    850,  Train Loss:  0.66,  Train Acc: 68.75%,  Val Loss:  0.81,  Val Acc: 71.32%,  Time: 0:18:53 *\n","Epoch [4/100]\n","Iter:    900,  Train Loss:  0.91,  Train Acc: 62.50%,  Val Loss:  0.88,  Val Acc: 70.04%,  Time: 0:19:57 \n","Iter:    950,  Train Loss:  0.57,  Train Acc: 81.25%,  Val Loss:  0.78,  Val Acc: 73.36%,  Time: 0:21:02 *\n","Iter:   1000,  Train Loss:  0.48,  Train Acc: 87.50%,  Val Loss:  0.79,  Val Acc: 73.53%,  Time: 0:22:07 \n","Iter:   1050,  Train Loss:  0.59,  Train Acc: 81.25%,  Val Loss:   0.8,  Val Acc: 75.32%,  Time: 0:23:11 \n","Iter:   1100,  Train Loss:  0.63,  Train Acc: 93.75%,  Val Loss:  0.96,  Val Acc: 66.98%,  Time: 0:24:15 \n","Iter:   1150,  Train Loss:  0.52,  Train Acc: 93.75%,  Val Loss:  0.79,  Val Acc: 74.55%,  Time: 0:25:20 \n","Epoch [5/100]\n","Iter:   1200,  Train Loss:  0.97,  Train Acc: 68.75%,  Val Loss:  0.87,  Val Acc: 73.53%,  Time: 0:26:24 \n","Iter:   1250,  Train Loss:  0.53,  Train Acc: 87.50%,  Val Loss:  0.81,  Val Acc: 73.36%,  Time: 0:27:28 \n","Iter:   1300,  Train Loss:  0.74,  Train Acc: 62.50%,  Val Loss:   0.8,  Val Acc: 74.64%,  Time: 0:28:32 \n","Iter:   1350,  Train Loss:  0.44,  Train Acc: 93.75%,  Val Loss:  0.81,  Val Acc: 74.21%,  Time: 0:29:37 \n","Iter:   1400,  Train Loss:  0.75,  Train Acc: 68.75%,  Val Loss:  0.96,  Val Acc: 68.94%,  Time: 0:30:41 \n","Iter:   1450,  Train Loss:  0.62,  Train Acc: 68.75%,  Val Loss:  0.81,  Val Acc: 73.79%,  Time: 0:31:45 \n","No optimization for a long time, auto-stopping...\n","0.7071240275811168 0.6014880952380953 0.7227297993785992 0.5744093079059732\n","Loading data...\n","mode:  branch\n","mode:  branch\n","Time usage: 0:00:15\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/drive/MyDrive/Prediction/pretrained_BERT were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/100]\n","Iter:      0,  Train Loss:   1.1,  Train Acc: 12.50%,  Val Loss:   1.1,  Val Acc: 26.89%,  Time: 0:00:24 *\n","Iter:     50,  Train Loss:   1.1,  Train Acc: 50.00%,  Val Loss:   1.1,  Val Acc: 51.74%,  Time: 0:01:29 *\n","Iter:    100,  Train Loss:  0.99,  Train Acc: 62.50%,  Val Loss:   1.0,  Val Acc: 51.57%,  Time: 0:02:35 *\n","Iter:    150,  Train Loss:   1.0,  Train Acc: 37.50%,  Val Loss:   1.0,  Val Acc: 53.02%,  Time: 0:03:40 *\n","Iter:    200,  Train Loss:  0.96,  Train Acc: 37.50%,  Val Loss:   1.0,  Val Acc: 53.79%,  Time: 0:04:46 *\n","Iter:    250,  Train Loss:   1.0,  Train Acc: 62.50%,  Val Loss:  0.99,  Val Acc: 53.62%,  Time: 0:05:51 *\n","Epoch [2/100]\n","Iter:    300,  Train Loss:   1.2,  Train Acc: 62.50%,  Val Loss:  0.97,  Val Acc: 54.98%,  Time: 0:06:56 *\n","Iter:    350,  Train Loss:  0.89,  Train Acc: 56.25%,  Val Loss:  0.97,  Val Acc: 53.11%,  Time: 0:08:02 *\n","Iter:    400,  Train Loss:   1.1,  Train Acc: 37.50%,  Val Loss:  0.94,  Val Acc: 55.23%,  Time: 0:09:07 *\n","Iter:    450,  Train Loss:   1.1,  Train Acc: 68.75%,  Val Loss:  0.92,  Val Acc: 60.17%,  Time: 0:10:13 *\n","Iter:    500,  Train Loss:  0.91,  Train Acc: 68.75%,  Val Loss:  0.91,  Val Acc: 63.57%,  Time: 0:11:18 *\n","Iter:    550,  Train Loss:   1.1,  Train Acc: 37.50%,  Val Loss:  0.93,  Val Acc: 62.04%,  Time: 0:12:23 \n","Epoch [3/100]\n","Iter:    600,  Train Loss:  0.84,  Train Acc: 68.75%,  Val Loss:  0.85,  Val Acc: 67.74%,  Time: 0:13:28 *\n","Iter:    650,  Train Loss:  0.84,  Train Acc: 68.75%,  Val Loss:  0.86,  Val Acc: 68.34%,  Time: 0:14:32 \n","Iter:    700,  Train Loss:  0.73,  Train Acc: 68.75%,  Val Loss:  0.85,  Val Acc: 69.45%,  Time: 0:15:36 \n","Iter:    750,  Train Loss:  0.88,  Train Acc: 75.00%,  Val Loss:  0.84,  Val Acc: 70.72%,  Time: 0:16:42 *\n","Iter:    800,  Train Loss:  0.69,  Train Acc: 93.75%,  Val Loss:  0.84,  Val Acc: 68.09%,  Time: 0:17:46 \n","Iter:    850,  Train Loss:  0.82,  Train Acc: 81.25%,  Val Loss:  0.83,  Val Acc: 71.23%,  Time: 0:18:52 *\n","Epoch [4/100]\n","Iter:    900,  Train Loss:  0.77,  Train Acc: 62.50%,  Val Loss:  0.82,  Val Acc: 71.74%,  Time: 0:19:57 *\n","Iter:    950,  Train Loss:  0.55,  Train Acc: 100.00%,  Val Loss:  0.81,  Val Acc: 72.94%,  Time: 0:21:03 *\n","Iter:   1000,  Train Loss:  0.91,  Train Acc: 56.25%,  Val Loss:  0.85,  Val Acc: 69.62%,  Time: 0:22:07 \n","Iter:   1050,  Train Loss:  0.77,  Train Acc: 87.50%,  Val Loss:  0.81,  Val Acc: 71.91%,  Time: 0:23:11 \n","Iter:   1100,  Train Loss:  0.75,  Train Acc: 75.00%,  Val Loss:  0.79,  Val Acc: 72.51%,  Time: 0:24:17 *\n","Iter:   1150,  Train Loss:  0.73,  Train Acc: 75.00%,  Val Loss:  0.82,  Val Acc: 73.02%,  Time: 0:25:21 \n","Epoch [5/100]\n","Iter:   1200,  Train Loss:   0.6,  Train Acc: 75.00%,  Val Loss:  0.79,  Val Acc: 73.02%,  Time: 0:26:25 \n","Iter:   1250,  Train Loss:  0.46,  Train Acc: 100.00%,  Val Loss:   0.8,  Val Acc: 72.68%,  Time: 0:27:30 \n","Iter:   1300,  Train Loss:  0.78,  Train Acc: 75.00%,  Val Loss:  0.89,  Val Acc: 72.17%,  Time: 0:28:34 \n","Iter:   1350,  Train Loss:   0.7,  Train Acc: 81.25%,  Val Loss:  0.86,  Val Acc: 69.53%,  Time: 0:29:38 \n","Iter:   1400,  Train Loss:  0.57,  Train Acc: 87.50%,  Val Loss:  0.78,  Val Acc: 72.68%,  Time: 0:30:44 *\n","Iter:   1450,  Train Loss:  0.64,  Train Acc: 68.75%,  Val Loss:  0.83,  Val Acc: 73.45%,  Time: 0:31:48 \n","Epoch [6/100]\n","Iter:   1500,  Train Loss:  0.55,  Train Acc: 81.25%,  Val Loss:  0.84,  Val Acc: 73.28%,  Time: 0:32:52 \n","Iter:   1550,  Train Loss:   0.4,  Train Acc: 87.50%,  Val Loss:  0.86,  Val Acc: 71.66%,  Time: 0:33:57 \n","Iter:   1600,  Train Loss:  0.43,  Train Acc: 93.75%,  Val Loss:   0.9,  Val Acc: 73.28%,  Time: 0:35:01 \n","Iter:   1650,  Train Loss:  0.59,  Train Acc: 75.00%,  Val Loss:  0.85,  Val Acc: 71.49%,  Time: 0:36:05 \n","Iter:   1700,  Train Loss:  0.65,  Train Acc: 75.00%,  Val Loss:  0.82,  Val Acc: 71.91%,  Time: 0:37:09 \n","Iter:   1750,  Train Loss:  0.43,  Train Acc: 87.50%,  Val Loss:  0.82,  Val Acc: 72.85%,  Time: 0:38:14 \n","Epoch [7/100]\n","Iter:   1800,  Train Loss:  0.41,  Train Acc: 87.50%,  Val Loss:  0.83,  Val Acc: 73.53%,  Time: 0:39:18 \n","Iter:   1850,  Train Loss:  0.49,  Train Acc: 81.25%,  Val Loss:  0.87,  Val Acc: 72.00%,  Time: 0:40:22 \n","Iter:   1900,  Train Loss:  0.26,  Train Acc: 93.75%,  Val Loss:  0.96,  Val Acc: 72.60%,  Time: 0:41:27 \n","No optimization for a long time, auto-stopping...\n","0.6999609923179883 0.7119952619667069 0.69065711452321 0.6646126255342429\n","Loading data...\n","mode:  branch\n","mode:  branch\n","Time usage: 0:00:14\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/drive/MyDrive/Prediction/pretrained_BERT were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/100]\n","Iter:      0,  Train Loss:   1.1,  Train Acc: 25.00%,  Val Loss:   1.1,  Val Acc: 14.13%,  Time: 0:00:24 *\n","Iter:     50,  Train Loss:   1.0,  Train Acc: 62.50%,  Val Loss:   1.1,  Val Acc: 50.89%,  Time: 0:01:29 *\n","Iter:    100,  Train Loss:   1.0,  Train Acc: 43.75%,  Val Loss:   1.0,  Val Acc: 51.57%,  Time: 0:02:35 *\n","Iter:    150,  Train Loss:  0.85,  Train Acc: 62.50%,  Val Loss:   1.0,  Val Acc: 52.17%,  Time: 0:03:40 *\n","Iter:    200,  Train Loss:  0.93,  Train Acc: 62.50%,  Val Loss:   1.0,  Val Acc: 53.45%,  Time: 0:04:45 \n","Iter:    250,  Train Loss:   1.1,  Train Acc: 56.25%,  Val Loss:  0.99,  Val Acc: 50.89%,  Time: 0:05:50 *\n","Epoch [2/100]\n","Iter:    300,  Train Loss:   1.0,  Train Acc: 50.00%,  Val Loss:  0.96,  Val Acc: 53.96%,  Time: 0:06:55 *\n","Iter:    350,  Train Loss:  0.92,  Train Acc: 50.00%,  Val Loss:  0.95,  Val Acc: 56.94%,  Time: 0:08:01 *\n","Iter:    400,  Train Loss:  0.94,  Train Acc: 56.25%,  Val Loss:  0.94,  Val Acc: 60.60%,  Time: 0:09:06 *\n","Iter:    450,  Train Loss:  0.89,  Train Acc: 62.50%,  Val Loss:  0.92,  Val Acc: 58.55%,  Time: 0:10:12 *\n","Iter:    500,  Train Loss:  0.94,  Train Acc: 50.00%,  Val Loss:  0.94,  Val Acc: 61.11%,  Time: 0:11:16 \n","Iter:    550,  Train Loss:  0.86,  Train Acc: 62.50%,  Val Loss:   0.9,  Val Acc: 60.00%,  Time: 0:12:22 *\n","Epoch [3/100]\n","Iter:    600,  Train Loss:  0.76,  Train Acc: 75.00%,  Val Loss:  0.89,  Val Acc: 65.53%,  Time: 0:13:27 *\n","Iter:    650,  Train Loss:   0.8,  Train Acc: 68.75%,  Val Loss:  0.93,  Val Acc: 64.17%,  Time: 0:14:31 \n","Iter:    700,  Train Loss:  0.74,  Train Acc: 93.75%,  Val Loss:  0.89,  Val Acc: 65.11%,  Time: 0:15:35 \n","Iter:    750,  Train Loss:  0.57,  Train Acc: 81.25%,  Val Loss:  0.93,  Val Acc: 65.87%,  Time: 0:16:40 \n","Iter:    800,  Train Loss:  0.68,  Train Acc: 81.25%,  Val Loss:  0.87,  Val Acc: 68.26%,  Time: 0:17:46 *\n","Iter:    850,  Train Loss:  0.86,  Train Acc: 75.00%,  Val Loss:  0.83,  Val Acc: 70.30%,  Time: 0:18:52 *\n","Epoch [4/100]\n","Iter:    900,  Train Loss:  0.66,  Train Acc: 81.25%,  Val Loss:  0.82,  Val Acc: 68.77%,  Time: 0:19:57 *\n","Iter:    950,  Train Loss:  0.63,  Train Acc: 81.25%,  Val Loss:  0.87,  Val Acc: 68.43%,  Time: 0:21:01 \n","Iter:   1000,  Train Loss:   0.6,  Train Acc: 81.25%,  Val Loss:  0.83,  Val Acc: 70.81%,  Time: 0:22:05 \n","Iter:   1050,  Train Loss:  0.65,  Train Acc: 87.50%,  Val Loss:  0.84,  Val Acc: 70.55%,  Time: 0:23:10 \n","Iter:   1100,  Train Loss:  0.82,  Train Acc: 81.25%,  Val Loss:  0.89,  Val Acc: 66.55%,  Time: 0:24:14 \n","Iter:   1150,  Train Loss:  0.44,  Train Acc: 100.00%,  Val Loss:  0.81,  Val Acc: 72.00%,  Time: 0:25:20 *\n","Epoch [5/100]\n","Iter:   1200,  Train Loss:   0.3,  Train Acc: 100.00%,  Val Loss:  0.83,  Val Acc: 69.70%,  Time: 0:26:24 \n","Iter:   1250,  Train Loss:  0.88,  Train Acc: 75.00%,  Val Loss:  0.84,  Val Acc: 70.13%,  Time: 0:27:28 \n","Iter:   1300,  Train Loss:  0.47,  Train Acc: 81.25%,  Val Loss:  0.84,  Val Acc: 72.17%,  Time: 0:28:32 \n","Iter:   1350,  Train Loss:  0.38,  Train Acc: 93.75%,  Val Loss:   0.9,  Val Acc: 69.53%,  Time: 0:29:37 \n","Iter:   1400,  Train Loss:  0.72,  Train Acc: 62.50%,  Val Loss:  0.82,  Val Acc: 70.04%,  Time: 0:30:41 \n","Iter:   1450,  Train Loss:  0.52,  Train Acc: 81.25%,  Val Loss:  0.82,  Val Acc: 71.49%,  Time: 0:31:45 \n","Epoch [6/100]\n","Iter:   1500,  Train Loss:  0.48,  Train Acc: 87.50%,  Val Loss:  0.84,  Val Acc: 70.72%,  Time: 0:32:50 \n","Iter:   1550,  Train Loss:  0.31,  Train Acc: 100.00%,  Val Loss:  0.89,  Val Acc: 69.53%,  Time: 0:33:54 \n","Iter:   1600,  Train Loss:  0.45,  Train Acc: 81.25%,  Val Loss:  0.87,  Val Acc: 72.68%,  Time: 0:34:58 \n","Iter:   1650,  Train Loss:  0.38,  Train Acc: 100.00%,  Val Loss:  0.94,  Val Acc: 70.04%,  Time: 0:36:02 \n","No optimization for a long time, auto-stopping...\n","0.679510713952285 0.5819567260042992 0.6825795009013108 0.6013251888719784\n","f: 0.6919571823891013+0.011457644336480452\n","f1: 0.6142125388244577+0.05601119245098891\n","f2: 0.695439730638082+0.015753936489928504\n","f3: 0.6084820583014648+0.03962041231224846\n","\n","[0.6835456938658305, 0.6896444842282863, 0.7071240275811168, 0.6999609923179883, 0.679510713952285]\n"]}],"source":["'''Branch-BERT'''\n","f=list()\n","f1=list()\n","f2=list()\n","f3=list()\n","\n","for i in range(5):\n","    dataset='/content/drive/MyDrive/Prediction'\n","    df = pd.concat([df2,df_info])\n","    df = df.sample(frac=1, random_state=i)   \n","    cut_idx1 = int(round(0.8 * df.shape[0]))\n","    df_train,df_test = df[:cut_idx1], df[cut_idx1:]\n","    config = Global_Pooling_Config(dataset,df_train,df_test)\n","    np.random.seed(1)\n","    torch.manual_seed(1)\n","    torch.cuda.manual_seed_all(1)\n","    torch.backends.cudnn.deterministic = True  # 保证每次结果一样\n","    start_time = time.time()\n","    print(\"Loading data...\")\n","    train_data, test_data = build_dataset(config)\n","    train_iter = build_iterator(train_data, config)\n","    dev_iter = build_iterator(test_data, config)\n","    test_iter = build_iterator(test_data, config)\n","    time_dif = get_time_dif(start_time)\n","    print(\"Time usage:\", time_dif)\n","    model = Branch_Averaging_Model(config).to(config.device)\n","    train(config, model, train_iter, dev_iter, test_iter)\n","    df_test,f_score = test(config, model, test_iter, df_test)\n","    print(f_score, get_depth_f1(df_test,1), get_depth_f1(df_test,2), get_depth_f1(df_test,3))\n","    f.append(f_score)\n","    f1.append(get_depth_f1(df_test,1))\n","    f2.append(get_depth_f1(df_test,2))\n","    f3.append(get_depth_f1(df_test,3))\n","\n","\n","print(\"f: \"+str(np.mean(f))+\"+\"+str(np.std(f, ddof = 1)))\n","print(\"f1: \"+str(np.mean(f1))+\"+\"+str(np.std(f1, ddof = 1)))\n","print(\"f2: \"+str(np.mean(f2))+\"+\"+str(np.std(f2, ddof = 1)))\n","print(\"f3: \"+str(np.mean(f3))+\"+\"+str(np.std(f3, ddof = 1)))\n","\n","print()\n","print(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":457,"status":"ok","timestamp":1648885691576,"user":{"displayName":"刘耘果","userId":"11745284739417236882"},"user_tz":-480},"id":"-fLjBXJGO1LG","outputId":"f8642407-ddef-4a4f-ce8a-024b60e8d704"},"outputs":[{"name":"stdout","output_type":"stream","text":["f: 0.6349046167559657+0.019168399086739528\n","f1: 0.4691658739810835+0.045414580609179155\n","f2: 0.6598154455316492+0.02365948021832502\n","f3: 0.5629314064536135+0.03798504210331831\n","\n","[0.6324664937868018, 0.6164744037866692, 0.6668646272163792, 0.633894085990005, 0.6248234729999733]\n"]}],"source":["f=[0.6324664937868018, 0.6164744037866692, 0.6668646272163792, 0.633894085990005, 0.6248234729999733]\n","f1=[0.5244530933847509, 0.49570228988707904, 0.4756438718443305, 0.4084541869119593, 0.44157592787729777]\n","f2=[0.6569040502189246, 0.6307125569727375, 0.6968214570286624, 0.6582485899013303, 0.6563905735365912]\n","f3=[0.5383672908943038, 0.5222277722277723, 0.5993957997160987, 0.6070551592764083, 0.5476110101534848]\n","\n","print(\"f: \"+str(np.mean(f))+\"+\"+str(np.std(f, ddof = 1)))\n","print(\"f1: \"+str(np.mean(f1))+\"+\"+str(np.std(f1, ddof = 1)))\n","print(\"f2: \"+str(np.mean(f2))+\"+\"+str(np.std(f2, ddof = 1)))\n","print(\"f3: \"+str(np.mean(f3))+\"+\"+str(np.std(f3, ddof = 1)))\n","\n","print()\n","print(f)"]},{"cell_type":"markdown","metadata":{"id":"Rjuffl54JfA7"},"source":["# Non-Branch Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"14Wm6JDhJtjT"},"outputs":[],"source":["import numpy as np\n","import torch\n","from tqdm import tqdm\n","import time\n","from datetime import timedelta\n","import pandas as pd\n","import numpy as np\n","from sklearn import preprocessing\n","PAD, CLS = '[PAD]', '[CLS]'  # padding符号, bert中综合信息符号\n","\n","def build_dataset(config):\n","    def load_dataset(df, pad_size=config.pad_size):\n","        contents = []\n","        tokenizer=config.tokenizer\n","        tokenizer.add_special_tokens({\"additional_special_tokens\": ['[PAD]','[CLS]']})\n","        for i in range(len(df)):\n","            content=df['text'].iloc[i]\n","            label=df['label'].iloc[i]\n","            token = tokenizer.tokenize(content)\n","            seq_len = len(token)\n","            mask = []\n","            token = [CLS] + token\n","            token_ids = tokenizer.convert_tokens_to_ids(token)\n","            if pad_size:\n","                if len(token) < pad_size:\n","                    mask = [1] * len(token_ids) + [0] * (pad_size - len(token))\n","                    token_ids += ([0] * (pad_size - len(token)))\n","                else:\n","                    mask = [1] * pad_size\n","                    token_ids = token_ids[:pad_size]\n","                    seq_len = pad_size\n","            contents.append((token_ids, int(label), seq_len, mask))\n","        return contents\n","    train = load_dataset(config.train_df, config.pad_size)\n","    test = load_dataset(config.test_df, config.pad_size)\n","    return train, test\n","\n","\n","class DatasetIterater(object):\n","    def __init__(self, batches, batch_size, device,):\n","        self.batch_size = batch_size\n","        self.batches = batches\n","        self.n_batches = len(batches) // batch_size\n","        self.residue = False  # 记录batch数量是否为整数\n","        if len(batches) % self.n_batches != 0:\n","            self.residue = True\n","        self.index = 0\n","        self.device = device        \n","    def _to_tensor(self, datas):\n","        x = torch.LongTensor([_[0] for _ in datas]).to(self.device)\n","        y = torch.LongTensor([_[1] for _ in datas]).to(self.device)\n","        # pad前的长度(超过pad_size的设为pad_size)\n","        seq_len = torch.LongTensor([_[2] for _ in datas]).to(self.device)\n","        mask = torch.LongTensor([_[3] for _ in datas]).to(self.device)\n","        return (x, seq_len, mask), y\n","\n","    def __next__(self):\n","        if self.residue and self.index == self.n_batches:\n","            batches = self.batches[self.index * self.batch_size: len(self.batches)]\n","            self.index += 1\n","            batches = self._to_tensor(batches)\n","            return batches\n","\n","        elif self.index >= self.n_batches:\n","            self.index = 0\n","            raise StopIteration\n","        else:\n","            batches = self.batches[self.index * self.batch_size: (self.index + 1) * self.batch_size]\n","            self.index += 1\n","            batches = self._to_tensor(batches)\n","            return batches\n","\n","    def __iter__(self):\n","        return self\n","\n","    def __len__(self):\n","        if self.residue:\n","            return self.n_batches + 1\n","        else:\n","            return self.n_batches\n","\n","\n","def build_iterator(dataset, config):\n","    iter = DatasetIterater(dataset, config.batch_size, config.device)\n","    return iter\n","\n","\n","def get_time_dif(start_time):\n","    \"\"\"获取已使用时间\"\"\"\n","    end_time = time.time()\n","    time_dif = end_time - start_time\n","    return timedelta(seconds=int(round(time_dif)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IDhkhnkhJtjU"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","from sklearn import metrics\n","import time\n","\n","def train(config, model, train_iter, dev_iter, test_iter):\n","    start_time = time.time()\n","    model.train()\n","    bert_params = list(map(id, model.bert.parameters()))\n","    other_params = filter(lambda p: id(p) not in bert_params, model.parameters())\n","  \n","    optimizer = torch.optim.AdamW([\n","             {'params': other_params, \"lr\": 1e-4},\n","             {'params': model.bert.parameters(), 'lr':config.learning_rate}])\n","    total_batch = 0  # 记录进行到多少batch\n","    dev_best_loss = float('inf')\n","    last_improve = 0  # 记录上次验证集loss下降的batch数\n","    flag = False  # 记录是否很久没有效果提升\n","    model.train()\n","    train_score=[]\n","    test_score=[]\n","    for epoch in range(config.num_epochs):\n","        print('Epoch [{}/{}]'.format(epoch + 1, config.num_epochs))\n","        for i, (trains, labels) in enumerate(train_iter):\n","            outputs = model(trains)\n","            model.zero_grad()\n","            loss = F.cross_entropy(outputs,labels)\n","            loss.backward()\n","            optimizer.step()\n","            \n","            if total_batch % 50 == 0:\n","                # 每多少轮输出在训练集和验证集上的效果\n","                true = labels.data.cpu()\n","                predic = torch.max(outputs.data, 1)[1].cpu()\n","                train_acc = metrics.accuracy_score(true, predic)\n","                dev_acc, dev_loss = evaluate(config, model, dev_iter)\n","                if dev_loss < dev_best_loss:\n","                    dev_best_loss = dev_loss\n","                    torch.save(model.state_dict(), config.save_path)\n","                    improve = '*'\n","                    last_improve = total_batch\n","                else:\n","                    improve = ''\n","                time_dif = get_time_dif(start_time)\n","                msg = 'Iter: {0:>6},  Train Loss: {1:>5.2},  Train Acc: {2:>6.2%},  Val Loss: {3:>5.2},  Val Acc: {4:>6.2%},  Time: {5} {6}'\n","                print(msg.format(total_batch, loss.item(), train_acc, dev_loss, dev_acc, time_dif, improve))\n","                train_score.append(loss.item())\n","                test_score.append(dev_loss)\n","                model.train()\n","            total_batch += 1\n","            if total_batch - last_improve > config.require_improvement:\n","              # 验证集loss超过1000batch没下降，结束训练\n","              print(\"No optimization for a long time, auto-stopping...\")\n","              flag = True\n","              break\n","        if flag:\n","          break\n","    return None\n","\n","def evaluate(config, model, data_iter, test=False):\n","    model.eval()\n","    loss_total = 0\n","    predict_all = np.array([], dtype=int)\n","    labels_all = np.array([], dtype=int)\n","    with torch.no_grad():\n","        for texts, labels in data_iter:\n","            outputs = model(texts)\n","            loss = F.cross_entropy(outputs, labels)\n","            loss_total += loss\n","            labels = labels.data.cpu().numpy()\n","            predic = torch.max(outputs.data, 1)[1].cpu().numpy()\n","            labels_all = np.append(labels_all, labels)\n","            predict_all = np.append(predict_all, predic)\n","    acc = metrics.accuracy_score(labels_all, predict_all)\n","    if test:\n","        f1 = metrics.f1_score(labels_all, predict_all,average='macro')\n","        return predict_all, f1\n","    return acc, loss_total / len(data_iter)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JgI-KCTs9_PP"},"outputs":[],"source":["def get_depth_f1(df,depth):\n","  if depth < 3:\n","    df_depth=df.loc[df['depth']==depth]\n","  else:\n","    df_depth=df.loc[df['depth']>=depth]\n","  f1 = metrics.f1_score(df_depth['label'].to_list(),df_depth['predicted'].to_list(),average='macro')\n","  return f1\n","\n","def test(config, model, test_iter, df_test):\n","# test\n","  model.load_state_dict(torch.load(config.save_path))\n","  start_time = time.time()\n","  predict_all,  f1 = evaluate(config, model, test_iter, test=True)\n","  df_test['predicted'] = list(predict_all)\n","  return df_test, f1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4BlJ-bjJJtjU"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from transformers import BertModel, BertTokenizer\n","from transformers import ElectraModel, ElectraTokenizer\n","\n","\n","class BERT_CNN_Config(object):\n","    \"\"\"配置参数\"\"\"\n","    def __init__(self, dataset, df_train, df_test):\n","        self.model_name = 'Branch_Bert_CNN'\n","        self.train_df = df_train  # 训练集\n","        self.dev_df = df_test  # 验证集\n","        self.test_df = df_test  # 测试集\n","        self.class_list = [0, 1, 2]  # 类别名单\n","        self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'        # 模型训练结果\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 设备\n","        self.require_improvement = 500                               # 若超过1000batch效果还没提升，则提前结束训练\n","        self.num_classes = len(self.class_list)                         # 类别数\n","        self.num_epochs = 100                                             # epoch数\n","        self.batch_size = 16                                   # mini-batch大小\n","        self.pad_size = 64\n","        self.learning_rate = 1e-5                                       # 学习率\n","        self.bert_path = '/content/drive/MyDrive/Prediction/pretrained_BERT/Chinese/'\n","        self.tokenizer = BertTokenizer.from_pretrained(self.bert_path)\n","        self.hidden_size = 768\n","        self.filter_sizes = (2, 3, 4) # 卷积核尺寸\n","        self.num_filters = 32# 卷积核数量\n","        # droptout\n","        self.dropout = 0.5\n","\n","class Electra_Config(object):\n","    \"\"\"配置参数\"\"\"\n","    def __init__(self, dataset, df_train, df_test):\n","        self.model_name = 'Electra'\n","        self.train_df = df_train  # 训练集\n","        self.dev_df = df_test  # 验证集\n","        self.test_df = df_test  # 测试集\n","        self.class_list = [0, 1, 2]  # 类别名单\n","        self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'        # 模型训练结果\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 设备\n","        self.require_improvement = 500                               # 若超过1000batch效果还没提升，则提前结束训练\n","        self.num_classes = len(self.class_list)                         # 类别数\n","        self.num_epochs = 100                                             # epoch数\n","        self.batch_size = 16                                   # mini-batch大小\n","        self.pad_size = 64\n","        self.learning_rate = 1e-5                                       # 学习率\n","        self.bert_path = \"toastynews/electra-hongkongese-base-discriminator\"\n","        self.tokenizer = ElectraTokenizer.from_pretrained(self.bert_path)\n","        self.hidden_size = 768\n","        self.dropout = 0.5\n","\n","class BERT_Config(object):\n","    \"\"\"配置参数\"\"\"\n","    def __init__(self, dataset, df_train, df_test):\n","        self.model_name = 'Electra'\n","        self.train_df = df_train  # 训练集\n","        self.dev_df = df_test  # 验证集\n","        self.test_df = df_test  # 测试集\n","        self.class_list = [0, 1, 2]  # 类别名单\n","        self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'        # 模型训练结果\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 设备\n","        self.require_improvement = 500                               # 若超过1000batch效果还没提升，则提前结束训练\n","        self.num_classes = len(self.class_list)                         # 类别数\n","        self.num_epochs = 100                                             # epoch数\n","        self.batch_size = 16                                   # mini-batch大小\n","        self.pad_size = 64\n","        self.learning_rate = 1e-5                                       # 学习率\n","        self.bert_path = '/content/drive/MyDrive/Prediction/pretrained_BERT/Chinese'\n","        self.tokenizer = BertTokenizer.from_pretrained(self.bert_path)\n","        self.hidden_size = 768\n","        self.dropout = 0.5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wbrL6bVvJtjU"},"outputs":[],"source":["class BERT_Model(nn.Module):\n","    def __init__(self, config):\n","        super(BERT_Model, self).__init__()\n","        self.config = config\n","        self.bert = BertModel.from_pretrained(config.bert_path)\n","        for param in self.bert.parameters():\n","            param.requires_grad = True\n","        self.dropout = nn.Dropout(config.dropout)\n","        self.fc = nn.Linear(config.hidden_size, config.num_classes)\n","\n","    def forward(self, x):\n","        context = x[0]  # 输入的句子\n","        seq_len = x[1]\n","        mask = x[2]  # 对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]\n","        output = self.bert(context, attention_mask=mask)\n","        out = self.dropout(output.pooler_output)\n","        out = self.fc(out)\n","        return out\n","\n","class Electra_Model(nn.Module):\n","    def __init__(self, config):\n","        super(Electra_Model, self).__init__()\n","        self.config = config\n","        self.bert = ElectraModel.from_pretrained(config.bert_path)\n","        for param in self.bert.parameters():\n","            param.requires_grad = True\n","        self.dropout = nn.Dropout(config.dropout)\n","        self.fc = nn.Linear(config.hidden_size, config.num_classes)\n","    def forward(self, x):\n","        context = x[0]  # 输入的句子\n","        seq_len = x[1]\n","        mask = x[2]  # 对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]\n","        discriminator_hidden_states = self.bert(context, attention_mask=mask)\n","        sequence_output = discriminator_hidden_states[0]\n","        output = sequence_output[:, 0, :]\n","        out = self.dropout(output)\n","        out = self.fc(out)\n","        return out\n","\n","class BERT_CNN(nn.Module):\n","    def __init__(self, config):\n","        super(BERT_CNN, self).__init__()\n","        self.bert = BertModel.from_pretrained(config.bert_path)\n","        for param in self.bert.parameters():\n","            param.requires_grad = True\n","        self.convs = nn.ModuleList(\n","            [nn.Conv2d(1, config.num_filters, (k, config.hidden_size)) for k in config.filter_sizes])\n","        self.dropout = nn.Dropout(config.dropout)\n","        self.fc_cnn = nn.Linear(config.num_filters * len(config.filter_sizes), config.num_classes)\n","\n","    def conv_and_pool(self, x, conv):\n","        x = F.relu(conv(x)).squeeze(3)\n","        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n","        return x\n","\n","    def forward(self, x):\n","        context = x[0]  # 输入的句子\n","        mask = x[2]  # 对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]\n","        output = self.bert(context, attention_mask=mask)\n","        last_hidden_state = output.last_hidden_state #(batch_size,max_lenth, hidden)\n","        out = last_hidden_state.unsqueeze(1)\n","        out = torch.cat([self.conv_and_pool(out, conv) for conv in self.convs], 1)\n","        out = self.dropout(out)\n","        out = self.fc_cnn(out)\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19635276,"status":"ok","timestamp":1648819809208,"user":{"displayName":"刘耘果","userId":"11745284739417236882"},"user_tz":-480},"id":"72H9n_dFJtjV","outputId":"0e40ecd4-c9ea-44d4-cb69-bae0df6a0cad"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading data...\n","Time usage: 0:00:03\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at /content/drive/MyDrive/Prediction/pretrained_BERT were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/100]\n","Iter:      0,  Train Loss:   1.2,  Train Acc: 31.25%,  Val Loss:   1.1,  Val Acc: 36.26%,  Time: 0:00:11 *\n","Iter:     50,  Train Loss:   1.2,  Train Acc: 25.00%,  Val Loss:  0.97,  Val Acc: 50.47%,  Time: 0:03:02 *\n","Iter:    100,  Train Loss:  0.98,  Train Acc: 50.00%,  Val Loss:  0.93,  Val Acc: 53.70%,  Time: 0:05:54 *\n","Iter:    150,  Train Loss:  0.69,  Train Acc: 81.25%,  Val Loss:  0.88,  Val Acc: 57.28%,  Time: 0:08:45 *\n","Iter:    200,  Train Loss:  0.88,  Train Acc: 43.75%,  Val Loss:  0.86,  Val Acc: 58.47%,  Time: 0:11:36 *\n","Iter:    250,  Train Loss:  0.75,  Train Acc: 68.75%,  Val Loss:  0.84,  Val Acc: 59.32%,  Time: 0:14:27 *\n","Epoch [2/100]\n","Iter:    300,  Train Loss:  0.82,  Train Acc: 62.50%,  Val Loss:  0.82,  Val Acc: 61.02%,  Time: 0:17:16 *\n","Iter:    350,  Train Loss:  0.74,  Train Acc: 62.50%,  Val Loss:  0.85,  Val Acc: 57.79%,  Time: 0:20:06 \n","Iter:    400,  Train Loss:   1.2,  Train Acc: 50.00%,  Val Loss:  0.88,  Val Acc: 58.55%,  Time: 0:22:56 \n","Iter:    450,  Train Loss:  0.63,  Train Acc: 56.25%,  Val Loss:  0.81,  Val Acc: 61.70%,  Time: 0:25:46 *\n","Iter:    500,  Train Loss:  0.86,  Train Acc: 56.25%,  Val Loss:  0.79,  Val Acc: 62.38%,  Time: 0:28:37 *\n","Iter:    550,  Train Loss:   1.1,  Train Acc: 43.75%,  Val Loss:  0.84,  Val Acc: 61.19%,  Time: 0:31:27 \n","Epoch [3/100]\n","Iter:    600,  Train Loss:  0.74,  Train Acc: 75.00%,  Val Loss:  0.79,  Val Acc: 61.79%,  Time: 0:34:15 \n","Iter:    650,  Train Loss:  0.51,  Train Acc: 87.50%,  Val Loss:  0.83,  Val Acc: 61.36%,  Time: 0:37:05 \n","Iter:    700,  Train Loss:  0.77,  Train Acc: 62.50%,  Val Loss:  0.86,  Val Acc: 62.64%,  Time: 0:39:55 \n","Iter:    750,  Train Loss:  0.79,  Train Acc: 75.00%,  Val Loss:  0.85,  Val Acc: 62.72%,  Time: 0:42:45 \n","Iter:    800,  Train Loss:  0.87,  Train Acc: 62.50%,  Val Loss:  0.83,  Val Acc: 60.68%,  Time: 0:45:34 \n","Iter:    850,  Train Loss:  0.72,  Train Acc: 75.00%,  Val Loss:  0.83,  Val Acc: 61.02%,  Time: 0:48:24 \n","Epoch [4/100]\n","Iter:    900,  Train Loss:  0.81,  Train Acc: 62.50%,  Val Loss:  0.85,  Val Acc: 61.02%,  Time: 0:51:12 \n","Iter:    950,  Train Loss:  0.69,  Train Acc: 75.00%,  Val Loss:  0.84,  Val Acc: 62.55%,  Time: 0:54:02 \n","Iter:   1000,  Train Loss:  0.18,  Train Acc: 100.00%,  Val Loss:  0.91,  Val Acc: 62.72%,  Time: 0:56:52 \n","No optimization for a long time, auto-stopping...\n","Loading data...\n","Time usage: 0:00:02\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at /content/drive/MyDrive/Prediction/pretrained_BERT were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/100]\n","Iter:      0,  Train Loss:   1.2,  Train Acc: 25.00%,  Val Loss:   1.1,  Val Acc: 33.62%,  Time: 0:00:07 *\n","Iter:     50,  Train Loss:  0.92,  Train Acc: 50.00%,  Val Loss:  0.95,  Val Acc: 52.00%,  Time: 0:02:59 *\n","Iter:    100,  Train Loss:   1.1,  Train Acc: 50.00%,  Val Loss:  0.92,  Val Acc: 55.23%,  Time: 0:05:49 *\n","Iter:    150,  Train Loss:  0.82,  Train Acc: 62.50%,  Val Loss:  0.88,  Val Acc: 57.45%,  Time: 0:08:42 *\n","Iter:    200,  Train Loss:  0.75,  Train Acc: 68.75%,  Val Loss:  0.86,  Val Acc: 57.53%,  Time: 0:11:32 *\n","Iter:    250,  Train Loss:   1.2,  Train Acc: 50.00%,  Val Loss:  0.85,  Val Acc: 57.87%,  Time: 0:14:23 *\n","Epoch [2/100]\n","Iter:    300,  Train Loss:  0.68,  Train Acc: 56.25%,  Val Loss:  0.84,  Val Acc: 59.83%,  Time: 0:17:13 *\n","Iter:    350,  Train Loss:  0.93,  Train Acc: 68.75%,  Val Loss:  0.83,  Val Acc: 61.87%,  Time: 0:20:04 *\n","Iter:    400,  Train Loss:  0.75,  Train Acc: 56.25%,  Val Loss:  0.86,  Val Acc: 56.51%,  Time: 0:22:53 \n","Iter:    450,  Train Loss:  0.72,  Train Acc: 62.50%,  Val Loss:  0.79,  Val Acc: 62.89%,  Time: 0:25:44 *\n","Iter:    500,  Train Loss:  0.93,  Train Acc: 56.25%,  Val Loss:  0.78,  Val Acc: 62.47%,  Time: 0:28:35 *\n","Iter:    550,  Train Loss:  0.93,  Train Acc: 50.00%,  Val Loss:  0.77,  Val Acc: 62.72%,  Time: 0:31:26 *\n","Epoch [3/100]\n","Iter:    600,  Train Loss:  0.65,  Train Acc: 81.25%,  Val Loss:  0.81,  Val Acc: 60.85%,  Time: 0:34:15 \n","Iter:    650,  Train Loss:  0.56,  Train Acc: 68.75%,  Val Loss:   0.8,  Val Acc: 61.70%,  Time: 0:37:04 \n","Iter:    700,  Train Loss:  0.56,  Train Acc: 75.00%,  Val Loss:  0.79,  Val Acc: 61.45%,  Time: 0:39:54 \n","Iter:    750,  Train Loss:  0.46,  Train Acc: 81.25%,  Val Loss:  0.81,  Val Acc: 62.72%,  Time: 0:42:44 \n","Iter:    800,  Train Loss:  0.74,  Train Acc: 68.75%,  Val Loss:   0.8,  Val Acc: 64.34%,  Time: 0:45:34 \n","Iter:    850,  Train Loss:  0.91,  Train Acc: 68.75%,  Val Loss:  0.77,  Val Acc: 62.30%,  Time: 0:48:24 *\n","Epoch [4/100]\n","Iter:    900,  Train Loss:  0.74,  Train Acc: 68.75%,  Val Loss:  0.83,  Val Acc: 63.15%,  Time: 0:51:13 \n","Iter:    950,  Train Loss:  0.34,  Train Acc: 93.75%,  Val Loss:  0.89,  Val Acc: 60.68%,  Time: 0:54:03 \n","Iter:   1000,  Train Loss:  0.57,  Train Acc: 68.75%,  Val Loss:  0.84,  Val Acc: 63.15%,  Time: 0:56:52 \n","Iter:   1050,  Train Loss:  0.34,  Train Acc: 93.75%,  Val Loss:  0.93,  Val Acc: 63.23%,  Time: 0:59:42 \n","Iter:   1100,  Train Loss:  0.68,  Train Acc: 75.00%,  Val Loss:  0.86,  Val Acc: 64.43%,  Time: 1:02:32 \n","Iter:   1150,  Train Loss:   0.6,  Train Acc: 75.00%,  Val Loss:  0.89,  Val Acc: 61.53%,  Time: 1:05:22 \n","Epoch [5/100]\n","Iter:   1200,  Train Loss:  0.51,  Train Acc: 87.50%,  Val Loss:   1.0,  Val Acc: 61.19%,  Time: 1:08:10 \n","Iter:   1250,  Train Loss:  0.27,  Train Acc: 93.75%,  Val Loss:   1.0,  Val Acc: 58.55%,  Time: 1:11:00 \n","Iter:   1300,  Train Loss:   0.8,  Train Acc: 68.75%,  Val Loss:  0.94,  Val Acc: 62.64%,  Time: 1:13:49 \n","Iter:   1350,  Train Loss:  0.34,  Train Acc: 81.25%,  Val Loss:   1.0,  Val Acc: 63.74%,  Time: 1:16:39 \n","No optimization for a long time, auto-stopping...\n","Loading data...\n","Time usage: 0:00:02\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at /content/drive/MyDrive/Prediction/pretrained_BERT were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/100]\n","Iter:      0,  Train Loss:   1.2,  Train Acc: 25.00%,  Val Loss:   1.1,  Val Acc: 34.47%,  Time: 0:00:08 *\n","Iter:     50,  Train Loss:  0.98,  Train Acc: 37.50%,  Val Loss:  0.94,  Val Acc: 46.47%,  Time: 0:02:59 *\n","Iter:    100,  Train Loss:  0.95,  Train Acc: 56.25%,  Val Loss:  0.93,  Val Acc: 52.43%,  Time: 0:05:51 *\n","Iter:    150,  Train Loss:  0.94,  Train Acc: 43.75%,  Val Loss:  0.89,  Val Acc: 57.79%,  Time: 0:08:42 *\n","Iter:    200,  Train Loss:  0.84,  Train Acc: 56.25%,  Val Loss:  0.85,  Val Acc: 59.15%,  Time: 0:11:34 *\n","Iter:    250,  Train Loss:  0.78,  Train Acc: 56.25%,  Val Loss:  0.83,  Val Acc: 59.23%,  Time: 0:14:25 *\n","Epoch [2/100]\n","Iter:    300,  Train Loss:  0.79,  Train Acc: 50.00%,  Val Loss:   0.8,  Val Acc: 62.55%,  Time: 0:17:14 *\n","Iter:    350,  Train Loss:  0.65,  Train Acc: 62.50%,  Val Loss:  0.81,  Val Acc: 60.68%,  Time: 0:20:04 \n","Iter:    400,  Train Loss:  0.95,  Train Acc: 56.25%,  Val Loss:  0.79,  Val Acc: 63.06%,  Time: 0:22:55 *\n","Iter:    450,  Train Loss:  0.88,  Train Acc: 62.50%,  Val Loss:  0.78,  Val Acc: 64.77%,  Time: 0:25:46 *\n","Iter:    500,  Train Loss:  0.65,  Train Acc: 62.50%,  Val Loss:  0.77,  Val Acc: 63.91%,  Time: 0:28:37 *\n","Iter:    550,  Train Loss:  0.77,  Train Acc: 75.00%,  Val Loss:  0.77,  Val Acc: 64.51%,  Time: 0:31:27 \n","Epoch [3/100]\n","Iter:    600,  Train Loss:  0.55,  Train Acc: 81.25%,  Val Loss:  0.81,  Val Acc: 62.72%,  Time: 0:34:15 \n","Iter:    650,  Train Loss:   1.5,  Train Acc: 50.00%,  Val Loss:  0.76,  Val Acc: 63.74%,  Time: 0:37:06 *\n","Iter:    700,  Train Loss:  0.53,  Train Acc: 75.00%,  Val Loss:  0.76,  Val Acc: 63.83%,  Time: 0:39:56 \n","Iter:    750,  Train Loss:  0.61,  Train Acc: 75.00%,  Val Loss:  0.76,  Val Acc: 65.70%,  Time: 0:42:45 \n","Iter:    800,  Train Loss:  0.47,  Train Acc: 75.00%,  Val Loss:  0.75,  Val Acc: 66.72%,  Time: 0:45:36 *\n","Iter:    850,  Train Loss:  0.61,  Train Acc: 81.25%,  Val Loss:  0.77,  Val Acc: 66.98%,  Time: 0:48:26 \n","Epoch [4/100]\n","Iter:    900,  Train Loss:  0.82,  Train Acc: 56.25%,  Val Loss:  0.88,  Val Acc: 62.21%,  Time: 0:51:14 \n","Iter:    950,  Train Loss:  0.52,  Train Acc: 68.75%,  Val Loss:  0.77,  Val Acc: 65.87%,  Time: 0:54:04 \n","Iter:   1000,  Train Loss:   0.5,  Train Acc: 75.00%,  Val Loss:  0.76,  Val Acc: 66.04%,  Time: 0:56:54 \n","Iter:   1050,  Train Loss:  0.39,  Train Acc: 87.50%,  Val Loss:  0.78,  Val Acc: 66.98%,  Time: 0:59:43 \n","Iter:   1100,  Train Loss:  0.37,  Train Acc: 87.50%,  Val Loss:  0.84,  Val Acc: 62.64%,  Time: 1:02:33 \n","Iter:   1150,  Train Loss:   0.4,  Train Acc: 87.50%,  Val Loss:  0.79,  Val Acc: 66.04%,  Time: 1:05:23 \n","Epoch [5/100]\n","Iter:   1200,  Train Loss:  0.54,  Train Acc: 62.50%,  Val Loss:  0.95,  Val Acc: 62.81%,  Time: 1:08:11 \n","Iter:   1250,  Train Loss:  0.61,  Train Acc: 68.75%,  Val Loss:  0.81,  Val Acc: 65.36%,  Time: 1:11:01 \n","Iter:   1300,  Train Loss:  0.63,  Train Acc: 68.75%,  Val Loss:  0.82,  Val Acc: 65.19%,  Time: 1:13:51 \n","No optimization for a long time, auto-stopping...\n","Loading data...\n","Time usage: 0:00:02\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at /content/drive/MyDrive/Prediction/pretrained_BERT were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/100]\n","Iter:      0,  Train Loss:   1.1,  Train Acc: 12.50%,  Val Loss:   1.1,  Val Acc: 41.28%,  Time: 0:00:08 *\n","Iter:     50,  Train Loss:  0.75,  Train Acc: 75.00%,  Val Loss:  0.95,  Val Acc: 51.06%,  Time: 0:02:59 *\n","Iter:    100,  Train Loss:  0.72,  Train Acc: 81.25%,  Val Loss:  0.92,  Val Acc: 52.60%,  Time: 0:05:50 *\n","Iter:    150,  Train Loss:   1.1,  Train Acc: 50.00%,  Val Loss:  0.87,  Val Acc: 56.26%,  Time: 0:08:42 *\n","Iter:    200,  Train Loss:  0.72,  Train Acc: 56.25%,  Val Loss:  0.86,  Val Acc: 55.74%,  Time: 0:11:33 *\n","Iter:    250,  Train Loss:  0.62,  Train Acc: 68.75%,  Val Loss:  0.83,  Val Acc: 60.26%,  Time: 0:14:24 *\n","Epoch [2/100]\n","Iter:    300,  Train Loss:  0.83,  Train Acc: 56.25%,  Val Loss:  0.83,  Val Acc: 58.81%,  Time: 0:17:12 \n","Iter:    350,  Train Loss:  0.76,  Train Acc: 68.75%,  Val Loss:  0.83,  Val Acc: 61.11%,  Time: 0:20:03 *\n","Iter:    400,  Train Loss:  0.66,  Train Acc: 75.00%,  Val Loss:  0.84,  Val Acc: 59.74%,  Time: 0:22:53 \n","Iter:    450,  Train Loss:  0.74,  Train Acc: 68.75%,  Val Loss:  0.88,  Val Acc: 57.11%,  Time: 0:25:42 \n","Iter:    500,  Train Loss:  0.59,  Train Acc: 81.25%,  Val Loss:   0.8,  Val Acc: 62.04%,  Time: 0:28:33 *\n","Iter:    550,  Train Loss:   1.1,  Train Acc: 62.50%,  Val Loss:  0.81,  Val Acc: 61.28%,  Time: 0:31:23 \n","Epoch [3/100]\n","Iter:    600,  Train Loss:  0.65,  Train Acc: 62.50%,  Val Loss:  0.81,  Val Acc: 61.36%,  Time: 0:34:11 \n","Iter:    650,  Train Loss:   0.7,  Train Acc: 68.75%,  Val Loss:  0.82,  Val Acc: 63.15%,  Time: 0:37:01 \n","Iter:    700,  Train Loss:  0.45,  Train Acc: 75.00%,  Val Loss:  0.85,  Val Acc: 61.28%,  Time: 0:39:51 \n","Iter:    750,  Train Loss:  0.59,  Train Acc: 68.75%,  Val Loss:  0.83,  Val Acc: 62.30%,  Time: 0:42:40 \n","Iter:    800,  Train Loss:  0.66,  Train Acc: 56.25%,  Val Loss:  0.84,  Val Acc: 63.40%,  Time: 0:45:30 \n","Iter:    850,  Train Loss:  0.62,  Train Acc: 75.00%,  Val Loss:  0.88,  Val Acc: 61.02%,  Time: 0:48:20 \n","Epoch [4/100]\n","Iter:    900,  Train Loss:  0.37,  Train Acc: 87.50%,  Val Loss:  0.84,  Val Acc: 61.45%,  Time: 0:51:08 \n","Iter:    950,  Train Loss:  0.39,  Train Acc: 87.50%,  Val Loss:  0.86,  Val Acc: 64.85%,  Time: 0:53:58 \n","Iter:   1000,  Train Loss:  0.51,  Train Acc: 75.00%,  Val Loss:  0.89,  Val Acc: 65.45%,  Time: 0:56:48 \n","No optimization for a long time, auto-stopping...\n","Loading data...\n","Time usage: 0:00:02\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at /content/drive/MyDrive/Prediction/pretrained_BERT were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/100]\n","Iter:      0,  Train Loss:   1.3,  Train Acc:  6.25%,  Val Loss:   1.1,  Val Acc: 36.51%,  Time: 0:00:08 *\n","Iter:     50,  Train Loss:  0.98,  Train Acc: 50.00%,  Val Loss:  0.92,  Val Acc: 53.02%,  Time: 0:03:00 *\n","Iter:    100,  Train Loss:   1.1,  Train Acc: 50.00%,  Val Loss:  0.89,  Val Acc: 55.32%,  Time: 0:05:52 *\n","Iter:    150,  Train Loss:  0.99,  Train Acc: 50.00%,  Val Loss:  0.87,  Val Acc: 56.60%,  Time: 0:08:42 *\n","Iter:    200,  Train Loss:  0.84,  Train Acc: 37.50%,  Val Loss:  0.87,  Val Acc: 57.11%,  Time: 0:11:32 \n","Iter:    250,  Train Loss:  0.96,  Train Acc: 43.75%,  Val Loss:  0.85,  Val Acc: 57.96%,  Time: 0:14:23 *\n","Epoch [2/100]\n","Iter:    300,  Train Loss:  0.77,  Train Acc: 68.75%,  Val Loss:  0.82,  Val Acc: 63.06%,  Time: 0:17:13 *\n","Iter:    350,  Train Loss:  0.76,  Train Acc: 56.25%,  Val Loss:   0.9,  Val Acc: 56.60%,  Time: 0:20:02 \n","Iter:    400,  Train Loss:  0.63,  Train Acc: 81.25%,  Val Loss:   0.8,  Val Acc: 60.09%,  Time: 0:22:53 *\n","Iter:    450,  Train Loss:  0.95,  Train Acc: 56.25%,  Val Loss:   0.8,  Val Acc: 61.28%,  Time: 0:25:44 *\n","Iter:    500,  Train Loss:  0.87,  Train Acc: 50.00%,  Val Loss:  0.88,  Val Acc: 57.02%,  Time: 0:28:34 \n","Iter:    550,  Train Loss:  0.67,  Train Acc: 62.50%,  Val Loss:  0.78,  Val Acc: 62.30%,  Time: 0:31:25 *\n","Epoch [3/100]\n","Iter:    600,  Train Loss:  0.68,  Train Acc: 75.00%,  Val Loss:  0.77,  Val Acc: 64.77%,  Time: 0:34:14 *\n","Iter:    650,  Train Loss:  0.54,  Train Acc: 75.00%,  Val Loss:  0.79,  Val Acc: 62.89%,  Time: 0:37:04 \n","Iter:    700,  Train Loss:  0.67,  Train Acc: 81.25%,  Val Loss:   0.8,  Val Acc: 64.00%,  Time: 0:39:54 \n","Iter:    750,  Train Loss:  0.59,  Train Acc: 75.00%,  Val Loss:  0.86,  Val Acc: 59.83%,  Time: 0:42:43 \n","Iter:    800,  Train Loss:  0.93,  Train Acc: 62.50%,  Val Loss:  0.86,  Val Acc: 60.85%,  Time: 0:45:33 \n","Iter:    850,  Train Loss:  0.59,  Train Acc: 62.50%,  Val Loss:  0.81,  Val Acc: 65.36%,  Time: 0:48:23 \n","Epoch [4/100]\n","Iter:    900,  Train Loss:  0.34,  Train Acc: 87.50%,  Val Loss:  0.78,  Val Acc: 63.06%,  Time: 0:51:11 \n","Iter:    950,  Train Loss:  0.28,  Train Acc: 93.75%,  Val Loss:  0.82,  Val Acc: 64.34%,  Time: 0:54:01 \n","Iter:   1000,  Train Loss:  0.45,  Train Acc: 87.50%,  Val Loss:  0.82,  Val Acc: 64.94%,  Time: 0:56:51 \n","Iter:   1050,  Train Loss:  0.35,  Train Acc: 81.25%,  Val Loss:  0.89,  Val Acc: 63.15%,  Time: 0:59:40 \n","Iter:   1100,  Train Loss:  0.81,  Train Acc: 62.50%,  Val Loss:  0.92,  Val Acc: 63.40%,  Time: 1:02:30 \n","No optimization for a long time, auto-stopping...\n","f: 0.6164336093862792+0.02002870716340793\n","f1: 0.611236448868665+0.03606183850726213\n","f2: 0.6168426067452059+0.028469784283964038\n","f3: 0.489966608460469+0.06632069375118999\n"]}],"source":["f=list()\n","f1=list()\n","f2=list()\n","f3=list()\n","\n","for i in range(5):\n","    dataset='/content/drive/MyDrive/Prediction'\n","    df = pd.concat([df2,df_info])\n","    df = df.sample(frac=1, random_state=i)   \n","    cut_idx1 = int(round(0.8 * df.shape[0]))\n","    df_train,df_test = df[:cut_idx1], df[cut_idx1:]\n","    config = BERT_CNN_Config(dataset,df_train,df_test)\n","    np.random.seed(1)\n","    torch.manual_seed(1)\n","    torch.cuda.manual_seed_all(1)\n","    torch.backends.cudnn.deterministic = True  # 保证每次结果一样\n","    start_time = time.time()\n","    print(\"Loading data...\")\n","    train_data, test_data = build_dataset(config)\n","    train_iter = build_iterator(train_data, config,)\n","    dev_iter = build_iterator(test_data, config,)\n","    test_iter = build_iterator(test_data, config)\n","    time_dif = get_time_dif(start_time)\n","    print(\"Time usage:\", time_dif)\n","    model = BERT_CNN(config).to(config.device)\n","    train(config, model, train_iter, dev_iter, test_iter)\n","    df_test,f_score = test(config, model, test_iter, df_test)\n","    f.append(f_score)\n","    f1.append(get_depth_f1(df_test,1))\n","    f2.append(get_depth_f1(df_test,2))\n","    f3.append(get_depth_f1(df_test,3))\n","\n","\n","print(\"f: \"+str(np.mean(f))+\"+\"+str(np.std(f, ddof = 1)))\n","print(\"f1: \"+str(np.mean(f1))+\"+\"+str(np.std(f1, ddof = 1)))\n","print(\"f2: \"+str(np.mean(f2))+\"+\"+str(np.std(f2, ddof = 1)))\n","print(\"f3: \"+str(np.mean(f3))+\"+\"+str(np.std(f3, ddof = 1)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1170408,"status":"ok","timestamp":1648990096055,"user":{"displayName":"刘耘果","userId":"11745284739417236882"},"user_tz":-480},"id":"hEFd-t_qJXYV","outputId":"52180670-862b-4ccf-cfe3-cb5039141d73"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading data...\n","Time usage: 0:00:02\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/drive/MyDrive/Prediction/pretrained_BERT/Chinese were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/100]\n","Iter:      0,  Train Loss:  0.97,  Train Acc: 56.25%,  Val Loss:   1.0,  Val Acc: 45.45%,  Time: 0:00:04 *\n","Iter:     50,  Train Loss:   1.0,  Train Acc: 37.50%,  Val Loss:  0.96,  Val Acc: 50.55%,  Time: 0:00:14 *\n","Iter:    100,  Train Loss:   1.0,  Train Acc: 43.75%,  Val Loss:  0.93,  Val Acc: 52.34%,  Time: 0:00:24 *\n","Iter:    150,  Train Loss:  0.74,  Train Acc: 56.25%,  Val Loss:   0.9,  Val Acc: 54.30%,  Time: 0:00:34 *\n","Iter:    200,  Train Loss:   1.0,  Train Acc: 50.00%,  Val Loss:  0.89,  Val Acc: 56.77%,  Time: 0:00:44 *\n","Iter:    250,  Train Loss:   0.8,  Train Acc: 56.25%,  Val Loss:  0.85,  Val Acc: 59.49%,  Time: 0:00:54 *\n","Epoch [2/100]\n","Iter:    300,  Train Loss:  0.99,  Train Acc: 43.75%,  Val Loss:  0.82,  Val Acc: 60.85%,  Time: 0:01:03 *\n","Iter:    350,  Train Loss:  0.79,  Train Acc: 50.00%,  Val Loss:  0.83,  Val Acc: 60.00%,  Time: 0:01:12 \n","Iter:    400,  Train Loss:   1.0,  Train Acc: 50.00%,  Val Loss:  0.84,  Val Acc: 59.49%,  Time: 0:01:20 \n","Iter:    450,  Train Loss:   0.7,  Train Acc: 62.50%,  Val Loss:  0.79,  Val Acc: 62.38%,  Time: 0:01:30 *\n","Iter:    500,  Train Loss:  0.78,  Train Acc: 62.50%,  Val Loss:  0.79,  Val Acc: 62.72%,  Time: 0:01:38 \n","Iter:    550,  Train Loss:   1.1,  Train Acc: 56.25%,  Val Loss:  0.79,  Val Acc: 62.55%,  Time: 0:01:47 \n","Epoch [3/100]\n","Iter:    600,  Train Loss:  0.71,  Train Acc: 62.50%,  Val Loss:  0.78,  Val Acc: 62.72%,  Time: 0:01:56 *\n","Iter:    650,  Train Loss:  0.68,  Train Acc: 68.75%,  Val Loss:  0.83,  Val Acc: 61.79%,  Time: 0:02:05 \n","Iter:    700,  Train Loss:  0.93,  Train Acc: 62.50%,  Val Loss:  0.86,  Val Acc: 60.51%,  Time: 0:02:13 \n","Iter:    750,  Train Loss:  0.78,  Train Acc: 75.00%,  Val Loss:   0.8,  Val Acc: 63.06%,  Time: 0:02:21 \n","Iter:    800,  Train Loss:  0.98,  Train Acc: 68.75%,  Val Loss:  0.78,  Val Acc: 62.72%,  Time: 0:02:30 \n","Iter:    850,  Train Loss:  0.92,  Train Acc: 62.50%,  Val Loss:  0.82,  Val Acc: 63.06%,  Time: 0:02:38 \n","Epoch [4/100]\n","Iter:    900,  Train Loss:  0.98,  Train Acc: 50.00%,  Val Loss:  0.82,  Val Acc: 62.55%,  Time: 0:02:47 \n","Iter:    950,  Train Loss:  0.66,  Train Acc: 68.75%,  Val Loss:  0.83,  Val Acc: 62.30%,  Time: 0:02:55 \n","Iter:   1000,  Train Loss:  0.32,  Train Acc: 100.00%,  Val Loss:  0.84,  Val Acc: 63.40%,  Time: 0:03:03 \n","Iter:   1050,  Train Loss:  0.61,  Train Acc: 81.25%,  Val Loss:  0.86,  Val Acc: 62.89%,  Time: 0:03:12 \n","Iter:   1100,  Train Loss:  0.56,  Train Acc: 68.75%,  Val Loss:  0.84,  Val Acc: 63.06%,  Time: 0:03:20 \n","No optimization for a long time, auto-stopping...\n","Loading data...\n","Time usage: 0:00:03\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/drive/MyDrive/Prediction/pretrained_BERT/Chinese were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/100]\n","Iter:      0,  Train Loss:   1.2,  Train Acc: 18.75%,  Val Loss:   1.1,  Val Acc: 42.30%,  Time: 0:00:04 *\n","Iter:     50,  Train Loss:   1.1,  Train Acc: 43.75%,  Val Loss:  0.97,  Val Acc: 49.02%,  Time: 0:00:14 *\n","Iter:    100,  Train Loss:  0.97,  Train Acc: 56.25%,  Val Loss:  0.92,  Val Acc: 52.60%,  Time: 0:00:24 *\n","Iter:    150,  Train Loss:  0.76,  Train Acc: 43.75%,  Val Loss:   0.9,  Val Acc: 56.26%,  Time: 0:00:34 *\n","Iter:    200,  Train Loss:  0.82,  Train Acc: 62.50%,  Val Loss:  0.89,  Val Acc: 56.26%,  Time: 0:00:44 *\n","Iter:    250,  Train Loss:   1.0,  Train Acc: 62.50%,  Val Loss:  0.87,  Val Acc: 55.74%,  Time: 0:00:54 *\n","Epoch [2/100]\n","Iter:    300,  Train Loss:  0.65,  Train Acc: 81.25%,  Val Loss:  0.84,  Val Acc: 57.53%,  Time: 0:01:04 *\n","Iter:    350,  Train Loss:  0.99,  Train Acc: 62.50%,  Val Loss:  0.83,  Val Acc: 59.15%,  Time: 0:01:14 *\n","Iter:    400,  Train Loss:  0.92,  Train Acc: 50.00%,  Val Loss:  0.89,  Val Acc: 54.04%,  Time: 0:01:22 \n","Iter:    450,  Train Loss:  0.61,  Train Acc: 75.00%,  Val Loss:  0.81,  Val Acc: 61.79%,  Time: 0:01:32 *\n","Iter:    500,  Train Loss:  0.92,  Train Acc: 62.50%,  Val Loss:  0.81,  Val Acc: 62.64%,  Time: 0:01:41 *\n","Iter:    550,  Train Loss:  0.94,  Train Acc: 50.00%,  Val Loss:  0.79,  Val Acc: 62.47%,  Time: 0:01:52 *\n","Epoch [3/100]\n","Iter:    600,  Train Loss:  0.57,  Train Acc: 87.50%,  Val Loss:  0.81,  Val Acc: 60.43%,  Time: 0:02:00 \n","Iter:    650,  Train Loss:  0.71,  Train Acc: 56.25%,  Val Loss:  0.79,  Val Acc: 61.96%,  Time: 0:02:09 \n","Iter:    700,  Train Loss:  0.53,  Train Acc: 75.00%,  Val Loss:  0.81,  Val Acc: 61.87%,  Time: 0:02:17 \n","Iter:    750,  Train Loss:  0.69,  Train Acc: 56.25%,  Val Loss:   0.8,  Val Acc: 64.60%,  Time: 0:02:26 \n","Iter:    800,  Train Loss:  0.53,  Train Acc: 81.25%,  Val Loss:  0.78,  Val Acc: 64.60%,  Time: 0:02:35 *\n","Iter:    850,  Train Loss:   1.1,  Train Acc: 50.00%,  Val Loss:  0.83,  Val Acc: 60.51%,  Time: 0:02:44 \n","Epoch [4/100]\n","Iter:    900,  Train Loss:  0.74,  Train Acc: 56.25%,  Val Loss:  0.84,  Val Acc: 61.19%,  Time: 0:02:52 \n","Iter:    950,  Train Loss:  0.35,  Train Acc: 87.50%,  Val Loss:   0.9,  Val Acc: 59.57%,  Time: 0:03:00 \n","Iter:   1000,  Train Loss:  0.53,  Train Acc: 81.25%,  Val Loss:  0.81,  Val Acc: 65.28%,  Time: 0:03:09 \n","Iter:   1050,  Train Loss:  0.44,  Train Acc: 81.25%,  Val Loss:  0.81,  Val Acc: 65.45%,  Time: 0:03:17 \n","Iter:   1100,  Train Loss:  0.72,  Train Acc: 56.25%,  Val Loss:  0.81,  Val Acc: 64.00%,  Time: 0:03:26 \n","Iter:   1150,  Train Loss:  0.61,  Train Acc: 68.75%,  Val Loss:  0.88,  Val Acc: 61.79%,  Time: 0:03:34 \n","Epoch [5/100]\n","Iter:   1200,  Train Loss:  0.45,  Train Acc: 87.50%,  Val Loss:  0.88,  Val Acc: 61.53%,  Time: 0:03:42 \n","Iter:   1250,  Train Loss:  0.39,  Train Acc: 93.75%,  Val Loss:  0.93,  Val Acc: 60.85%,  Time: 0:03:51 \n","Iter:   1300,  Train Loss:   0.4,  Train Acc: 87.50%,  Val Loss:   0.9,  Val Acc: 63.40%,  Time: 0:03:59 \n","No optimization for a long time, auto-stopping...\n","Loading data...\n","Time usage: 0:00:02\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/drive/MyDrive/Prediction/pretrained_BERT/Chinese were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/100]\n","Iter:      0,  Train Loss:   1.2,  Train Acc: 31.25%,  Val Loss:   1.0,  Val Acc: 46.55%,  Time: 0:00:04 *\n","Iter:     50,  Train Loss:   1.0,  Train Acc: 25.00%,  Val Loss:  0.96,  Val Acc: 44.85%,  Time: 0:00:15 *\n","Iter:    100,  Train Loss:  0.87,  Train Acc: 43.75%,  Val Loss:  0.92,  Val Acc: 52.17%,  Time: 0:00:24 *\n","Iter:    150,  Train Loss:   1.1,  Train Acc: 43.75%,  Val Loss:  0.87,  Val Acc: 54.81%,  Time: 0:00:34 *\n","Iter:    200,  Train Loss:  0.93,  Train Acc: 43.75%,  Val Loss:  0.88,  Val Acc: 54.89%,  Time: 0:00:42 \n","Iter:    250,  Train Loss:  0.69,  Train Acc: 68.75%,  Val Loss:  0.86,  Val Acc: 54.30%,  Time: 0:00:53 *\n","Epoch [2/100]\n","Iter:    300,  Train Loss:   1.1,  Train Acc: 25.00%,  Val Loss:  0.83,  Val Acc: 58.89%,  Time: 0:01:03 *\n","Iter:    350,  Train Loss:  0.68,  Train Acc: 62.50%,  Val Loss:  0.83,  Val Acc: 59.32%,  Time: 0:01:12 *\n","Iter:    400,  Train Loss:   1.1,  Train Acc: 37.50%,  Val Loss:  0.85,  Val Acc: 57.70%,  Time: 0:01:21 \n","Iter:    450,  Train Loss:   1.0,  Train Acc: 43.75%,  Val Loss:  0.85,  Val Acc: 59.57%,  Time: 0:01:29 \n","Iter:    500,  Train Loss:  0.72,  Train Acc: 68.75%,  Val Loss:   0.8,  Val Acc: 59.91%,  Time: 0:01:39 *\n","Iter:    550,  Train Loss:  0.97,  Train Acc: 56.25%,  Val Loss:  0.79,  Val Acc: 62.21%,  Time: 0:01:49 *\n","Epoch [3/100]\n","Iter:    600,  Train Loss:  0.69,  Train Acc: 68.75%,  Val Loss:  0.82,  Val Acc: 60.34%,  Time: 0:01:57 \n","Iter:    650,  Train Loss:   1.6,  Train Acc: 50.00%,  Val Loss:  0.78,  Val Acc: 63.49%,  Time: 0:02:07 *\n","Iter:    700,  Train Loss:  0.72,  Train Acc: 75.00%,  Val Loss:  0.79,  Val Acc: 61.28%,  Time: 0:02:15 \n","Iter:    750,  Train Loss:  0.88,  Train Acc: 62.50%,  Val Loss:  0.78,  Val Acc: 63.49%,  Time: 0:02:25 *\n","Iter:    800,  Train Loss:  0.51,  Train Acc: 81.25%,  Val Loss:  0.76,  Val Acc: 63.66%,  Time: 0:02:35 *\n","Iter:    850,  Train Loss:  0.62,  Train Acc: 68.75%,  Val Loss:  0.77,  Val Acc: 63.74%,  Time: 0:02:43 \n","Epoch [4/100]\n","Iter:    900,  Train Loss:   1.3,  Train Acc: 43.75%,  Val Loss:  0.82,  Val Acc: 62.30%,  Time: 0:02:52 \n","Iter:    950,  Train Loss:  0.69,  Train Acc: 68.75%,  Val Loss:  0.83,  Val Acc: 62.21%,  Time: 0:03:00 \n","Iter:   1000,  Train Loss:  0.64,  Train Acc: 75.00%,  Val Loss:  0.78,  Val Acc: 64.43%,  Time: 0:03:08 \n","Iter:   1050,  Train Loss:   0.6,  Train Acc: 68.75%,  Val Loss:  0.77,  Val Acc: 65.87%,  Time: 0:03:17 \n","Iter:   1100,  Train Loss:  0.56,  Train Acc: 75.00%,  Val Loss:  0.78,  Val Acc: 64.17%,  Time: 0:03:25 \n","Iter:   1150,  Train Loss:  0.52,  Train Acc: 68.75%,  Val Loss:  0.79,  Val Acc: 64.34%,  Time: 0:03:34 \n","Epoch [5/100]\n","Iter:   1200,  Train Loss:  0.82,  Train Acc: 75.00%,  Val Loss:  0.93,  Val Acc: 61.19%,  Time: 0:03:42 \n","Iter:   1250,  Train Loss:  0.96,  Train Acc: 68.75%,  Val Loss:   0.8,  Val Acc: 63.66%,  Time: 0:03:50 \n","Iter:   1300,  Train Loss:  0.76,  Train Acc: 68.75%,  Val Loss:  0.94,  Val Acc: 60.85%,  Time: 0:03:59 \n","No optimization for a long time, auto-stopping...\n","Loading data...\n","Time usage: 0:00:02\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/drive/MyDrive/Prediction/pretrained_BERT/Chinese were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/100]\n","Iter:      0,  Train Loss:   1.1,  Train Acc: 43.75%,  Val Loss:   1.0,  Val Acc: 50.13%,  Time: 0:00:04 *\n","Iter:     50,  Train Loss:  0.89,  Train Acc: 62.50%,  Val Loss:  0.96,  Val Acc: 49.02%,  Time: 0:00:14 *\n","Iter:    100,  Train Loss:   1.1,  Train Acc: 25.00%,  Val Loss:  0.95,  Val Acc: 51.15%,  Time: 0:00:25 *\n","Iter:    150,  Train Loss:   1.3,  Train Acc: 31.25%,  Val Loss:  0.91,  Val Acc: 53.96%,  Time: 0:00:34 *\n","Iter:    200,  Train Loss:   1.0,  Train Acc: 50.00%,  Val Loss:  0.91,  Val Acc: 53.62%,  Time: 0:00:43 \n","Iter:    250,  Train Loss:  0.81,  Train Acc: 56.25%,  Val Loss:  0.89,  Val Acc: 54.98%,  Time: 0:00:52 *\n","Epoch [2/100]\n","Iter:    300,  Train Loss:  0.83,  Train Acc: 56.25%,  Val Loss:  0.85,  Val Acc: 59.15%,  Time: 0:01:02 *\n","Iter:    350,  Train Loss:  0.84,  Train Acc: 50.00%,  Val Loss:  0.92,  Val Acc: 55.23%,  Time: 0:01:10 \n","Iter:    400,  Train Loss:  0.85,  Train Acc: 56.25%,  Val Loss:  0.87,  Val Acc: 57.62%,  Time: 0:01:19 \n","Iter:    450,  Train Loss:  0.75,  Train Acc: 62.50%,  Val Loss:  0.89,  Val Acc: 56.94%,  Time: 0:01:27 \n","Iter:    500,  Train Loss:  0.82,  Train Acc: 56.25%,  Val Loss:  0.85,  Val Acc: 57.45%,  Time: 0:01:35 \n","Iter:    550,  Train Loss:  0.84,  Train Acc: 68.75%,  Val Loss:  0.84,  Val Acc: 58.81%,  Time: 0:01:45 *\n","Epoch [3/100]\n","Iter:    600,  Train Loss:  0.56,  Train Acc: 75.00%,  Val Loss:  0.81,  Val Acc: 61.36%,  Time: 0:01:56 *\n","Iter:    650,  Train Loss:  0.95,  Train Acc: 68.75%,  Val Loss:  0.81,  Val Acc: 62.21%,  Time: 0:02:06 *\n","Iter:    700,  Train Loss:  0.59,  Train Acc: 68.75%,  Val Loss:  0.87,  Val Acc: 58.30%,  Time: 0:02:14 \n","Iter:    750,  Train Loss:   0.7,  Train Acc: 62.50%,  Val Loss:  0.82,  Val Acc: 61.79%,  Time: 0:02:22 \n","Iter:    800,  Train Loss:  0.65,  Train Acc: 62.50%,  Val Loss:   0.8,  Val Acc: 63.32%,  Time: 0:02:32 *\n","Iter:    850,  Train Loss:   0.6,  Train Acc: 81.25%,  Val Loss:  0.83,  Val Acc: 62.13%,  Time: 0:02:40 \n","Epoch [4/100]\n","Iter:    900,  Train Loss:  0.49,  Train Acc: 87.50%,  Val Loss:   0.8,  Val Acc: 61.79%,  Time: 0:02:50 *\n","Iter:    950,  Train Loss:  0.46,  Train Acc: 87.50%,  Val Loss:  0.84,  Val Acc: 62.98%,  Time: 0:02:58 \n","Iter:   1000,  Train Loss:  0.73,  Train Acc: 56.25%,  Val Loss:  0.82,  Val Acc: 63.23%,  Time: 0:03:07 \n","Iter:   1050,  Train Loss:  0.71,  Train Acc: 56.25%,  Val Loss:  0.83,  Val Acc: 64.00%,  Time: 0:03:15 \n","Iter:   1100,  Train Loss:  0.53,  Train Acc: 75.00%,  Val Loss:  0.86,  Val Acc: 64.34%,  Time: 0:03:24 \n","Iter:   1150,  Train Loss:  0.61,  Train Acc: 75.00%,  Val Loss:  0.92,  Val Acc: 62.81%,  Time: 0:03:32 \n","Epoch [5/100]\n","Iter:   1200,  Train Loss:  0.44,  Train Acc: 81.25%,  Val Loss:  0.86,  Val Acc: 62.81%,  Time: 0:03:40 \n","Iter:   1250,  Train Loss:  0.45,  Train Acc: 81.25%,  Val Loss:  0.87,  Val Acc: 65.11%,  Time: 0:03:49 \n","Iter:   1300,  Train Loss:   0.7,  Train Acc: 68.75%,  Val Loss:  0.93,  Val Acc: 64.26%,  Time: 0:03:57 \n","Iter:   1350,  Train Loss:  0.47,  Train Acc: 75.00%,  Val Loss:  0.96,  Val Acc: 61.28%,  Time: 0:04:06 \n","Iter:   1400,  Train Loss:  0.53,  Train Acc: 81.25%,  Val Loss:  0.97,  Val Acc: 62.55%,  Time: 0:04:14 \n","No optimization for a long time, auto-stopping...\n","Loading data...\n","Time usage: 0:00:02\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/drive/MyDrive/Prediction/pretrained_BERT/Chinese were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/100]\n","Iter:      0,  Train Loss:   1.0,  Train Acc: 50.00%,  Val Loss:   1.0,  Val Acc: 44.68%,  Time: 0:00:04 *\n","Iter:     50,  Train Loss:   1.1,  Train Acc: 37.50%,  Val Loss:  0.93,  Val Acc: 50.72%,  Time: 0:00:14 *\n","Iter:    100,  Train Loss:   1.0,  Train Acc: 43.75%,  Val Loss:  0.91,  Val Acc: 50.04%,  Time: 0:00:24 *\n","Iter:    150,  Train Loss:   1.3,  Train Acc: 31.25%,  Val Loss:  0.91,  Val Acc: 53.36%,  Time: 0:00:33 *\n","Iter:    200,  Train Loss:  0.85,  Train Acc: 62.50%,  Val Loss:  0.87,  Val Acc: 56.00%,  Time: 0:00:43 *\n","Iter:    250,  Train Loss:   1.1,  Train Acc: 43.75%,  Val Loss:  0.88,  Val Acc: 55.91%,  Time: 0:00:52 \n","Epoch [2/100]\n","Iter:    300,  Train Loss:  0.93,  Train Acc: 62.50%,  Val Loss:  0.84,  Val Acc: 58.98%,  Time: 0:01:01 *\n","Iter:    350,  Train Loss:  0.86,  Train Acc: 62.50%,  Val Loss:  0.93,  Val Acc: 53.11%,  Time: 0:01:09 \n","Iter:    400,  Train Loss:  0.71,  Train Acc: 87.50%,  Val Loss:  0.84,  Val Acc: 57.02%,  Time: 0:01:18 \n","Iter:    450,  Train Loss:  0.64,  Train Acc: 68.75%,  Val Loss:  0.82,  Val Acc: 60.51%,  Time: 0:01:27 *\n","Iter:    500,  Train Loss:  0.95,  Train Acc: 43.75%,  Val Loss:   0.9,  Val Acc: 56.00%,  Time: 0:01:36 \n","Iter:    550,  Train Loss:  0.79,  Train Acc: 56.25%,  Val Loss:  0.83,  Val Acc: 60.85%,  Time: 0:01:44 \n","Epoch [3/100]\n","Iter:    600,  Train Loss:  0.71,  Train Acc: 62.50%,  Val Loss:  0.81,  Val Acc: 63.32%,  Time: 0:01:54 *\n","Iter:    650,  Train Loss:  0.51,  Train Acc: 87.50%,  Val Loss:  0.86,  Val Acc: 58.47%,  Time: 0:02:02 \n","Iter:    700,  Train Loss:  0.68,  Train Acc: 75.00%,  Val Loss:  0.84,  Val Acc: 61.62%,  Time: 0:02:11 \n","Iter:    750,  Train Loss:  0.62,  Train Acc: 68.75%,  Val Loss:  0.87,  Val Acc: 58.04%,  Time: 0:02:19 \n","Iter:    800,  Train Loss:  0.63,  Train Acc: 68.75%,  Val Loss:  0.88,  Val Acc: 58.55%,  Time: 0:02:28 \n","Iter:    850,  Train Loss:  0.74,  Train Acc: 68.75%,  Val Loss:  0.84,  Val Acc: 62.21%,  Time: 0:02:36 \n","Epoch [4/100]\n","Iter:    900,  Train Loss:  0.41,  Train Acc: 87.50%,  Val Loss:  0.83,  Val Acc: 60.94%,  Time: 0:02:44 \n","Iter:    950,  Train Loss:   0.5,  Train Acc: 81.25%,  Val Loss:  0.83,  Val Acc: 61.19%,  Time: 0:02:53 \n","Iter:   1000,  Train Loss:  0.53,  Train Acc: 81.25%,  Val Loss:  0.85,  Val Acc: 63.32%,  Time: 0:03:01 \n","Iter:   1050,  Train Loss:  0.62,  Train Acc: 62.50%,  Val Loss:   0.9,  Val Acc: 61.62%,  Time: 0:03:10 \n","Iter:   1100,  Train Loss:   1.0,  Train Acc: 68.75%,  Val Loss:  0.93,  Val Acc: 63.49%,  Time: 0:03:18 \n","No optimization for a long time, auto-stopping...\n","f: 0.6108192885367123+0.016656742774987965\n","f1: 0.6225419636864198+0.040860810427804456\n","f2: 0.6123756180708566+0.023507997276620247\n","f3: 0.47658153445330387+0.06005815195690622\n"]}],"source":["f=list()\n","f1=list()\n","f2=list()\n","f3=list()\n","#BERT-CANTONESE\n","for i in range(5):\n","    dataset='/content/drive/MyDrive/Prediction'\n","    df = pd.concat([df2,df_info])\n","    df = df.sample(frac=1, random_state=i)   \n","    cut_idx1 = int(round(0.8 * df.shape[0]))\n","    df_train,df_test = df[:cut_idx1], df[cut_idx1:]\n","    config = BERT_Config(dataset,df_train,df_test)\n","    np.random.seed(1)\n","    torch.manual_seed(1)\n","    torch.cuda.manual_seed_all(1)\n","    torch.backends.cudnn.deterministic = True  # 保证每次结果一样\n","    start_time = time.time()\n","    print(\"Loading data...\")\n","    train_data, test_data = build_dataset(config)\n","    train_iter = build_iterator(train_data, config,)\n","    dev_iter = build_iterator(test_data, config,)\n","    test_iter = build_iterator(test_data, config)\n","    time_dif = get_time_dif(start_time)\n","    print(\"Time usage:\", time_dif)\n","    model = BERT_Model(config).to(config.device)\n","    train(config, model, train_iter, dev_iter, test_iter)\n","    df_test,f_score = test(config, model, test_iter, df_test)\n","    f.append(f_score)\n","    f1.append(get_depth_f1(df_test,1))\n","    f2.append(get_depth_f1(df_test,2))\n","    f3.append(get_depth_f1(df_test,3))\n","\n","\n","print(\"f: \"+str(np.mean(f))+\"+\"+str(np.std(f, ddof = 1)))\n","print(\"f1: \"+str(np.mean(f1))+\"+\"+str(np.std(f1, ddof = 1)))\n","print(\"f2: \"+str(np.mean(f2))+\"+\"+str(np.std(f2, ddof = 1)))\n","print(\"f3: \"+str(np.mean(f3))+\"+\"+str(np.std(f3, ddof = 1)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["d35c5e198ab340c59b6df7962333c0cf","cffc7ab6456d44079aca76c3bf7a8a54","b215df49c8274f7c999886ae1d0f2a67","0d0be826c7a84839af6b8bea6ad58886","4b6b68186b924e5da4f387e5e8aa88ab","fe94bdb132d84b68a0f07ce6f1d5d533","74f09aeaef414f72adcfc9da0101f24a","dbb92e31c0ed4b5f9d1d501bf14903f4","63295c5bced44cfea8912fdad7970b6d","562b632249184d118d1cbedad28e9906","c5a3d1b59f6c4a5093a972af19b771cd","40899b2b7ec9499db3eebb6f91382410","fce9e6de750f40de80e7924072a4c701","8a16107863ff4e678d1f09d52428a5b2","2fe248d6678f49d1b960f20853ddf24e","70eaac4cc699458d9ae9ad7a303afa19","b36e90de30d549e0800e4fa622303a05","f515a7928ef5421598261a34135f675e","3ec0848f71b341bcb37ecec59064dda6","932656006afc418091148cfd06f56321","1e8f7caf3bd54a6eabae710d12ab032d","f3d3c990201b4fab8d40acd4c4b9bbdb","632a0533a6ba4abf90c018b7b5037125","f7531eb574ac4a2db3be5cc9ff23cd6a","6b97bc5c87af4f8cab6c20398bb223db","5a4698d5a8974d97a261ed005f6cec0b","a3c3e4d26c2044a0bebabcede63862eb","db3366d76ab441ecb64efcfffa1bf3e0","73445e2a7c874d20b00c240da624c795","3bdc219ba22346eeab25b71f0a89ee25","6801197e17274ebf8410169977a39ea0","875e214302724e0188b214e22449305c","08658c1a73894a77bbfc4c7541e21bb3"]},"executionInfo":{"elapsed":1351177,"status":"ok","timestamp":1648988200236,"user":{"displayName":"刘耘果","userId":"11745284739417236882"},"user_tz":-480},"id":"gt19BmeXZU4E","outputId":"691d2612-f777-4350-8057-03a5918990ba"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/125k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d35c5e198ab340c59b6df7962333c0cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/585 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40899b2b7ec9499db3eebb6f91382410"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loading data...\n","Time usage: 0:00:02\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"632a0533a6ba4abf90c018b7b5037125"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at toastynews/electra-hongkongese-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias']\n","- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/100]\n","Iter:      0,  Train Loss:   1.2,  Train Acc: 18.75%,  Val Loss:   1.1,  Val Acc: 19.40%,  Time: 0:00:04 *\n","Iter:     50,  Train Loss:   1.1,  Train Acc: 37.50%,  Val Loss:   1.0,  Val Acc: 47.40%,  Time: 0:00:14 *\n","Iter:    100,  Train Loss:   1.0,  Train Acc: 50.00%,  Val Loss:   1.0,  Val Acc: 47.40%,  Time: 0:00:23 *\n","Iter:    150,  Train Loss:  0.88,  Train Acc: 50.00%,  Val Loss:  0.98,  Val Acc: 47.40%,  Time: 0:00:34 *\n","Iter:    200,  Train Loss:  0.96,  Train Acc: 37.50%,  Val Loss:  0.95,  Val Acc: 48.60%,  Time: 0:00:45 *\n","Iter:    250,  Train Loss:  0.86,  Train Acc: 75.00%,  Val Loss:  0.92,  Val Acc: 52.77%,  Time: 0:00:55 *\n","Epoch [2/100]\n","Iter:    300,  Train Loss:  0.86,  Train Acc: 68.75%,  Val Loss:  0.91,  Val Acc: 52.77%,  Time: 0:01:05 *\n","Iter:    350,  Train Loss:   0.9,  Train Acc: 50.00%,  Val Loss:  0.89,  Val Acc: 53.62%,  Time: 0:01:14 *\n","Iter:    400,  Train Loss:  0.81,  Train Acc: 50.00%,  Val Loss:  0.87,  Val Acc: 54.98%,  Time: 0:01:24 *\n","Iter:    450,  Train Loss:  0.75,  Train Acc: 62.50%,  Val Loss:  0.89,  Val Acc: 54.81%,  Time: 0:01:33 \n","Iter:    500,  Train Loss:   1.1,  Train Acc: 43.75%,  Val Loss:  0.88,  Val Acc: 54.64%,  Time: 0:01:41 \n","Iter:    550,  Train Loss:   1.1,  Train Acc: 56.25%,  Val Loss:  0.84,  Val Acc: 57.19%,  Time: 0:01:52 *\n","Epoch [3/100]\n","Iter:    600,  Train Loss:  0.89,  Train Acc: 50.00%,  Val Loss:  0.84,  Val Acc: 57.19%,  Time: 0:02:02 *\n","Iter:    650,  Train Loss:  0.82,  Train Acc: 56.25%,  Val Loss:  0.85,  Val Acc: 58.30%,  Time: 0:02:10 \n","Iter:    700,  Train Loss:   1.1,  Train Acc: 43.75%,  Val Loss:  0.84,  Val Acc: 58.64%,  Time: 0:02:19 \n","Iter:    750,  Train Loss:  0.92,  Train Acc: 50.00%,  Val Loss:  0.87,  Val Acc: 57.45%,  Time: 0:02:27 \n","Iter:    800,  Train Loss:  0.83,  Train Acc: 75.00%,  Val Loss:  0.81,  Val Acc: 60.85%,  Time: 0:02:37 *\n","Iter:    850,  Train Loss:   1.0,  Train Acc: 43.75%,  Val Loss:  0.88,  Val Acc: 58.38%,  Time: 0:02:45 \n","Epoch [4/100]\n","Iter:    900,  Train Loss:  0.68,  Train Acc: 62.50%,  Val Loss:  0.85,  Val Acc: 61.11%,  Time: 0:02:54 \n","Iter:    950,  Train Loss:  0.66,  Train Acc: 75.00%,  Val Loss:  0.92,  Val Acc: 57.28%,  Time: 0:03:02 \n","Iter:   1000,  Train Loss:  0.52,  Train Acc: 75.00%,  Val Loss:  0.87,  Val Acc: 59.83%,  Time: 0:03:11 \n","Iter:   1050,  Train Loss:   0.8,  Train Acc: 75.00%,  Val Loss:  0.95,  Val Acc: 57.70%,  Time: 0:03:19 \n","Iter:   1100,  Train Loss:  0.94,  Train Acc: 50.00%,  Val Loss:  0.86,  Val Acc: 60.17%,  Time: 0:03:28 \n","Iter:   1150,  Train Loss:  0.65,  Train Acc: 81.25%,  Val Loss:  0.87,  Val Acc: 59.06%,  Time: 0:03:36 \n","Epoch [5/100]\n","Iter:   1200,  Train Loss:  0.61,  Train Acc: 75.00%,  Val Loss:  0.86,  Val Acc: 61.79%,  Time: 0:03:44 \n","Iter:   1250,  Train Loss:  0.67,  Train Acc: 68.75%,  Val Loss:   0.9,  Val Acc: 59.91%,  Time: 0:03:53 \n","Iter:   1300,  Train Loss:   0.4,  Train Acc: 87.50%,  Val Loss:   0.9,  Val Acc: 60.85%,  Time: 0:04:01 \n","No optimization for a long time, auto-stopping...\n","Loading data...\n","Time usage: 0:00:02\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at toastynews/electra-hongkongese-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias']\n","- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/100]\n","Iter:      0,  Train Loss:   1.3,  Train Acc:  6.25%,  Val Loss:   1.1,  Val Acc: 18.81%,  Time: 0:00:04 *\n","Iter:     50,  Train Loss:  0.94,  Train Acc: 43.75%,  Val Loss:  0.99,  Val Acc: 49.19%,  Time: 0:00:14 *\n","Iter:    100,  Train Loss:   1.1,  Train Acc: 31.25%,  Val Loss:  0.97,  Val Acc: 49.19%,  Time: 0:00:23 *\n","Iter:    150,  Train Loss:  0.85,  Train Acc: 43.75%,  Val Loss:  0.96,  Val Acc: 50.04%,  Time: 0:00:33 *\n","Iter:    200,  Train Loss:  0.84,  Train Acc: 43.75%,  Val Loss:  0.94,  Val Acc: 52.17%,  Time: 0:00:43 *\n","Iter:    250,  Train Loss:   1.0,  Train Acc: 50.00%,  Val Loss:  0.94,  Val Acc: 53.28%,  Time: 0:00:52 *\n","Epoch [2/100]\n","Iter:    300,  Train Loss:  0.81,  Train Acc: 56.25%,  Val Loss:   0.9,  Val Acc: 54.55%,  Time: 0:01:02 *\n","Iter:    350,  Train Loss:  0.99,  Train Acc: 56.25%,  Val Loss:  0.89,  Val Acc: 53.53%,  Time: 0:01:12 *\n","Iter:    400,  Train Loss:  0.96,  Train Acc: 31.25%,  Val Loss:  0.89,  Val Acc: 52.51%,  Time: 0:01:20 \n","Iter:    450,  Train Loss:  0.79,  Train Acc: 50.00%,  Val Loss:  0.88,  Val Acc: 54.64%,  Time: 0:01:30 *\n","Iter:    500,  Train Loss:  0.93,  Train Acc: 50.00%,  Val Loss:   0.9,  Val Acc: 53.45%,  Time: 0:01:39 \n","Iter:    550,  Train Loss:  0.78,  Train Acc: 56.25%,  Val Loss:  0.88,  Val Acc: 55.06%,  Time: 0:01:47 \n","Epoch [3/100]\n","Iter:    600,  Train Loss:  0.66,  Train Acc: 68.75%,  Val Loss:  0.87,  Val Acc: 55.32%,  Time: 0:01:57 *\n","Iter:    650,  Train Loss:  0.79,  Train Acc: 68.75%,  Val Loss:   0.9,  Val Acc: 54.81%,  Time: 0:02:05 \n","Iter:    700,  Train Loss:   0.6,  Train Acc: 81.25%,  Val Loss:  0.86,  Val Acc: 56.43%,  Time: 0:02:15 *\n","Iter:    750,  Train Loss:  0.65,  Train Acc: 75.00%,  Val Loss:  0.89,  Val Acc: 56.85%,  Time: 0:02:23 \n","Iter:    800,  Train Loss:  0.86,  Train Acc: 50.00%,  Val Loss:  0.89,  Val Acc: 56.77%,  Time: 0:02:32 \n","Iter:    850,  Train Loss:  0.97,  Train Acc: 56.25%,  Val Loss:  0.86,  Val Acc: 57.70%,  Time: 0:02:41 *\n","Epoch [4/100]\n","Iter:    900,  Train Loss:  0.93,  Train Acc: 62.50%,  Val Loss:  0.86,  Val Acc: 57.36%,  Time: 0:02:50 \n","Iter:    950,  Train Loss:  0.48,  Train Acc: 93.75%,  Val Loss:  0.85,  Val Acc: 58.38%,  Time: 0:03:00 *\n","Iter:   1000,  Train Loss:  0.42,  Train Acc: 87.50%,  Val Loss:  0.87,  Val Acc: 59.32%,  Time: 0:03:08 \n","Iter:   1050,  Train Loss:  0.56,  Train Acc: 75.00%,  Val Loss:  0.89,  Val Acc: 59.74%,  Time: 0:03:17 \n","Iter:   1100,  Train Loss:  0.95,  Train Acc: 62.50%,  Val Loss:  0.89,  Val Acc: 59.32%,  Time: 0:03:25 \n","Iter:   1150,  Train Loss:  0.45,  Train Acc: 87.50%,  Val Loss:  0.87,  Val Acc: 59.91%,  Time: 0:03:33 \n","Epoch [5/100]\n","Iter:   1200,  Train Loss:  0.46,  Train Acc: 81.25%,  Val Loss:   0.9,  Val Acc: 58.89%,  Time: 0:03:42 \n","Iter:   1250,  Train Loss:  0.62,  Train Acc: 81.25%,  Val Loss:   0.9,  Val Acc: 58.89%,  Time: 0:03:50 \n","Iter:   1300,  Train Loss:  0.39,  Train Acc: 87.50%,  Val Loss:   0.9,  Val Acc: 60.85%,  Time: 0:03:59 \n","Iter:   1350,  Train Loss:  0.79,  Train Acc: 75.00%,  Val Loss:  0.98,  Val Acc: 60.43%,  Time: 0:04:07 \n","Iter:   1400,  Train Loss:  0.63,  Train Acc: 68.75%,  Val Loss:  0.96,  Val Acc: 59.57%,  Time: 0:04:16 \n","Iter:   1450,  Train Loss:  0.33,  Train Acc: 93.75%,  Val Loss:  0.93,  Val Acc: 60.17%,  Time: 0:04:24 \n","No optimization for a long time, auto-stopping...\n","Loading data...\n","Time usage: 0:00:02\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at toastynews/electra-hongkongese-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias']\n","- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/100]\n","Iter:      0,  Train Loss:   1.0,  Train Acc: 37.50%,  Val Loss:   1.2,  Val Acc: 16.85%,  Time: 0:00:04 *\n","Iter:     50,  Train Loss:  0.87,  Train Acc: 68.75%,  Val Loss:  0.98,  Val Acc: 48.43%,  Time: 0:00:14 *\n","Iter:    100,  Train Loss:  0.89,  Train Acc: 56.25%,  Val Loss:  0.96,  Val Acc: 48.43%,  Time: 0:00:24 *\n","Iter:    150,  Train Loss:   1.0,  Train Acc: 56.25%,  Val Loss:  0.94,  Val Acc: 50.98%,  Time: 0:00:34 *\n","Iter:    200,  Train Loss:  0.91,  Train Acc: 56.25%,  Val Loss:  0.91,  Val Acc: 53.53%,  Time: 0:00:43 *\n","Iter:    250,  Train Loss:  0.74,  Train Acc: 62.50%,  Val Loss:  0.89,  Val Acc: 53.45%,  Time: 0:00:53 *\n","Epoch [2/100]\n","Iter:    300,  Train Loss:  0.91,  Train Acc: 37.50%,  Val Loss:  0.88,  Val Acc: 54.47%,  Time: 0:01:04 *\n","Iter:    350,  Train Loss:  0.76,  Train Acc: 43.75%,  Val Loss:  0.87,  Val Acc: 55.23%,  Time: 0:01:14 *\n","Iter:    400,  Train Loss:   1.1,  Train Acc: 43.75%,  Val Loss:  0.85,  Val Acc: 54.98%,  Time: 0:01:23 *\n","Iter:    450,  Train Loss:   0.9,  Train Acc: 50.00%,  Val Loss:  0.84,  Val Acc: 57.62%,  Time: 0:01:33 *\n","Iter:    500,  Train Loss:  0.63,  Train Acc: 81.25%,  Val Loss:  0.83,  Val Acc: 57.53%,  Time: 0:01:44 *\n","Iter:    550,  Train Loss:  0.84,  Train Acc: 68.75%,  Val Loss:  0.84,  Val Acc: 55.57%,  Time: 0:01:52 \n","Epoch [3/100]\n","Iter:    600,  Train Loss:  0.69,  Train Acc: 81.25%,  Val Loss:  0.83,  Val Acc: 58.55%,  Time: 0:02:02 *\n","Iter:    650,  Train Loss:   1.3,  Train Acc: 56.25%,  Val Loss:  0.83,  Val Acc: 59.40%,  Time: 0:02:11 \n","Iter:    700,  Train Loss:  0.79,  Train Acc: 43.75%,  Val Loss:  0.83,  Val Acc: 59.15%,  Time: 0:02:19 \n","Iter:    750,  Train Loss:  0.85,  Train Acc: 43.75%,  Val Loss:  0.81,  Val Acc: 60.17%,  Time: 0:02:29 *\n","Iter:    800,  Train Loss:  0.58,  Train Acc: 68.75%,  Val Loss:  0.81,  Val Acc: 60.17%,  Time: 0:02:38 *\n","Iter:    850,  Train Loss:  0.73,  Train Acc: 56.25%,  Val Loss:   0.8,  Val Acc: 62.21%,  Time: 0:02:48 *\n","Epoch [4/100]\n","Iter:    900,  Train Loss:  0.91,  Train Acc: 56.25%,  Val Loss:  0.82,  Val Acc: 61.87%,  Time: 0:02:57 \n","Iter:    950,  Train Loss:  0.65,  Train Acc: 68.75%,  Val Loss:  0.82,  Val Acc: 61.36%,  Time: 0:03:05 \n","Iter:   1000,  Train Loss:  0.66,  Train Acc: 56.25%,  Val Loss:  0.81,  Val Acc: 61.70%,  Time: 0:03:13 \n","Iter:   1050,  Train Loss:  0.52,  Train Acc: 75.00%,  Val Loss:  0.81,  Val Acc: 62.72%,  Time: 0:03:22 \n","Iter:   1100,  Train Loss:  0.61,  Train Acc: 81.25%,  Val Loss:  0.83,  Val Acc: 62.81%,  Time: 0:03:30 \n","Iter:   1150,  Train Loss:   0.7,  Train Acc: 68.75%,  Val Loss:  0.83,  Val Acc: 63.83%,  Time: 0:03:39 \n","Epoch [5/100]\n","Iter:   1200,  Train Loss:  0.48,  Train Acc: 75.00%,  Val Loss:  0.87,  Val Acc: 62.04%,  Time: 0:03:47 \n","Iter:   1250,  Train Loss:  0.68,  Train Acc: 81.25%,  Val Loss:  0.85,  Val Acc: 62.55%,  Time: 0:03:56 \n","Iter:   1300,  Train Loss:  0.66,  Train Acc: 68.75%,  Val Loss:  0.86,  Val Acc: 61.45%,  Time: 0:04:04 \n","Iter:   1350,  Train Loss:   0.6,  Train Acc: 75.00%,  Val Loss:  0.85,  Val Acc: 61.79%,  Time: 0:04:13 \n","No optimization for a long time, auto-stopping...\n","Loading data...\n","Time usage: 0:00:02\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at toastynews/electra-hongkongese-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias']\n","- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/100]\n","Iter:      0,  Train Loss:   1.1,  Train Acc: 37.50%,  Val Loss:   1.1,  Val Acc: 19.32%,  Time: 0:00:04 *\n","Iter:     50,  Train Loss:   0.9,  Train Acc: 43.75%,  Val Loss:  0.99,  Val Acc: 49.53%,  Time: 0:00:14 *\n","Iter:    100,  Train Loss:  0.83,  Train Acc: 62.50%,  Val Loss:  0.97,  Val Acc: 49.70%,  Time: 0:00:23 *\n","Iter:    150,  Train Loss:   1.0,  Train Acc: 50.00%,  Val Loss:  0.96,  Val Acc: 49.87%,  Time: 0:00:34 *\n","Iter:    200,  Train Loss:   1.0,  Train Acc: 31.25%,  Val Loss:  0.94,  Val Acc: 51.32%,  Time: 0:00:44 *\n","Iter:    250,  Train Loss:  0.91,  Train Acc: 56.25%,  Val Loss:  0.93,  Val Acc: 53.02%,  Time: 0:00:53 *\n","Epoch [2/100]\n","Iter:    300,  Train Loss:  0.88,  Train Acc: 43.75%,  Val Loss:   0.9,  Val Acc: 53.11%,  Time: 0:01:04 *\n","Iter:    350,  Train Loss:  0.73,  Train Acc: 62.50%,  Val Loss:  0.89,  Val Acc: 54.04%,  Time: 0:01:14 *\n","Iter:    400,  Train Loss:  0.83,  Train Acc: 56.25%,  Val Loss:  0.88,  Val Acc: 54.38%,  Time: 0:01:23 *\n","Iter:    450,  Train Loss:  0.89,  Train Acc: 56.25%,  Val Loss:  0.88,  Val Acc: 53.02%,  Time: 0:01:33 *\n","Iter:    500,  Train Loss:  0.82,  Train Acc: 50.00%,  Val Loss:  0.86,  Val Acc: 56.60%,  Time: 0:01:43 *\n","Iter:    550,  Train Loss:  0.84,  Train Acc: 56.25%,  Val Loss:  0.85,  Val Acc: 56.17%,  Time: 0:01:52 *\n","Epoch [3/100]\n","Iter:    600,  Train Loss:  0.73,  Train Acc: 56.25%,  Val Loss:  0.85,  Val Acc: 57.02%,  Time: 0:02:02 *\n","Iter:    650,  Train Loss:  0.91,  Train Acc: 56.25%,  Val Loss:  0.84,  Val Acc: 57.53%,  Time: 0:02:12 *\n","Iter:    700,  Train Loss:  0.87,  Train Acc: 56.25%,  Val Loss:  0.84,  Val Acc: 57.70%,  Time: 0:02:20 \n","Iter:    750,  Train Loss:  0.71,  Train Acc: 62.50%,  Val Loss:  0.85,  Val Acc: 57.02%,  Time: 0:02:29 \n","Iter:    800,  Train Loss:  0.94,  Train Acc: 56.25%,  Val Loss:  0.86,  Val Acc: 59.15%,  Time: 0:02:37 \n","Iter:    850,  Train Loss:  0.75,  Train Acc: 81.25%,  Val Loss:  0.82,  Val Acc: 58.64%,  Time: 0:02:47 *\n","Epoch [4/100]\n","Iter:    900,  Train Loss:  0.56,  Train Acc: 81.25%,  Val Loss:  0.82,  Val Acc: 59.66%,  Time: 0:02:58 *\n","Iter:    950,  Train Loss:   0.6,  Train Acc: 81.25%,  Val Loss:  0.83,  Val Acc: 59.91%,  Time: 0:03:06 \n","Iter:   1000,  Train Loss:  0.77,  Train Acc: 56.25%,  Val Loss:  0.83,  Val Acc: 60.26%,  Time: 0:03:15 \n","Iter:   1050,  Train Loss:  0.68,  Train Acc: 56.25%,  Val Loss:  0.88,  Val Acc: 58.89%,  Time: 0:03:23 \n","Iter:   1100,  Train Loss:  0.56,  Train Acc: 62.50%,  Val Loss:  0.83,  Val Acc: 61.53%,  Time: 0:03:32 \n","Iter:   1150,  Train Loss:  0.58,  Train Acc: 87.50%,  Val Loss:  0.82,  Val Acc: 61.79%,  Time: 0:03:40 \n","Epoch [5/100]\n","Iter:   1200,  Train Loss:  0.71,  Train Acc: 75.00%,  Val Loss:  0.82,  Val Acc: 61.45%,  Time: 0:03:49 \n","Iter:   1250,  Train Loss:  0.39,  Train Acc: 87.50%,  Val Loss:  0.84,  Val Acc: 61.79%,  Time: 0:03:57 \n","Iter:   1300,  Train Loss:   0.7,  Train Acc: 81.25%,  Val Loss:  0.86,  Val Acc: 61.79%,  Time: 0:04:05 \n","Iter:   1350,  Train Loss:  0.57,  Train Acc: 75.00%,  Val Loss:  0.87,  Val Acc: 62.30%,  Time: 0:04:14 \n","Iter:   1400,  Train Loss:  0.79,  Train Acc: 68.75%,  Val Loss:  0.87,  Val Acc: 60.77%,  Time: 0:04:22 \n","No optimization for a long time, auto-stopping...\n","Loading data...\n","Time usage: 0:00:02\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at toastynews/electra-hongkongese-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias']\n","- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/100]\n","Iter:      0,  Train Loss:   1.3,  Train Acc:  6.25%,  Val Loss:   1.2,  Val Acc: 16.51%,  Time: 0:00:04 *\n","Iter:     50,  Train Loss:   1.1,  Train Acc: 43.75%,  Val Loss:  0.96,  Val Acc: 50.04%,  Time: 0:00:14 *\n","Iter:    100,  Train Loss:  0.99,  Train Acc: 37.50%,  Val Loss:  0.94,  Val Acc: 50.13%,  Time: 0:00:24 *\n","Iter:    150,  Train Loss:   1.1,  Train Acc: 37.50%,  Val Loss:  0.94,  Val Acc: 50.13%,  Time: 0:00:33 *\n","Iter:    200,  Train Loss:   0.9,  Train Acc: 50.00%,  Val Loss:  0.91,  Val Acc: 54.04%,  Time: 0:00:44 *\n","Iter:    250,  Train Loss:  0.99,  Train Acc: 43.75%,  Val Loss:   0.9,  Val Acc: 53.19%,  Time: 0:00:54 *\n","Epoch [2/100]\n","Iter:    300,  Train Loss:  0.91,  Train Acc: 56.25%,  Val Loss:  0.87,  Val Acc: 54.72%,  Time: 0:01:04 *\n","Iter:    350,  Train Loss:  0.78,  Train Acc: 62.50%,  Val Loss:  0.87,  Val Acc: 54.72%,  Time: 0:01:12 \n","Iter:    400,  Train Loss:  0.71,  Train Acc: 75.00%,  Val Loss:  0.86,  Val Acc: 55.91%,  Time: 0:01:22 *\n","Iter:    450,  Train Loss:   1.1,  Train Acc: 50.00%,  Val Loss:  0.87,  Val Acc: 55.83%,  Time: 0:01:30 \n","Iter:    500,  Train Loss:  0.84,  Train Acc: 50.00%,  Val Loss:  0.86,  Val Acc: 56.43%,  Time: 0:01:41 *\n","Iter:    550,  Train Loss:  0.72,  Train Acc: 56.25%,  Val Loss:  0.86,  Val Acc: 55.06%,  Time: 0:01:50 \n","Epoch [3/100]\n","Iter:    600,  Train Loss:  0.78,  Train Acc: 62.50%,  Val Loss:  0.85,  Val Acc: 56.26%,  Time: 0:01:59 *\n","Iter:    650,  Train Loss:   0.9,  Train Acc: 50.00%,  Val Loss:  0.89,  Val Acc: 55.83%,  Time: 0:02:08 \n","Iter:    700,  Train Loss:  0.73,  Train Acc: 81.25%,  Val Loss:  0.84,  Val Acc: 57.19%,  Time: 0:02:17 *\n","Iter:    750,  Train Loss:  0.88,  Train Acc: 50.00%,  Val Loss:  0.83,  Val Acc: 58.55%,  Time: 0:02:27 *\n","Iter:    800,  Train Loss:  0.82,  Train Acc: 62.50%,  Val Loss:  0.87,  Val Acc: 56.17%,  Time: 0:02:36 \n","Iter:    850,  Train Loss:  0.69,  Train Acc: 62.50%,  Val Loss:  0.84,  Val Acc: 59.40%,  Time: 0:02:44 \n","Epoch [4/100]\n","Iter:    900,  Train Loss:  0.67,  Train Acc: 50.00%,  Val Loss:  0.87,  Val Acc: 59.57%,  Time: 0:02:52 \n","Iter:    950,  Train Loss:  0.78,  Train Acc: 68.75%,  Val Loss:  0.86,  Val Acc: 59.83%,  Time: 0:03:01 \n","Iter:   1000,  Train Loss:  0.72,  Train Acc: 68.75%,  Val Loss:  0.82,  Val Acc: 59.74%,  Time: 0:03:11 *\n","Iter:   1050,  Train Loss:  0.58,  Train Acc: 75.00%,  Val Loss:  0.83,  Val Acc: 61.11%,  Time: 0:03:19 \n","Iter:   1100,  Train Loss:  0.98,  Train Acc: 56.25%,  Val Loss:  0.85,  Val Acc: 60.43%,  Time: 0:03:28 \n","Iter:   1150,  Train Loss:  0.77,  Train Acc: 75.00%,  Val Loss:  0.84,  Val Acc: 61.53%,  Time: 0:03:36 \n","Epoch [5/100]\n","Iter:   1200,  Train Loss:  0.39,  Train Acc: 93.75%,  Val Loss:  0.85,  Val Acc: 60.94%,  Time: 0:03:44 \n","Iter:   1250,  Train Loss:  0.83,  Train Acc: 50.00%,  Val Loss:  0.86,  Val Acc: 61.11%,  Time: 0:03:53 \n","Iter:   1300,  Train Loss:  0.85,  Train Acc: 75.00%,  Val Loss:  0.85,  Val Acc: 61.45%,  Time: 0:04:01 \n","Iter:   1350,  Train Loss:  0.58,  Train Acc: 68.75%,  Val Loss:  0.86,  Val Acc: 61.87%,  Time: 0:04:10 \n","Iter:   1400,  Train Loss:  0.74,  Train Acc: 68.75%,  Val Loss:  0.88,  Val Acc: 61.53%,  Time: 0:04:18 \n","Iter:   1450,  Train Loss:  0.49,  Train Acc: 75.00%,  Val Loss:  0.86,  Val Acc: 62.04%,  Time: 0:04:27 \n","Epoch [6/100]\n","Iter:   1500,  Train Loss:  0.62,  Train Acc: 81.25%,  Val Loss:  0.88,  Val Acc: 63.23%,  Time: 0:04:35 \n","No optimization for a long time, auto-stopping...\n","f: 0.5752357913358883+0.020908488682439913\n","f1: 0.5683684125547487+0.0513452383668131\n","f2: 0.5806906758587996+0.03179448910833221\n","f3: 0.4615974673378406+0.034780130588372746\n"]}],"source":["f=list()\n","f1=list()\n","f2=list()\n","f3=list()\n","#ELECTRA-CHINESE\n","for i in range(5):\n","    dataset='/content/drive/MyDrive/Prediction'\n","    df = pd.concat([df2,df_info])\n","    df = df.sample(frac=1, random_state=i)   \n","    cut_idx1 = int(round(0.8 * df.shape[0]))\n","    df_train,df_test = df[:cut_idx1], df[cut_idx1:]\n","    config = Electra_Config(dataset,df_train,df_test)\n","    np.random.seed(1)\n","    torch.manual_seed(1)\n","    torch.cuda.manual_seed_all(1)\n","    torch.backends.cudnn.deterministic = True  # 保证每次结果一样\n","    start_time = time.time()\n","    print(\"Loading data...\")\n","    train_data, test_data = build_dataset(config)\n","    train_iter = build_iterator(train_data, config,)\n","    dev_iter = build_iterator(test_data, config,)\n","    test_iter = build_iterator(test_data, config)\n","    time_dif = get_time_dif(start_time)\n","    print(\"Time usage:\", time_dif)\n","    model = Electra_Model(config).to(config.device)\n","    train(config, model, train_iter, dev_iter, test_iter)\n","    df_test,f_score = test(config, model, test_iter, df_test)\n","    f.append(f_score)\n","    f1.append(get_depth_f1(df_test,1))\n","    f2.append(get_depth_f1(df_test,2))\n","    f3.append(get_depth_f1(df_test,3))\n","\n","\n","print(\"f: \"+str(np.mean(f))+\"+\"+str(np.std(f, ddof = 1)))\n","print(\"f1: \"+str(np.mean(f1))+\"+\"+str(np.std(f1, ddof = 1)))\n","print(\"f2: \"+str(np.mean(f2))+\"+\"+str(np.std(f2, ddof = 1)))\n","print(\"f3: \"+str(np.mean(f3))+\"+\"+str(np.std(f3, ddof = 1)))"]},{"cell_type":"markdown","metadata":{"id":"ju43j9EvfnRb"},"source":["# SVM-ngram Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3vyMRlVKftb1"},"outputs":[],"source":["import pycantonese\n","\n","stop_words = pycantonese.stop_words()\n","def create_sentences(df):\n","  sentences=[]\n","  for i in range(len(df)):\n","      text_raw=df['text'].iloc[i]\n","      segs = pycantonese.segment(text_raw)\n","      segs = filter(lambda x:x not in stop_words, segs)\n","      sentences.append((\" \".join(segs),df['label'].iloc[i]))\n","  return sentences\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":69472,"status":"ok","timestamp":1648717916341,"user":{"displayName":"刘耘果","userId":"11745284739417236882"},"user_tz":-480},"id":"JpTpioBpyvcN","outputId":"2b2a0cc5-fb2c-4b97-90c9-4f5cdc086132"},"outputs":[{"name":"stdout","output_type":"stream","text":["f: 0.5814944220161355+0.014710859049724941\n","f1: 0.6006518201108779+0.06745646440246371\n","f2: 0.5746718842473272+0.01750741325619133\n","f3: 0.5083059074370128+0.03277908112537229\n"]}],"source":["import random\n","from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC,LinearSVC\n","from sklearn import svm\n","from sklearn.metrics import *\n","import tensorflow as tf\n","import scipy.sparse as sp\n","from scipy.sparse import hstack\n","\n","\n","# import warnings\n","# warnings.filterwarnings(\"ignore\")\n","class TextClassifier():\n","    def __init__(self, classifier=SVC(kernel='linear')):\n","        self.classifier = LinearSVC(C=0.2, class_weight=None, dual=True, fit_intercept=True,\n","                       intercept_scaling=1, loss='squared_hinge', max_iter=300,\n","                       multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n","                       verbose=0)\n","        self.vectorizer1 = CountVectorizer(analyzer='word', ngram_range=(1,3), stop_words=stop_words)\n","        self.vectorizer2 = CountVectorizer(analyzer='char', ngram_range=(2,5), stop_words=stop_words)\n","    def features(self, X):\n","        word=self.vectorizer1.transform(X)\n","        character=self.vectorizer2.transform(X)\n","        feature = hstack([word,character])\n","        return feature\n","    def fit(self, X, y):\n","        self.vectorizer1.fit(X)\n","        self.vectorizer2.fit(X)\n","        self.classifier.fit(self.features(X), y)\n"," \n","    def predict(self, x):\n","        return self.classifier.predict(self.features(x))\n"," \n","    def score(self, X, y):\n","        return self.classifier.score(self.features(X), y)\n","\n","f=list()\n","f1=list()\n","f2=list()\n","f3=list()\n","\n","def get_depth_f1(df,depth):\n","  if depth < 3:\n","    df_depth=df.loc[df['depth']==depth]\n","  else:\n","    df_depth=df.loc[df['depth']>=depth]\n","  f1 = f1_score(df_depth['label'].to_list(),df_depth['predicted'].to_list(),average='macro')\n","  return f1\n","\n","for i in range(10):\n","  dataset='/content/drive/MyDrive/Prediction'\n","  df = pd.concat([df2,df_info])\n","  df = df.sample(frac=1, random_state=i)   \n","  cut_idx1 = int(round(0.8 * df.shape[0]))\n","  df_train,df_test = df[:cut_idx1], df[cut_idx1:]\n","  sentences_train=create_sentences(df_train)\n","  sentences_test=create_sentences(df_test)\n","  x_train, y_train = zip(*sentences_train)\n","  x_test, y_test = zip(*sentences_test)\n","  text_classifier=TextClassifier()\n","  text_classifier.fit(x_train, y_train)\n","  # text_classifier.score(x_test, y_test)\n","  y_pred = text_classifier.predict(x_test)\n","  df_test['predicted']=y_pred\n","  f.append(f1_score(y_test, y_pred,average='macro'))\n","  f1.append(get_depth_f1(df_test,1))\n","  f2.append(get_depth_f1(df_test,2))\n","  f3.append(get_depth_f1(df_test,3))\n","\n","\n","print(\"f: \"+str(np.mean(f))+\"+\"+str(np.std(f, ddof = 1)))\n","print(\"f1: \"+str(np.mean(f1))+\"+\"+str(np.std(f1, ddof = 1)))\n","print(\"f2: \"+str(np.mean(f2))+\"+\"+str(np.std(f2, ddof = 1)))\n","print(\"f3: \"+str(np.mean(f3))+\"+\"+str(np.std(f3, ddof = 1)))"]},{"cell_type":"markdown","metadata":{"id":"a_2XFAhokmEP"},"source":["# Deep Learning Based Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jARI2e4vo7iu"},"outputs":[],"source":["# coding: UTF-8\n","import os\n","import torch\n","import numpy as np\n","import gensim\n","import numpy as np\n","import pickle as pkl\n","from tqdm import tqdm\n","import time\n","from datetime import timedelta\n","import pycantonese\n","import jieba\n","MAX_VOCAB_SIZE = 20000  # 词表长度限制\n","UNK, PAD = '<UNK>', '<PAD>'  # 未知字，padding符号\n","\n","#fenci=lambda x: [y for y in x] \n","fenci=jieba.lcut\n","def build_vocab(tokenizer, max_size, min_freq, df):\n","    vocab_dic = {}\n","    for i in range(len(df)):\n","      content = df['text'].iloc[i]\n","      for word in tokenizer(content):\n","        vocab_dic[word] = vocab_dic.get(word, 0) + 1\n","    vocab_list = sorted([_ for _ in vocab_dic.items() if _[1] >= min_freq], key=lambda x: x[1], reverse=True)[:max_size-2]\n","    vocab_dic = {word_count[0]: idx for idx, word_count in enumerate(vocab_list)}\n","    vocab_dic.update({UNK: len(vocab_dic), PAD: len(vocab_dic) + 1})\n","    return vocab_dic\n","\n","def build_dataset(config):\n","    tokenizer = fenci\n","    if os.path.exists(config.vocab_path):\n","        vocab = pkl.load(open(config.vocab_path, 'rb'))\n","    else:\n","        vocab = build_vocab(config.train_path, tokenizer=tokenizer, max_size=MAX_VOCAB_SIZE, min_freq=1)\n","        pkl.dump(vocab, open(config.vocab_path, 'wb'))\n","    print(f\"Vocab size: {len(vocab)}\")\n","\n","    def load_dataset(data, pad_size=32):\n","        contents = []\n","        for i in range(len(data)):\n","                content = data['text'].iloc[i]\n","                label = data['label'].iloc[i]\n","                words_line = []\n","                token = tokenizer(content)\n","                seq_len = len(token)\n","                if pad_size:\n","                    if len(token) < pad_size:\n","                        token.extend([PAD] * (pad_size - len(token)))\n","                    else:\n","                        token = token[:pad_size]\n","                        seq_len = pad_size\n","                # word to id\n","                for word in token:\n","                    words_line.append(vocab.get(word, vocab.get(UNK)))\n","                contents.append((words_line, int(label), seq_len))\n","        return contents \n","    train = load_dataset(config.train_df, config.pad_size)\n","    test = load_dataset(config.test_df, config.pad_size)\n","    return vocab, train, test\n","\n","\n","class DatasetIterater(object):\n","    def __init__(self, batches, batch_size, device, pad_size, is_Tan):\n","        self.batch_size = batch_size\n","        self.batches = batches\n","        self.n_batches = len(batches) // batch_size\n","        self.residue = False  # 记录batch数量是否为整数\n","        if len(batches) % self.n_batches != 0:\n","            self.residue = True\n","        self.index = 0\n","        self.device = device\n","        self.is_Tan = is_Tan\n","        self.target = '疫苗'\n","        tokenizer=fenci\n","        vocab = pkl.load(open(config.vocab_path, 'rb'))\n","        self.target_ids= [vocab.get(word, vocab.get(UNK)) for word in tokenizer(self.target)]\n","        if len(self.target_ids) < pad_size:\n","            self.target_ids.extend([vocab.get(PAD)] * (pad_size - len(self.target_ids)))\n","        \n","    def _to_tensor(self, datas):\n","        x = torch.LongTensor([_[0] for _ in datas]).to(self.device)\n","        y = torch.LongTensor([_[1] for _ in datas]).to(self.device)\n","        # pad前的长度(超过pad_size的设为pad_size)\n","        seq_len = torch.LongTensor([_[2] for _ in datas]).to(self.device)\n","        if self.is_Tan:\n","          t = torch.LongTensor([self.target_ids for _ in range(len(datas))]).to(self.device)\n","          return (x, t, seq_len), y\n","        else:\n","          return (x, seq_len), y\n","\n","    def __next__(self):\n","        if self.residue and self.index == self.n_batches:\n","            batches = self.batches[self.index * self.batch_size: len(self.batches)]\n","            self.index += 1\n","            batches = self._to_tensor(batches)\n","            return batches\n","\n","        elif self.index >= self.n_batches:\n","            self.index = 0\n","            raise StopIteration\n","        else:\n","            batches = self.batches[self.index * self.batch_size: (self.index + 1) * self.batch_size]\n","            self.index += 1\n","            batches = self._to_tensor(batches)\n","            return batches\n","\n","    def __iter__(self):\n","        return self\n","\n","    def __len__(self):\n","        if self.residue:\n","            return self.n_batches + 1\n","        else:\n","            return self.n_batches\n","\n","\n","def build_iterator(dataset, config ,is_Tan=True):\n","    \n","    iter = DatasetIterater(dataset, config.batch_size, config.device, config.pad_size, is_Tan)\n","    return iter\n","\n","\n","def get_time_dif(start_time):\n","    \"\"\"获取已使用时间\"\"\"\n","    end_time = time.time()\n","    time_dif = end_time - start_time\n","    return timedelta(seconds=int(round(time_dif)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":413},"executionInfo":{"elapsed":1896,"status":"error","timestamp":1644147763168,"user":{"displayName":"刘耘果","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11745284739417236882"},"user_tz":-480},"id":"Lb_KnGDNLB_5","outputId":"505e9d53-0fe8-49cd-9abd-4b0aa09a8764"},"outputs":[{"name":"stdout","output_type":"stream","text":["11694\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-98-80662cb1e039>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_to_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_to_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0memb_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'/content/drive/MyDrive/DATA/sgns.wiki.word.bz2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_to_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_to_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1436\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid vector on line %s (is this really the text format?)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mline_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m                 \u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid vector on line %s (is this really the text format?)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mline_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m                 \u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["\n","'''提取预训练词向量'''\n","# 下面的目录、文件名按需更改。\n","emb_dim = 300\n","vocab_dir = \"/content/drive/MyDrive/Prediction/data/vocab.pkl\"\n","filename_trimmed_dir = \"/content/drive/MyDrive/Prediction/data/embedding_CantoStance\"\n","\n","if os.path.exists(vocab_dir):\n","    word_to_id = pkl.load(open(vocab_dir, 'rb'))\n","else:\n","    tokenizer = fenci\n","    #tokenizer = lambda x: [y for y in x]  # 以字为单位构建词表\n","    word_to_id = build_vocab(tokenizer=tokenizer, max_size=MAX_VOCAB_SIZE, min_freq=1,df=df)\n","    pkl.dump(word_to_id, open(vocab_dir, 'wb'))\n","\n","count=0\n","embeddings = np.random.rand(len(word_to_id), emb_dim)\n","print(len(word_to_id))\n","emb_model = gensim.models.KeyedVectors.load_word2vec_format(r'/content/drive/MyDrive/DATA/sgns.wiki.word.bz2')\n","for i in word_to_id.keys():\n","  idx = word_to_id[i]\n","  if i not in emb_model.vocab:\n","    count+=1\n","    emb = [0]* emb_dim\n","  else:\n","    emb = emb_model[i]\n","  embeddings[idx] = np.asarray(emb, dtype='float32')\n","print(count)\n","np.savez_compressed(filename_trimmed_dir, embeddings=embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tE5bYBf4ztyg"},"outputs":[],"source":["# coding: UTF-8\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from sklearn import metrics\n","import time\n","\n","def init_network(model, method='xavier', exclude='embedding', seed=123):\n","    for name, w in model.named_parameters():\n","        if exclude not in name:\n","            if 'weight' in name:\n","                if method == 'xavier':\n","                    nn.init.xavier_normal_(w)\n","                elif method == 'kaiming':\n","                    nn.init.kaiming_normal_(w)\n","                else:\n","                    nn.init.normal_(w)\n","            elif 'bias' in name:\n","                nn.init.constant_(w, 0)\n","            else:\n","                pass\n","\n","\n","def train(config, model, train_iter, dev_iter, test_iter):\n","    start_time = time.time()\n","    model.train()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate,weight_decay=1e-6)\n","\n","    total_batch = 0  # 记录进行到多少batch\n","    dev_best_loss = float('inf')\n","    last_improve = 0  # 记录上次验证集loss下降的batch数\n","    flag = False  # 记录是否很久没有效果提升\n","    for epoch in range(config.num_epochs):\n","        print('Epoch [{}/{}]'.format(epoch + 1, config.num_epochs))\n","        # scheduler.step() # 学习率衰减\n","        for i, (trains, labels) in enumerate(train_iter):\n","            outputs = model(trains)\n","            model.zero_grad()\n","            loss = F.cross_entropy(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            if total_batch % 50 == 0:\n","                # 每多少轮输出在训练集和验证集上的效果\n","                true = labels.data.cpu()\n","                predic = torch.max(outputs.data, 1)[1].cpu()\n","                train_acc = metrics.accuracy_score(true, predic)\n","                dev_acc, dev_loss = evaluate(config, model, dev_iter)\n","                if dev_loss < dev_best_loss:\n","                    dev_best_loss = dev_loss\n","                    torch.save(model.state_dict(), config.save_path)\n","                    improve = '*'\n","                    last_improve = total_batch\n","                else:\n","                    improve = ''\n","                time_dif = get_time_dif(start_time)\n","                msg = 'Iter: {0:>6},  Train Loss: {1:>5.2},  Train Acc: {2:>6.2%},  Val Loss: {3:>5.2},  Val Acc: {4:>6.2%},  Time: {5} {6}'\n","                print(msg.format(total_batch, loss.item(), train_acc, dev_loss, dev_acc, time_dif, improve))\n","                model.train()\n","            total_batch += 1\n","            if total_batch - last_improve > config.require_improvement:\n","                # 验证集loss超过1000batch没下降，结束训练\n","                print(\"No optimization for a long time, auto-stopping...\")\n","                flag = True\n","                break\n","        if flag:\n","            break\n","    return None\n","    \n","def evaluate(config, model, data_iter, test=False):\n","    model.eval()\n","    loss_total = 0\n","    predict_all = np.array([], dtype=int)\n","    labels_all = np.array([], dtype=int)\n","    with torch.no_grad():\n","        for texts, labels in data_iter:\n","            outputs = model(texts)\n","            loss = F.cross_entropy(outputs, labels)\n","            loss_total += loss\n","            labels = labels.data.cpu().numpy()\n","            predic = torch.max(outputs.data, 1)[1].cpu().numpy()\n","            labels_all = np.append(labels_all, labels)\n","            predict_all = np.append(predict_all, predic)\n","    acc = metrics.accuracy_score(labels_all, predict_all)\n","    if test:\n","        f1 = metrics.f1_score(labels_all, predict_all,average='macro')\n","        return predict_all, f1\n","    return acc, loss_total / len(data_iter)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CgKb-tXQaV1C"},"outputs":[],"source":["def get_depth_f1(df,depth):\n","  if depth < 3:\n","    df_depth=df.loc[df['depth']==depth]\n","  else:\n","    df_depth=df.loc[df['depth']>=depth]\n","  f1 = metrics.f1_score(df_depth['label'].to_list(),df_depth['predicted'].to_list(),average='macro')\n","  return f1\n","\n","def test(config, model, test_iter, df):\n","# test\n","  model.load_state_dict(torch.load(config.save_path))\n","  model.eval()\n","  start_time = time.time()\n","  predict_all,  f1 = evaluate(config, model, test_iter, test=True)\n","  df['predicted'] = list(predict_all)\n","  return df, f1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rzrCHn9s1JqF"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","class CNN_Config(object):\n","    \"\"\"配置参数\"\"\"\n","    def __init__(self, dataset, embedding, df_train, df_test):\n","        self.model_name = 'Text_CNN'\n","        self.train_df = df_train  # 训练集\n","        self.dev_df = df_test  # 验证集\n","        self.test_df = df_test  # 测试集\n","        self.class_list = [0, 1, 2]  # 类别名单\n","        self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'# 模型训练结果          \n","        self.vocab_path = dataset + '/data/vocab.pkl' # 词表\n","        self.embedding_pretrained = torch.tensor(np.load(dataset + '/data/' + embedding)[\"embeddings\"].astype('float32')) # 预训练词向量\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')# 设备\n","        self.require_improvement = 500 # 若超过1000batch效果还没提升，则提前结束训练\n","        self.num_classes = len(self.class_list) # 类别数\n","        self.n_vocab = 0     # 词表大小，在运行时赋值\n","        self.num_epochs = 20   # epoch数\n","        self.batch_size = 16   # mini-batch大小\n","        self.pad_size = 64    # 每句话处理成的长度(短填长切)\n","        self.learning_rate = 2e-4 # 学习率\n","        self.embed = self.embedding_pretrained.size(1)if self.embedding_pretrained is not None else 300  # 字向量维度\n","        self.filter_sizes = (2, 3, 4) # 卷积核尺寸\n","        self.num_filters = 32 # 卷积核数量(channels数)\n","        self.dropout = 0.5  # 随机失活\n","\n","class Bi_LSTM_Att_Config(object):\n","    \"\"\"配置参数\"\"\"\n","    def __init__(self, dataset, embedding, df_train, df_test):\n","      self.model_name = 'Bi_LSTM'\n","      self.train_df = df_train  # 训练集\n","      self.dev_df = df_test  # 验证集\n","      self.test_df = df_test  # 测试集\n","      self.class_list = [0, 1, 2]  # 类别名单\n","      self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'# 模型训练结果          \n","      self.vocab_path = dataset + '/data/vocab.pkl' # 词表\n","      self.embedding_pretrained = torch.tensor(np.load(dataset + '/data/' + embedding)[\"embeddings\"].astype('float32')) # 预训练词向量\n","      self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')# 设备\n","      self.dropout = 0.5   # 随机失活\n","      self.require_improvement = 500 # 若超过1000batch效果还没提升，则提前结束训练\n","      self.num_classes = len(self.class_list) # 类别数\n","      self.n_vocab = 0  # 词表大小，在运行时赋值\n","      self.num_epochs = 20     # epoch数\n","      self.batch_size = 16     # mini-batch大小\n","      self.pad_size = 64      # 每句话处理成的长度(短填长切)\n","      self.learning_rate = 2e-4 # 学习率\n","      self.embed = self.embedding_pretrained.size(1)if self.embedding_pretrained is not None else 300  # 字向量维度\n","      self.hidden_size = 256 # lstm隐藏层\n","      self.num_layers = 2  # lstm层数\n","      self.hidden_size2 = 64"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XqiN_ces6xVi"},"outputs":[],"source":["class TextCNN(nn.Module):\n","    def __init__(self, config):\n","        super(TextCNN, self).__init__()\n","        if config.embedding_pretrained is not None:\n","            self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=False)\n","        else:\n","            self.embedding = nn.Embedding(config.n_vocab, config.embed, padding_idx=config.n_vocab - 1)\n","        self.convs = nn.ModuleList(\n","            [nn.Conv2d(1, config.num_filters, (k, config.embed)) for k in config.filter_sizes])\n","        self.dropout = nn.Dropout(config.dropout)\n","        self.fc = nn.Linear(config.num_filters * len(config.filter_sizes), config.num_classes)\n","\n","    def conv_and_pool(self, x, conv):\n","        x = F.relu(conv(x)).squeeze(3)\n","        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n","        return x\n","\n","    def forward(self, x):\n","        out = self.embedding(x[0])\n","        out = out.unsqueeze(1)\n","        out = torch.cat([self.conv_and_pool(out, conv) for conv in self.convs], 1)\n","        out = self.dropout(out)\n","        out = self.fc(out)\n","        return out\n","\n","class TAN(nn.Module):\n","    def __init__(self,config):\n","        super(TAN, self).__init__()\n","        self.embedding_dim = config.embed\n","        if config.embedding_pretrained is not None:\n","            self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=False)\n","        else:\n","            self.embedding = nn.Embedding(config.n_vocab, config.embed, padding_idx=config.n_vocab - 1)\n","        self.attention = nn.Linear(2*config.embed,1)\n","        self.lstm = nn.LSTM(config.embed, config.hidden_size, bidirectional=True)\n","        self.dropout = nn.Dropout(config.dropout)\n","        # self.fc = nn.Linear(2*config.hidden_size, config.num_classes)\n","        self.fc1 = nn.Linear(config.hidden_size * 2, config.hidden_size2)\n","        self.fc = nn.Linear(config.hidden_size2, config.num_classes)\n","        self.w = nn.Parameter(torch.zeros(config.embed * 2))\n","\n","    def forward(self,x):\n","        sentence,target, _ = x\n","        x_emb = self.embedding(sentence)# [batch_size, seq_len, embeding]\n","        t_emb = self.embedding(target)# [batch_size, seq_len, embeding]\n","        z = torch.sum(t_emb, dim=1) \n","        z = torch.div(z, x_emb.size()[1]) #[batch_size,  embeding]\n","        z = z.unsqueeze(1)    # (batch_size, 1, emb_dim)\n","        z = torch.tile(z, [1, x_emb.size()[1], 1]) # (batch_size, seq_len, embeding)      \n","        xt_emb = torch.cat((x_emb,z),dim=2)# (batch_size, seq_len, 2*embeding) \n","        # attention_layer\n","        a = F.softmax(torch.matmul(xt_emb, self.w), dim=1)\n","        a = a.unsqueeze(-1)  \n","        h, _ = self.lstm(x_emb)# (batch_size, seq_len, 2*hidden) \n","        out = h * a # (batch_size, seq_len, 2*hidden) \n","        # = torch.mean(out,dim=1)# (batch_size, 2*hidden)\n","        # out = self.fc(final_hidden_state)\n","        out = torch.sum(out, 1)  # [128, 256]\n","        out = F.relu(out)\n","        out = self.fc1(out)\n","        out = self.fc(out)  # [128, 64]\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":79871,"status":"ok","timestamp":1648734360532,"user":{"displayName":"刘耘果","userId":"11745284739417236882"},"user_tz":-480},"id":"JMOWAWYp0buU","outputId":"868431be-32ff-4fe8-931f-11975304fbb1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading data...\n","Vocab size: 11694\n","Time usage: 0:00:02\n","Epoch [1/20]\n","Iter:      0,  Train Loss:   1.1,  Train Acc: 56.25%,  Val Loss:   1.1,  Val Acc: 36.85%,  Time: 0:00:00 *\n","Iter:     50,  Train Loss:   1.1,  Train Acc: 43.75%,  Val Loss:   1.0,  Val Acc: 47.40%,  Time: 0:00:01 *\n","Iter:    100,  Train Loss:   1.0,  Train Acc: 56.25%,  Val Loss:   1.0,  Val Acc: 47.40%,  Time: 0:00:01 *\n","Iter:    150,  Train Loss:  0.88,  Train Acc: 56.25%,  Val Loss:   1.0,  Val Acc: 47.40%,  Time: 0:00:01 *\n","Iter:    200,  Train Loss:   1.0,  Train Acc: 56.25%,  Val Loss:   1.0,  Val Acc: 49.11%,  Time: 0:00:02 *\n","Iter:    250,  Train Loss:  0.97,  Train Acc: 56.25%,  Val Loss:  0.97,  Val Acc: 49.79%,  Time: 0:00:02 *\n","Epoch [2/20]\n","Iter:    300,  Train Loss:   0.9,  Train Acc: 62.50%,  Val Loss:  0.98,  Val Acc: 49.53%,  Time: 0:00:02 \n","Iter:    350,  Train Loss:  0.88,  Train Acc: 68.75%,  Val Loss:  0.97,  Val Acc: 49.62%,  Time: 0:00:03 *\n","Iter:    400,  Train Loss:   1.1,  Train Acc: 56.25%,  Val Loss:  0.96,  Val Acc: 49.62%,  Time: 0:00:04 *\n","Iter:    450,  Train Loss:  0.89,  Train Acc: 43.75%,  Val Loss:  0.95,  Val Acc: 49.70%,  Time: 0:00:04 *\n","Iter:    500,  Train Loss:   1.0,  Train Acc: 43.75%,  Val Loss:  0.95,  Val Acc: 50.30%,  Time: 0:00:04 *\n","Iter:    550,  Train Loss:   1.5,  Train Acc: 18.75%,  Val Loss:  0.93,  Val Acc: 52.43%,  Time: 0:00:05 *\n","Epoch [3/20]\n","Iter:    600,  Train Loss:  0.94,  Train Acc: 25.00%,  Val Loss:  0.93,  Val Acc: 52.60%,  Time: 0:00:05 \n","Iter:    650,  Train Loss:  0.87,  Train Acc: 62.50%,  Val Loss:  0.94,  Val Acc: 52.26%,  Time: 0:00:05 \n","Iter:    700,  Train Loss:   1.1,  Train Acc: 37.50%,  Val Loss:  0.91,  Val Acc: 52.94%,  Time: 0:00:06 *\n","Iter:    750,  Train Loss:   1.0,  Train Acc: 56.25%,  Val Loss:  0.91,  Val Acc: 53.62%,  Time: 0:00:06 *\n","Iter:    800,  Train Loss:  0.96,  Train Acc: 56.25%,  Val Loss:  0.89,  Val Acc: 54.04%,  Time: 0:00:07 *\n","Iter:    850,  Train Loss:   1.0,  Train Acc: 56.25%,  Val Loss:  0.89,  Val Acc: 54.47%,  Time: 0:00:07 *\n","Epoch [4/20]\n","Iter:    900,  Train Loss:  0.87,  Train Acc: 50.00%,  Val Loss:  0.89,  Val Acc: 54.13%,  Time: 0:00:07 \n","Iter:    950,  Train Loss:  0.67,  Train Acc: 68.75%,  Val Loss:  0.89,  Val Acc: 53.96%,  Time: 0:00:08 \n","Iter:   1000,  Train Loss:  0.72,  Train Acc: 62.50%,  Val Loss:  0.88,  Val Acc: 53.87%,  Time: 0:00:08 *\n","Iter:   1050,  Train Loss:   0.7,  Train Acc: 68.75%,  Val Loss:  0.88,  Val Acc: 54.47%,  Time: 0:00:08 *\n","Iter:   1100,  Train Loss:   1.0,  Train Acc: 43.75%,  Val Loss:  0.87,  Val Acc: 55.74%,  Time: 0:00:09 *\n","Iter:   1150,  Train Loss:  0.73,  Train Acc: 81.25%,  Val Loss:  0.87,  Val Acc: 56.09%,  Time: 0:00:09 *\n","Epoch [5/20]\n","Iter:   1200,  Train Loss:   0.6,  Train Acc: 75.00%,  Val Loss:  0.88,  Val Acc: 56.26%,  Time: 0:00:09 \n","Iter:   1250,  Train Loss:   0.8,  Train Acc: 56.25%,  Val Loss:  0.89,  Val Acc: 54.21%,  Time: 0:00:10 \n","Iter:   1300,  Train Loss:  0.78,  Train Acc: 75.00%,  Val Loss:  0.89,  Val Acc: 53.79%,  Time: 0:00:10 \n","Iter:   1350,  Train Loss:  0.81,  Train Acc: 62.50%,  Val Loss:  0.88,  Val Acc: 55.23%,  Time: 0:00:10 \n","Iter:   1400,  Train Loss:  0.71,  Train Acc: 87.50%,  Val Loss:  0.87,  Val Acc: 57.11%,  Time: 0:00:11 *\n","Iter:   1450,  Train Loss:  0.77,  Train Acc: 56.25%,  Val Loss:  0.87,  Val Acc: 56.34%,  Time: 0:00:11 *\n","Epoch [6/20]\n","Iter:   1500,  Train Loss:  0.75,  Train Acc: 62.50%,  Val Loss:  0.88,  Val Acc: 54.89%,  Time: 0:00:12 \n","Iter:   1550,  Train Loss:   0.9,  Train Acc: 68.75%,  Val Loss:  0.93,  Val Acc: 53.96%,  Time: 0:00:12 \n","Iter:   1600,  Train Loss:  0.94,  Train Acc: 62.50%,  Val Loss:  0.91,  Val Acc: 53.62%,  Time: 0:00:12 \n","Iter:   1650,  Train Loss:  0.96,  Train Acc: 43.75%,  Val Loss:  0.89,  Val Acc: 54.89%,  Time: 0:00:13 \n","Iter:   1700,  Train Loss:   0.6,  Train Acc: 75.00%,  Val Loss:  0.88,  Val Acc: 55.40%,  Time: 0:00:13 \n","Iter:   1750,  Train Loss:   0.8,  Train Acc: 56.25%,  Val Loss:  0.88,  Val Acc: 55.83%,  Time: 0:00:13 \n","Epoch [7/20]\n","Iter:   1800,  Train Loss:  0.56,  Train Acc: 87.50%,  Val Loss:  0.92,  Val Acc: 54.72%,  Time: 0:00:14 \n","Iter:   1850,  Train Loss:  0.54,  Train Acc: 81.25%,  Val Loss:  0.97,  Val Acc: 55.15%,  Time: 0:00:14 \n","Iter:   1900,  Train Loss:  0.92,  Train Acc: 62.50%,  Val Loss:  0.96,  Val Acc: 54.38%,  Time: 0:00:14 \n","Iter:   1950,  Train Loss:  0.57,  Train Acc: 81.25%,  Val Loss:  0.93,  Val Acc: 55.74%,  Time: 0:00:15 \n","No optimization for a long time, auto-stopping...\n","Loading data...\n","Vocab size: 11694\n","Time usage: 0:00:02\n","Epoch [1/20]\n","Iter:      0,  Train Loss:   1.1,  Train Acc: 37.50%,  Val Loss:   1.1,  Val Acc: 34.72%,  Time: 0:00:00 *\n","Iter:     50,  Train Loss:  0.96,  Train Acc: 56.25%,  Val Loss:   1.0,  Val Acc: 49.19%,  Time: 0:00:01 *\n","Iter:    100,  Train Loss:   1.1,  Train Acc: 31.25%,  Val Loss:   1.0,  Val Acc: 49.19%,  Time: 0:00:01 *\n","Iter:    150,  Train Loss:  0.89,  Train Acc: 62.50%,  Val Loss:   1.0,  Val Acc: 49.62%,  Time: 0:00:01 *\n","Iter:    200,  Train Loss:  0.84,  Train Acc: 43.75%,  Val Loss:  0.99,  Val Acc: 49.79%,  Time: 0:00:02 *\n","Iter:    250,  Train Loss:  0.97,  Train Acc: 50.00%,  Val Loss:  0.98,  Val Acc: 50.98%,  Time: 0:00:02 *\n","Epoch [2/20]\n","Iter:    300,  Train Loss:  0.87,  Train Acc: 50.00%,  Val Loss:  0.98,  Val Acc: 50.72%,  Time: 0:00:03 *\n","Iter:    350,  Train Loss:   1.0,  Train Acc: 62.50%,  Val Loss:  0.97,  Val Acc: 50.21%,  Time: 0:00:03 *\n","Iter:    400,  Train Loss:  0.95,  Train Acc: 37.50%,  Val Loss:  0.97,  Val Acc: 50.13%,  Time: 0:00:03 *\n","Iter:    450,  Train Loss:  0.88,  Train Acc: 56.25%,  Val Loss:  0.96,  Val Acc: 50.72%,  Time: 0:00:04 *\n","Iter:    500,  Train Loss:  0.79,  Train Acc: 62.50%,  Val Loss:  0.95,  Val Acc: 51.06%,  Time: 0:00:04 *\n","Iter:    550,  Train Loss:  0.94,  Train Acc: 37.50%,  Val Loss:  0.94,  Val Acc: 51.40%,  Time: 0:00:04 *\n","Epoch [3/20]\n","Iter:    600,  Train Loss:  0.88,  Train Acc: 43.75%,  Val Loss:  0.93,  Val Acc: 51.40%,  Time: 0:00:05 *\n","Iter:    650,  Train Loss:  0.84,  Train Acc: 68.75%,  Val Loss:  0.93,  Val Acc: 51.66%,  Time: 0:00:05 *\n","Iter:    700,  Train Loss:  0.78,  Train Acc: 68.75%,  Val Loss:  0.92,  Val Acc: 51.91%,  Time: 0:00:06 *\n","Iter:    750,  Train Loss:  0.76,  Train Acc: 56.25%,  Val Loss:  0.93,  Val Acc: 51.40%,  Time: 0:00:06 \n","Iter:    800,  Train Loss:  0.72,  Train Acc: 68.75%,  Val Loss:  0.91,  Val Acc: 54.38%,  Time: 0:00:06 *\n","Iter:    850,  Train Loss:   1.0,  Train Acc: 56.25%,  Val Loss:  0.89,  Val Acc: 54.38%,  Time: 0:00:07 *\n","Epoch [4/20]\n","Iter:    900,  Train Loss:   1.1,  Train Acc: 43.75%,  Val Loss:   0.9,  Val Acc: 52.43%,  Time: 0:00:07 \n","Iter:    950,  Train Loss:  0.71,  Train Acc: 62.50%,  Val Loss:  0.89,  Val Acc: 52.60%,  Time: 0:00:07 *\n","Iter:   1000,  Train Loss:   0.7,  Train Acc: 75.00%,  Val Loss:  0.89,  Val Acc: 53.45%,  Time: 0:00:08 \n","Iter:   1050,  Train Loss:  0.72,  Train Acc: 75.00%,  Val Loss:  0.89,  Val Acc: 54.81%,  Time: 0:00:08 \n","Iter:   1100,  Train Loss:  0.86,  Train Acc: 50.00%,  Val Loss:  0.88,  Val Acc: 55.91%,  Time: 0:00:08 *\n","Iter:   1150,  Train Loss:   0.7,  Train Acc: 93.75%,  Val Loss:  0.88,  Val Acc: 54.30%,  Time: 0:00:09 *\n","Epoch [5/20]\n","Iter:   1200,  Train Loss:  0.72,  Train Acc: 68.75%,  Val Loss:  0.89,  Val Acc: 52.34%,  Time: 0:00:09 \n","Iter:   1250,  Train Loss:  0.72,  Train Acc: 62.50%,  Val Loss:  0.88,  Val Acc: 54.81%,  Time: 0:00:10 *\n","Iter:   1300,  Train Loss:  0.78,  Train Acc: 62.50%,  Val Loss:  0.88,  Val Acc: 55.57%,  Time: 0:00:10 \n","Iter:   1350,  Train Loss:  0.79,  Train Acc: 75.00%,  Val Loss:  0.89,  Val Acc: 55.66%,  Time: 0:00:10 \n","Iter:   1400,  Train Loss:  0.73,  Train Acc: 50.00%,  Val Loss:  0.88,  Val Acc: 55.15%,  Time: 0:00:11 \n","Iter:   1450,  Train Loss:  0.82,  Train Acc: 56.25%,  Val Loss:  0.88,  Val Acc: 53.02%,  Time: 0:00:11 \n","Epoch [6/20]\n","Iter:   1500,  Train Loss:  0.69,  Train Acc: 62.50%,  Val Loss:  0.89,  Val Acc: 55.57%,  Time: 0:00:11 \n","Iter:   1550,  Train Loss:  0.67,  Train Acc: 62.50%,  Val Loss:  0.88,  Val Acc: 55.49%,  Time: 0:00:12 \n","Iter:   1600,  Train Loss:  0.78,  Train Acc: 68.75%,  Val Loss:   0.9,  Val Acc: 55.66%,  Time: 0:00:12 \n","Iter:   1650,  Train Loss:  0.58,  Train Acc: 62.50%,  Val Loss:   0.9,  Val Acc: 54.64%,  Time: 0:00:12 \n","Iter:   1700,  Train Loss:  0.75,  Train Acc: 68.75%,  Val Loss:  0.88,  Val Acc: 54.89%,  Time: 0:00:12 \n","Iter:   1750,  Train Loss:  0.68,  Train Acc: 68.75%,  Val Loss:  0.88,  Val Acc: 54.72%,  Time: 0:00:13 \n","No optimization for a long time, auto-stopping...\n","Loading data...\n","Vocab size: 11694\n","Time usage: 0:00:02\n","Epoch [1/20]\n","Iter:      0,  Train Loss:   1.1,  Train Acc: 25.00%,  Val Loss:   1.1,  Val Acc: 46.81%,  Time: 0:00:00 *\n","Iter:     50,  Train Loss:   0.9,  Train Acc: 68.75%,  Val Loss:   1.0,  Val Acc: 48.51%,  Time: 0:00:01 *\n","Iter:    100,  Train Loss:  0.93,  Train Acc: 56.25%,  Val Loss:  0.99,  Val Acc: 48.51%,  Time: 0:00:01 *\n","Iter:    150,  Train Loss:   1.2,  Train Acc: 25.00%,  Val Loss:  0.98,  Val Acc: 49.19%,  Time: 0:00:01 *\n","Iter:    200,  Train Loss:  0.89,  Train Acc: 50.00%,  Val Loss:  0.97,  Val Acc: 50.98%,  Time: 0:00:02 *\n","Iter:    250,  Train Loss:  0.79,  Train Acc: 62.50%,  Val Loss:  0.96,  Val Acc: 49.45%,  Time: 0:00:02 *\n","Epoch [2/20]\n","Iter:    300,  Train Loss:   1.1,  Train Acc: 12.50%,  Val Loss:  0.95,  Val Acc: 49.36%,  Time: 0:00:02 *\n","Iter:    350,  Train Loss:  0.78,  Train Acc: 62.50%,  Val Loss:  0.96,  Val Acc: 48.85%,  Time: 0:00:03 \n","Iter:    400,  Train Loss:   1.0,  Train Acc: 43.75%,  Val Loss:  0.95,  Val Acc: 50.04%,  Time: 0:00:03 *\n","Iter:    450,  Train Loss:   1.0,  Train Acc: 50.00%,  Val Loss:  0.94,  Val Acc: 49.79%,  Time: 0:00:04 *\n","Iter:    500,  Train Loss:   0.8,  Train Acc: 68.75%,  Val Loss:  0.93,  Val Acc: 50.81%,  Time: 0:00:04 *\n","Iter:    550,  Train Loss:  0.98,  Train Acc: 56.25%,  Val Loss:  0.93,  Val Acc: 49.96%,  Time: 0:00:04 \n","Epoch [3/20]\n","Iter:    600,  Train Loss:  0.81,  Train Acc: 68.75%,  Val Loss:  0.91,  Val Acc: 52.43%,  Time: 0:00:05 *\n","Iter:    650,  Train Loss:   1.4,  Train Acc: 37.50%,  Val Loss:  0.91,  Val Acc: 52.51%,  Time: 0:00:05 *\n","Iter:    700,  Train Loss:  0.89,  Train Acc: 62.50%,  Val Loss:   0.9,  Val Acc: 54.55%,  Time: 0:00:05 *\n","Iter:    750,  Train Loss:  0.91,  Train Acc: 56.25%,  Val Loss:  0.88,  Val Acc: 55.06%,  Time: 0:00:06 *\n","Iter:    800,  Train Loss:  0.72,  Train Acc: 62.50%,  Val Loss:  0.88,  Val Acc: 55.91%,  Time: 0:00:06 *\n","Iter:    850,  Train Loss:  0.72,  Train Acc: 62.50%,  Val Loss:  0.87,  Val Acc: 56.00%,  Time: 0:00:07 *\n","Epoch [4/20]\n","Iter:    900,  Train Loss:  0.94,  Train Acc: 43.75%,  Val Loss:  0.87,  Val Acc: 55.57%,  Time: 0:00:07 *\n","Iter:    950,  Train Loss:  0.73,  Train Acc: 68.75%,  Val Loss:  0.87,  Val Acc: 55.57%,  Time: 0:00:07 *\n","Iter:   1000,  Train Loss:  0.72,  Train Acc: 68.75%,  Val Loss:  0.87,  Val Acc: 57.28%,  Time: 0:00:08 *\n","Iter:   1050,  Train Loss:  0.74,  Train Acc: 68.75%,  Val Loss:  0.86,  Val Acc: 56.94%,  Time: 0:00:08 *\n","Iter:   1100,  Train Loss:  0.76,  Train Acc: 50.00%,  Val Loss:  0.86,  Val Acc: 56.51%,  Time: 0:00:09 *\n","Iter:   1150,  Train Loss:  0.77,  Train Acc: 68.75%,  Val Loss:  0.86,  Val Acc: 56.77%,  Time: 0:00:09 \n","Epoch [5/20]\n","Iter:   1200,  Train Loss:  0.77,  Train Acc: 81.25%,  Val Loss:  0.87,  Val Acc: 55.15%,  Time: 0:00:09 \n","Iter:   1250,  Train Loss:  0.89,  Train Acc: 75.00%,  Val Loss:  0.86,  Val Acc: 55.49%,  Time: 0:00:09 \n","Iter:   1300,  Train Loss:  0.95,  Train Acc: 56.25%,  Val Loss:  0.86,  Val Acc: 55.57%,  Time: 0:00:10 \n","Iter:   1350,  Train Loss:  0.61,  Train Acc: 81.25%,  Val Loss:  0.87,  Val Acc: 55.40%,  Time: 0:00:10 \n","Iter:   1400,  Train Loss:  0.79,  Train Acc: 62.50%,  Val Loss:  0.86,  Val Acc: 55.91%,  Time: 0:00:10 \n","Iter:   1450,  Train Loss:  0.55,  Train Acc: 81.25%,  Val Loss:  0.86,  Val Acc: 55.57%,  Time: 0:00:11 \n","Epoch [6/20]\n","Iter:   1500,  Train Loss:  0.67,  Train Acc: 75.00%,  Val Loss:  0.87,  Val Acc: 55.91%,  Time: 0:00:11 \n","Iter:   1550,  Train Loss:  0.83,  Train Acc: 50.00%,  Val Loss:  0.87,  Val Acc: 55.23%,  Time: 0:00:11 \n","Iter:   1600,  Train Loss:  0.59,  Train Acc: 81.25%,  Val Loss:  0.87,  Val Acc: 55.74%,  Time: 0:00:12 \n","No optimization for a long time, auto-stopping...\n","Loading data...\n","Vocab size: 11694\n","Time usage: 0:00:02\n","Epoch [1/20]\n","Iter:      0,  Train Loss:   1.1,  Train Acc: 31.25%,  Val Loss:   1.1,  Val Acc: 47.66%,  Time: 0:00:00 *\n","Iter:     50,  Train Loss:  0.88,  Train Acc: 50.00%,  Val Loss:   1.0,  Val Acc: 49.53%,  Time: 0:00:01 *\n","Iter:    100,  Train Loss:   0.9,  Train Acc: 62.50%,  Val Loss:   1.0,  Val Acc: 49.53%,  Time: 0:00:01 *\n","Iter:    150,  Train Loss:   1.1,  Train Acc: 43.75%,  Val Loss:  0.99,  Val Acc: 50.55%,  Time: 0:00:01 *\n","Iter:    200,  Train Loss:   1.0,  Train Acc: 37.50%,  Val Loss:  0.98,  Val Acc: 50.47%,  Time: 0:00:02 *\n","Iter:    250,  Train Loss:  0.91,  Train Acc: 50.00%,  Val Loss:  0.97,  Val Acc: 50.72%,  Time: 0:00:02 *\n","Epoch [2/20]\n","Iter:    300,  Train Loss:  0.85,  Train Acc: 62.50%,  Val Loss:  0.96,  Val Acc: 50.89%,  Time: 0:00:02 *\n","Iter:    350,  Train Loss:  0.86,  Train Acc: 56.25%,  Val Loss:  0.96,  Val Acc: 50.89%,  Time: 0:00:03 *\n","Iter:    400,  Train Loss:  0.88,  Train Acc: 37.50%,  Val Loss:  0.96,  Val Acc: 50.98%,  Time: 0:00:03 *\n","Iter:    450,  Train Loss:  0.96,  Train Acc: 62.50%,  Val Loss:  0.95,  Val Acc: 50.98%,  Time: 0:00:04 *\n","Iter:    500,  Train Loss:   0.9,  Train Acc: 62.50%,  Val Loss:  0.94,  Val Acc: 51.49%,  Time: 0:00:04 *\n","Iter:    550,  Train Loss:  0.81,  Train Acc: 50.00%,  Val Loss:  0.94,  Val Acc: 52.17%,  Time: 0:00:04 *\n","Epoch [3/20]\n","Iter:    600,  Train Loss:  0.91,  Train Acc: 56.25%,  Val Loss:  0.93,  Val Acc: 52.60%,  Time: 0:00:05 *\n","Iter:    650,  Train Loss:  0.87,  Train Acc: 50.00%,  Val Loss:  0.92,  Val Acc: 53.62%,  Time: 0:00:05 *\n","Iter:    700,  Train Loss:   0.9,  Train Acc: 37.50%,  Val Loss:  0.91,  Val Acc: 53.53%,  Time: 0:00:06 *\n","Iter:    750,  Train Loss:  0.89,  Train Acc: 56.25%,  Val Loss:  0.91,  Val Acc: 53.53%,  Time: 0:00:06 *\n","Iter:    800,  Train Loss:  0.96,  Train Acc: 43.75%,  Val Loss:   0.9,  Val Acc: 54.64%,  Time: 0:00:07 *\n","Iter:    850,  Train Loss:  0.97,  Train Acc: 62.50%,  Val Loss:   0.9,  Val Acc: 55.32%,  Time: 0:00:07 *\n","Epoch [4/20]\n","Iter:    900,  Train Loss:  0.78,  Train Acc: 56.25%,  Val Loss:  0.89,  Val Acc: 54.64%,  Time: 0:00:07 *\n","Iter:    950,  Train Loss:  0.73,  Train Acc: 62.50%,  Val Loss:  0.89,  Val Acc: 55.91%,  Time: 0:00:08 \n","Iter:   1000,  Train Loss:  0.89,  Train Acc: 50.00%,  Val Loss:   0.9,  Val Acc: 54.55%,  Time: 0:00:08 \n","Iter:   1050,  Train Loss:  0.79,  Train Acc: 50.00%,  Val Loss:   0.9,  Val Acc: 54.72%,  Time: 0:00:08 \n","Iter:   1100,  Train Loss:  0.68,  Train Acc: 68.75%,  Val Loss:  0.89,  Val Acc: 55.74%,  Time: 0:00:09 *\n","Iter:   1150,  Train Loss:  0.86,  Train Acc: 68.75%,  Val Loss:  0.89,  Val Acc: 54.98%,  Time: 0:00:09 *\n","Epoch [5/20]\n","Iter:   1200,  Train Loss:  0.94,  Train Acc: 50.00%,  Val Loss:  0.88,  Val Acc: 56.34%,  Time: 0:00:09 *\n","Iter:   1250,  Train Loss:  0.63,  Train Acc: 81.25%,  Val Loss:  0.89,  Val Acc: 55.74%,  Time: 0:00:10 \n","Iter:   1300,  Train Loss:  0.73,  Train Acc: 68.75%,  Val Loss:   0.9,  Val Acc: 54.81%,  Time: 0:00:10 \n","Iter:   1350,  Train Loss:  0.83,  Train Acc: 43.75%,  Val Loss:   0.9,  Val Acc: 55.74%,  Time: 0:00:10 \n","Iter:   1400,  Train Loss:   0.8,  Train Acc: 62.50%,  Val Loss:  0.89,  Val Acc: 56.09%,  Time: 0:00:11 \n","Iter:   1450,  Train Loss:  0.96,  Train Acc: 43.75%,  Val Loss:  0.89,  Val Acc: 55.40%,  Time: 0:00:11 \n","Epoch [6/20]\n","Iter:   1500,  Train Loss:  0.68,  Train Acc: 62.50%,  Val Loss:  0.88,  Val Acc: 56.26%,  Time: 0:00:11 *\n","Iter:   1550,  Train Loss:  0.57,  Train Acc: 75.00%,  Val Loss:   0.9,  Val Acc: 55.40%,  Time: 0:00:12 \n","Iter:   1600,  Train Loss:  0.78,  Train Acc: 68.75%,  Val Loss:   0.9,  Val Acc: 54.98%,  Time: 0:00:12 \n","Iter:   1650,  Train Loss:  0.86,  Train Acc: 43.75%,  Val Loss:  0.93,  Val Acc: 55.23%,  Time: 0:00:12 \n","Iter:   1700,  Train Loss:  0.69,  Train Acc: 56.25%,  Val Loss:  0.92,  Val Acc: 54.30%,  Time: 0:00:13 \n","Iter:   1750,  Train Loss:  0.93,  Train Acc: 56.25%,  Val Loss:  0.91,  Val Acc: 54.89%,  Time: 0:00:13 \n","Epoch [7/20]\n","Iter:   1800,  Train Loss:  0.56,  Train Acc: 81.25%,  Val Loss:   0.9,  Val Acc: 57.53%,  Time: 0:00:13 \n","Iter:   1850,  Train Loss:  0.45,  Train Acc: 87.50%,  Val Loss:  0.92,  Val Acc: 55.83%,  Time: 0:00:14 \n","Iter:   1900,  Train Loss:  0.54,  Train Acc: 75.00%,  Val Loss:  0.94,  Val Acc: 55.40%,  Time: 0:00:14 \n","Iter:   1950,  Train Loss:  0.59,  Train Acc: 75.00%,  Val Loss:  0.95,  Val Acc: 56.34%,  Time: 0:00:14 \n","Iter:   2000,  Train Loss:  0.65,  Train Acc: 68.75%,  Val Loss:  0.94,  Val Acc: 54.89%,  Time: 0:00:15 \n","No optimization for a long time, auto-stopping...\n","Loading data...\n","Vocab size: 11694\n","Time usage: 0:00:02\n","Epoch [1/20]\n","Iter:      0,  Train Loss:   1.1,  Train Acc: 43.75%,  Val Loss:   1.1,  Val Acc: 37.45%,  Time: 0:00:00 *\n","Iter:     50,  Train Loss:   1.1,  Train Acc: 50.00%,  Val Loss:  0.98,  Val Acc: 50.13%,  Time: 0:00:01 *\n","Iter:    100,  Train Loss:  0.98,  Train Acc: 43.75%,  Val Loss:  0.97,  Val Acc: 50.13%,  Time: 0:00:01 *\n","Iter:    150,  Train Loss:   1.1,  Train Acc: 37.50%,  Val Loss:  0.97,  Val Acc: 50.21%,  Time: 0:00:01 *\n","Iter:    200,  Train Loss:  0.93,  Train Acc: 56.25%,  Val Loss:  0.97,  Val Acc: 49.28%,  Time: 0:00:02 \n","Iter:    250,  Train Loss:   1.0,  Train Acc: 43.75%,  Val Loss:  0.96,  Val Acc: 50.55%,  Time: 0:00:02 *\n","Epoch [2/20]\n","Iter:    300,  Train Loss:  0.94,  Train Acc: 50.00%,  Val Loss:  0.95,  Val Acc: 50.04%,  Time: 0:00:02 *\n","Iter:    350,  Train Loss:   1.0,  Train Acc: 50.00%,  Val Loss:  0.94,  Val Acc: 50.38%,  Time: 0:00:03 *\n","Iter:    400,  Train Loss:  0.98,  Train Acc: 50.00%,  Val Loss:  0.93,  Val Acc: 50.72%,  Time: 0:00:04 *\n","Iter:    450,  Train Loss:   1.2,  Train Acc: 37.50%,  Val Loss:  0.93,  Val Acc: 51.40%,  Time: 0:00:04 *\n","Iter:    500,  Train Loss:  0.95,  Train Acc: 37.50%,  Val Loss:  0.93,  Val Acc: 50.55%,  Time: 0:00:04 \n","Iter:    550,  Train Loss:   0.8,  Train Acc: 50.00%,  Val Loss:  0.92,  Val Acc: 50.98%,  Time: 0:00:05 *\n","Epoch [3/20]\n","Iter:    600,  Train Loss:  0.84,  Train Acc: 62.50%,  Val Loss:  0.91,  Val Acc: 52.00%,  Time: 0:00:05 *\n","Iter:    650,  Train Loss:  0.76,  Train Acc: 87.50%,  Val Loss:  0.91,  Val Acc: 52.26%,  Time: 0:00:05 \n","Iter:    700,  Train Loss:  0.82,  Train Acc: 68.75%,  Val Loss:  0.89,  Val Acc: 52.68%,  Time: 0:00:06 *\n","Iter:    750,  Train Loss:  0.68,  Train Acc: 81.25%,  Val Loss:   0.9,  Val Acc: 53.19%,  Time: 0:00:06 \n","Iter:    800,  Train Loss:   1.0,  Train Acc: 43.75%,  Val Loss:  0.88,  Val Acc: 54.38%,  Time: 0:00:07 *\n","Iter:    850,  Train Loss:   0.8,  Train Acc: 56.25%,  Val Loss:  0.87,  Val Acc: 54.21%,  Time: 0:00:07 *\n","Epoch [4/20]\n","Iter:    900,  Train Loss:  0.77,  Train Acc: 62.50%,  Val Loss:  0.87,  Val Acc: 54.81%,  Time: 0:00:07 \n","Iter:    950,  Train Loss:  0.87,  Train Acc: 68.75%,  Val Loss:  0.89,  Val Acc: 52.09%,  Time: 0:00:08 \n","Iter:   1000,  Train Loss:  0.79,  Train Acc: 62.50%,  Val Loss:  0.86,  Val Acc: 55.06%,  Time: 0:00:08 *\n","Iter:   1050,  Train Loss:   1.1,  Train Acc: 37.50%,  Val Loss:  0.88,  Val Acc: 54.13%,  Time: 0:00:08 \n","Iter:   1100,  Train Loss:   1.0,  Train Acc: 56.25%,  Val Loss:  0.86,  Val Acc: 54.98%,  Time: 0:00:09 \n","Iter:   1150,  Train Loss:  0.85,  Train Acc: 43.75%,  Val Loss:  0.85,  Val Acc: 55.91%,  Time: 0:00:09 *\n","Epoch [5/20]\n","Iter:   1200,  Train Loss:  0.75,  Train Acc: 68.75%,  Val Loss:  0.86,  Val Acc: 55.49%,  Time: 0:00:10 \n","Iter:   1250,  Train Loss:   1.0,  Train Acc: 25.00%,  Val Loss:  0.86,  Val Acc: 55.57%,  Time: 0:00:10 \n","Iter:   1300,  Train Loss:  0.88,  Train Acc: 62.50%,  Val Loss:  0.85,  Val Acc: 56.34%,  Time: 0:00:10 *\n","Iter:   1350,  Train Loss:  0.75,  Train Acc: 68.75%,  Val Loss:  0.86,  Val Acc: 55.49%,  Time: 0:00:11 \n","Iter:   1400,  Train Loss:   0.8,  Train Acc: 56.25%,  Val Loss:  0.85,  Val Acc: 55.32%,  Time: 0:00:11 \n","Iter:   1450,  Train Loss:  0.82,  Train Acc: 50.00%,  Val Loss:  0.85,  Val Acc: 57.70%,  Time: 0:00:11 \n","Epoch [6/20]\n","Iter:   1500,  Train Loss:  0.71,  Train Acc: 68.75%,  Val Loss:  0.84,  Val Acc: 56.85%,  Time: 0:00:12 *\n","Iter:   1550,  Train Loss:  0.57,  Train Acc: 81.25%,  Val Loss:  0.84,  Val Acc: 56.85%,  Time: 0:00:12 \n","Iter:   1600,  Train Loss:  0.56,  Train Acc: 68.75%,  Val Loss:  0.85,  Val Acc: 56.60%,  Time: 0:00:12 \n","Iter:   1650,  Train Loss:  0.62,  Train Acc: 68.75%,  Val Loss:  0.86,  Val Acc: 56.34%,  Time: 0:00:13 \n","Iter:   1700,  Train Loss:  0.53,  Train Acc: 68.75%,  Val Loss:  0.85,  Val Acc: 57.45%,  Time: 0:00:13 \n","Iter:   1750,  Train Loss:  0.61,  Train Acc: 68.75%,  Val Loss:  0.86,  Val Acc: 57.11%,  Time: 0:00:13 \n","Epoch [7/20]\n","Iter:   1800,  Train Loss:  0.64,  Train Acc: 68.75%,  Val Loss:  0.86,  Val Acc: 56.17%,  Time: 0:00:14 \n","Iter:   1850,  Train Loss:   0.6,  Train Acc: 81.25%,  Val Loss:  0.86,  Val Acc: 57.11%,  Time: 0:00:14 \n","Iter:   1900,  Train Loss:   1.0,  Train Acc: 62.50%,  Val Loss:  0.87,  Val Acc: 57.36%,  Time: 0:00:14 \n","Iter:   1950,  Train Loss:  0.36,  Train Acc: 93.75%,  Val Loss:  0.91,  Val Acc: 54.89%,  Time: 0:00:15 \n","Iter:   2000,  Train Loss:  0.59,  Train Acc: 75.00%,  Val Loss:  0.88,  Val Acc: 57.11%,  Time: 0:00:15 \n","No optimization for a long time, auto-stopping...\n","f: 0.529750544037204+0.012396576714650753\n","f1: 0.5286056413572779+0.03930121899690105\n","f2: 0.5191057724589931+0.014208150113466817\n","f3: 0.4534374986084019+0.012234806232011236\n"]}],"source":["# coding: UTF-8\n","import time\n","import torch\n","import numpy as np\n","\n","embedding = 'embedding_CantoStance.npz'\n","model_name = 'Tan' \n","\n","f=list()\n","f1=list()\n","f2=list()\n","f3=list()\n","\n","for i in range(5):\n","    dataset='/content/drive/MyDrive/Prediction'\n","    df = pd.concat([df2,df_info])\n","    df = df.sample(frac=1, random_state=i)   \n","    cut_idx1 = int(round(0.8 * df.shape[0]))\n","    df_train,df_test = df[:cut_idx1], df[cut_idx1:]\n","\n","    config = Bi_LSTM_Att_Config(dataset, embedding,df_train,df_test)\n","    np.random.seed(1)\n","    torch.manual_seed(1)\n","    torch.cuda.manual_seed_all(1)\n","    torch.backends.cudnn.deterministic = True  # 保证每次结果一样\n","\n","    start_time = time.time()\n","    print(\"Loading data...\")\n","    vocab, train_data, test_data = build_dataset(config)\n","    train_iter = build_iterator(train_data, config,)\n","    dev_iter = build_iterator(test_data, config,)\n","    test_iter = build_iterator(test_data, config)\n","    time_dif = get_time_dif(start_time)\n","    print(\"Time usage:\", time_dif)\n","\n","    config.n_vocab = len(vocab)\n","    model = TAN(config).to(config.device)\n","    init_network(model)\n","    train(config, model, train_iter, dev_iter, test_iter)\n","    df_test,f_score = test(config, model, test_iter, df_test)\n","    f.append(f_score)\n","    f1.append(get_depth_f1(df_test,1))\n","    f2.append(get_depth_f1(df_test,2))\n","    f3.append(get_depth_f1(df_test,3))\n","\n","\n","print(\"f: \"+str(np.mean(f))+\"+\"+str(np.std(f, ddof = 1)))\n","print(\"f1: \"+str(np.mean(f1))+\"+\"+str(np.std(f1, ddof = 1)))\n","print(\"f2: \"+str(np.mean(f2))+\"+\"+str(np.std(f2, ddof = 1)))\n","print(\"f3: \"+str(np.mean(f3))+\"+\"+str(np.std(f3, ddof = 1)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2985167,"status":"ok","timestamp":1648733902247,"user":{"displayName":"刘耘果","userId":"11745284739417236882"},"user_tz":-480},"id":"rjYn67LyiUgy","outputId":"65cc1abd-4736-45f8-fa94-2aae77980cfc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading data...\n","Vocab size: 11694\n","Time usage: 0:00:02\n","Epoch [1/20]\n","Iter:      0,  Train Loss:   1.1,  Train Acc: 31.25%,  Val Loss:   1.1,  Val Acc: 40.26%,  Time: 0:00:00 *\n","Iter:     50,  Train Loss:   1.1,  Train Acc: 37.50%,  Val Loss:   1.0,  Val Acc: 47.40%,  Time: 0:00:14 *\n","Iter:    100,  Train Loss:  0.99,  Train Acc: 50.00%,  Val Loss:  0.98,  Val Acc: 48.94%,  Time: 0:00:27 *\n","Iter:    150,  Train Loss:  0.77,  Train Acc: 75.00%,  Val Loss:  0.96,  Val Acc: 51.74%,  Time: 0:00:40 *\n","Iter:    200,  Train Loss:   1.1,  Train Acc: 31.25%,  Val Loss:  0.96,  Val Acc: 51.40%,  Time: 0:00:54 *\n","Iter:    250,  Train Loss:  0.81,  Train Acc: 56.25%,  Val Loss:  0.94,  Val Acc: 53.62%,  Time: 0:01:07 *\n","Epoch [2/20]\n","Iter:    300,  Train Loss:  0.82,  Train Acc: 68.75%,  Val Loss:  0.94,  Val Acc: 51.49%,  Time: 0:01:21 \n","Iter:    350,  Train Loss:  0.89,  Train Acc: 75.00%,  Val Loss:  0.93,  Val Acc: 53.53%,  Time: 0:01:34 *\n","Iter:    400,  Train Loss:  0.98,  Train Acc: 50.00%,  Val Loss:  0.93,  Val Acc: 53.45%,  Time: 0:01:47 *\n","Iter:    450,  Train Loss:  0.87,  Train Acc: 43.75%,  Val Loss:  0.92,  Val Acc: 54.21%,  Time: 0:02:01 *\n","Iter:    500,  Train Loss:  0.98,  Train Acc: 43.75%,  Val Loss:  0.92,  Val Acc: 53.53%,  Time: 0:02:14 *\n","Iter:    550,  Train Loss:   1.2,  Train Acc: 43.75%,  Val Loss:   0.9,  Val Acc: 55.83%,  Time: 0:02:27 *\n","Epoch [3/20]\n","Iter:    600,  Train Loss:  0.92,  Train Acc: 31.25%,  Val Loss:  0.91,  Val Acc: 54.04%,  Time: 0:02:41 \n","Iter:    650,  Train Loss:  0.82,  Train Acc: 68.75%,  Val Loss:   0.9,  Val Acc: 54.98%,  Time: 0:02:55 \n","Iter:    700,  Train Loss:   1.2,  Train Acc: 43.75%,  Val Loss:   0.9,  Val Acc: 54.81%,  Time: 0:03:08 *\n","Iter:    750,  Train Loss:   0.9,  Train Acc: 56.25%,  Val Loss:  0.89,  Val Acc: 55.15%,  Time: 0:03:21 *\n","Iter:    800,  Train Loss:  0.97,  Train Acc: 62.50%,  Val Loss:  0.89,  Val Acc: 55.06%,  Time: 0:03:34 \n","Iter:    850,  Train Loss:  0.95,  Train Acc: 62.50%,  Val Loss:  0.88,  Val Acc: 56.77%,  Time: 0:03:48 *\n","Epoch [4/20]\n","Iter:    900,  Train Loss:  0.76,  Train Acc: 62.50%,  Val Loss:   0.9,  Val Acc: 54.64%,  Time: 0:04:02 \n","Iter:    950,  Train Loss:   0.7,  Train Acc: 56.25%,  Val Loss:   0.9,  Val Acc: 54.47%,  Time: 0:04:15 \n","Iter:   1000,  Train Loss:  0.57,  Train Acc: 81.25%,  Val Loss:  0.89,  Val Acc: 55.32%,  Time: 0:04:28 \n","Iter:   1050,  Train Loss:  0.78,  Train Acc: 56.25%,  Val Loss:  0.88,  Val Acc: 55.83%,  Time: 0:04:42 *\n","Iter:   1100,  Train Loss:   1.1,  Train Acc: 37.50%,  Val Loss:  0.88,  Val Acc: 56.85%,  Time: 0:04:55 \n","Iter:   1150,  Train Loss:  0.81,  Train Acc: 62.50%,  Val Loss:  0.88,  Val Acc: 57.19%,  Time: 0:05:08 *\n","Epoch [5/20]\n","Iter:   1200,  Train Loss:  0.49,  Train Acc: 81.25%,  Val Loss:  0.88,  Val Acc: 56.51%,  Time: 0:05:22 \n","Iter:   1250,  Train Loss:  0.68,  Train Acc: 93.75%,  Val Loss:  0.88,  Val Acc: 56.94%,  Time: 0:05:35 \n","Iter:   1300,  Train Loss:  0.78,  Train Acc: 62.50%,  Val Loss:  0.88,  Val Acc: 56.43%,  Time: 0:05:48 \n","Iter:   1350,  Train Loss:  0.86,  Train Acc: 50.00%,  Val Loss:  0.87,  Val Acc: 57.36%,  Time: 0:06:02 *\n","Iter:   1400,  Train Loss:  0.69,  Train Acc: 81.25%,  Val Loss:  0.87,  Val Acc: 57.96%,  Time: 0:06:15 \n","Iter:   1450,  Train Loss:   0.8,  Train Acc: 50.00%,  Val Loss:  0.87,  Val Acc: 58.04%,  Time: 0:06:28 *\n","Epoch [6/20]\n","Iter:   1500,  Train Loss:  0.62,  Train Acc: 68.75%,  Val Loss:  0.88,  Val Acc: 57.79%,  Time: 0:06:42 \n","Iter:   1550,  Train Loss:  0.92,  Train Acc: 43.75%,  Val Loss:  0.88,  Val Acc: 57.28%,  Time: 0:06:55 \n","Iter:   1600,  Train Loss:  0.89,  Train Acc: 62.50%,  Val Loss:  0.88,  Val Acc: 56.77%,  Time: 0:07:09 \n","Iter:   1650,  Train Loss:  0.77,  Train Acc: 50.00%,  Val Loss:  0.87,  Val Acc: 57.79%,  Time: 0:07:22 *\n","Iter:   1700,  Train Loss:  0.71,  Train Acc: 68.75%,  Val Loss:  0.87,  Val Acc: 58.04%,  Time: 0:07:35 \n","Iter:   1750,  Train Loss:  0.95,  Train Acc: 68.75%,  Val Loss:  0.87,  Val Acc: 58.81%,  Time: 0:07:49 *\n","Epoch [7/20]\n","Iter:   1800,  Train Loss:  0.55,  Train Acc: 87.50%,  Val Loss:  0.88,  Val Acc: 56.68%,  Time: 0:08:02 \n","Iter:   1850,  Train Loss:  0.67,  Train Acc: 75.00%,  Val Loss:  0.87,  Val Acc: 57.70%,  Time: 0:08:16 \n","Iter:   1900,  Train Loss:   0.7,  Train Acc: 68.75%,  Val Loss:  0.89,  Val Acc: 56.51%,  Time: 0:08:29 \n","Iter:   1950,  Train Loss:  0.63,  Train Acc: 56.25%,  Val Loss:  0.87,  Val Acc: 56.68%,  Time: 0:08:42 \n","Iter:   2000,  Train Loss:  0.88,  Train Acc: 68.75%,  Val Loss:  0.87,  Val Acc: 58.13%,  Time: 0:08:56 \n","Iter:   2050,  Train Loss:  0.62,  Train Acc: 75.00%,  Val Loss:  0.87,  Val Acc: 57.53%,  Time: 0:09:09 \n","Epoch [8/20]\n","Iter:   2100,  Train Loss:   0.5,  Train Acc: 81.25%,  Val Loss:  0.88,  Val Acc: 56.94%,  Time: 0:09:23 \n","Iter:   2150,  Train Loss:  0.67,  Train Acc: 75.00%,  Val Loss:  0.88,  Val Acc: 56.17%,  Time: 0:09:36 \n","Iter:   2200,  Train Loss:  0.65,  Train Acc: 68.75%,  Val Loss:  0.89,  Val Acc: 56.68%,  Time: 0:09:49 \n","Iter:   2250,  Train Loss:  0.53,  Train Acc: 81.25%,  Val Loss:  0.88,  Val Acc: 57.45%,  Time: 0:10:02 \n","No optimization for a long time, auto-stopping...\n","Loading data...\n","Vocab size: 11694\n","Time usage: 0:00:02\n","Epoch [1/20]\n","Iter:      0,  Train Loss:   1.1,  Train Acc: 25.00%,  Val Loss:   1.1,  Val Acc: 34.81%,  Time: 0:00:00 *\n","Iter:     50,  Train Loss:  0.99,  Train Acc: 50.00%,  Val Loss:   1.0,  Val Acc: 49.36%,  Time: 0:00:14 *\n","Iter:    100,  Train Loss:   1.1,  Train Acc: 50.00%,  Val Loss:  0.97,  Val Acc: 51.49%,  Time: 0:00:27 *\n","Iter:    150,  Train Loss:  0.99,  Train Acc: 31.25%,  Val Loss:  0.96,  Val Acc: 52.00%,  Time: 0:00:40 *\n","Iter:    200,  Train Loss:  0.76,  Train Acc: 62.50%,  Val Loss:  0.95,  Val Acc: 52.94%,  Time: 0:00:54 *\n","Iter:    250,  Train Loss:   1.0,  Train Acc: 50.00%,  Val Loss:  0.94,  Val Acc: 53.53%,  Time: 0:01:07 *\n","Epoch [2/20]\n","Iter:    300,  Train Loss:  0.76,  Train Acc: 56.25%,  Val Loss:  0.93,  Val Acc: 52.94%,  Time: 0:01:21 *\n","Iter:    350,  Train Loss:  0.94,  Train Acc: 56.25%,  Val Loss:  0.92,  Val Acc: 53.87%,  Time: 0:01:34 *\n","Iter:    400,  Train Loss:  0.88,  Train Acc: 43.75%,  Val Loss:  0.91,  Val Acc: 54.72%,  Time: 0:01:48 *\n","Iter:    450,  Train Loss:  0.72,  Train Acc: 68.75%,  Val Loss:  0.91,  Val Acc: 54.55%,  Time: 0:02:01 \n","Iter:    500,  Train Loss:  0.87,  Train Acc: 56.25%,  Val Loss:  0.91,  Val Acc: 54.55%,  Time: 0:02:14 *\n","Iter:    550,  Train Loss:  0.99,  Train Acc: 31.25%,  Val Loss:   0.9,  Val Acc: 54.55%,  Time: 0:02:27 *\n","Epoch [3/20]\n","Iter:    600,  Train Loss:  0.82,  Train Acc: 56.25%,  Val Loss:  0.89,  Val Acc: 54.64%,  Time: 0:02:41 *\n","Iter:    650,  Train Loss:  0.69,  Train Acc: 87.50%,  Val Loss:  0.89,  Val Acc: 54.21%,  Time: 0:02:55 \n","Iter:    700,  Train Loss:  0.72,  Train Acc: 56.25%,  Val Loss:   0.9,  Val Acc: 54.47%,  Time: 0:03:08 \n","Iter:    750,  Train Loss:   0.8,  Train Acc: 56.25%,  Val Loss:  0.89,  Val Acc: 56.09%,  Time: 0:03:21 *\n","Iter:    800,  Train Loss:  0.66,  Train Acc: 75.00%,  Val Loss:  0.89,  Val Acc: 55.91%,  Time: 0:03:35 *\n","Iter:    850,  Train Loss:   0.9,  Train Acc: 75.00%,  Val Loss:  0.88,  Val Acc: 55.23%,  Time: 0:03:48 *\n","Epoch [4/20]\n","Iter:    900,  Train Loss:   1.1,  Train Acc: 56.25%,  Val Loss:  0.88,  Val Acc: 55.06%,  Time: 0:04:02 *\n","Iter:    950,  Train Loss:  0.76,  Train Acc: 75.00%,  Val Loss:  0.88,  Val Acc: 55.06%,  Time: 0:04:15 \n","Iter:   1000,  Train Loss:  0.93,  Train Acc: 56.25%,  Val Loss:  0.88,  Val Acc: 54.89%,  Time: 0:04:28 \n","Iter:   1050,  Train Loss:  0.64,  Train Acc: 81.25%,  Val Loss:  0.88,  Val Acc: 54.98%,  Time: 0:04:42 \n","Iter:   1100,  Train Loss:  0.83,  Train Acc: 50.00%,  Val Loss:  0.88,  Val Acc: 55.91%,  Time: 0:04:55 \n","Iter:   1150,  Train Loss:  0.72,  Train Acc: 68.75%,  Val Loss:  0.87,  Val Acc: 56.68%,  Time: 0:05:08 *\n","Epoch [5/20]\n","Iter:   1200,  Train Loss:  0.66,  Train Acc: 68.75%,  Val Loss:  0.87,  Val Acc: 56.85%,  Time: 0:05:22 *\n","Iter:   1250,  Train Loss:  0.68,  Train Acc: 68.75%,  Val Loss:  0.87,  Val Acc: 56.09%,  Time: 0:05:35 \n","Iter:   1300,  Train Loss:  0.85,  Train Acc: 68.75%,  Val Loss:  0.87,  Val Acc: 56.09%,  Time: 0:05:49 \n","Iter:   1350,  Train Loss:  0.76,  Train Acc: 68.75%,  Val Loss:  0.88,  Val Acc: 57.19%,  Time: 0:06:02 \n","Iter:   1400,  Train Loss:  0.75,  Train Acc: 56.25%,  Val Loss:  0.87,  Val Acc: 56.60%,  Time: 0:06:15 \n","Iter:   1450,  Train Loss:  0.69,  Train Acc: 56.25%,  Val Loss:  0.87,  Val Acc: 56.43%,  Time: 0:06:29 *\n","Epoch [6/20]\n","Iter:   1500,  Train Loss:  0.65,  Train Acc: 81.25%,  Val Loss:  0.86,  Val Acc: 57.36%,  Time: 0:06:43 *\n","Iter:   1550,  Train Loss:  0.54,  Train Acc: 81.25%,  Val Loss:  0.86,  Val Acc: 56.09%,  Time: 0:06:56 \n","Iter:   1600,  Train Loss:  0.92,  Train Acc: 56.25%,  Val Loss:  0.87,  Val Acc: 57.53%,  Time: 0:07:09 \n","Iter:   1650,  Train Loss:  0.79,  Train Acc: 50.00%,  Val Loss:  0.88,  Val Acc: 56.51%,  Time: 0:07:22 \n","Iter:   1700,  Train Loss:  0.63,  Train Acc: 75.00%,  Val Loss:  0.87,  Val Acc: 56.34%,  Time: 0:07:36 \n","Iter:   1750,  Train Loss:   0.5,  Train Acc: 81.25%,  Val Loss:  0.86,  Val Acc: 57.11%,  Time: 0:07:49 \n","Epoch [7/20]\n","Iter:   1800,  Train Loss:  0.71,  Train Acc: 62.50%,  Val Loss:  0.86,  Val Acc: 57.36%,  Time: 0:08:03 *\n","Iter:   1850,  Train Loss:  0.69,  Train Acc: 68.75%,  Val Loss:  0.86,  Val Acc: 57.11%,  Time: 0:08:16 \n","Iter:   1900,  Train Loss:  0.68,  Train Acc: 68.75%,  Val Loss:  0.88,  Val Acc: 55.57%,  Time: 0:08:29 \n","Iter:   1950,  Train Loss:  0.75,  Train Acc: 50.00%,  Val Loss:  0.87,  Val Acc: 56.77%,  Time: 0:08:43 \n","Iter:   2000,  Train Loss:  0.73,  Train Acc: 62.50%,  Val Loss:  0.88,  Val Acc: 56.43%,  Time: 0:08:56 \n","Iter:   2050,  Train Loss:  0.41,  Train Acc: 87.50%,  Val Loss:  0.87,  Val Acc: 56.77%,  Time: 0:09:09 \n","Epoch [8/20]\n","Iter:   2100,  Train Loss:  0.55,  Train Acc: 68.75%,  Val Loss:  0.87,  Val Acc: 57.11%,  Time: 0:09:23 \n","Iter:   2150,  Train Loss:  0.66,  Train Acc: 75.00%,  Val Loss:  0.87,  Val Acc: 57.45%,  Time: 0:09:36 \n","Iter:   2200,  Train Loss:  0.53,  Train Acc: 81.25%,  Val Loss:  0.88,  Val Acc: 55.83%,  Time: 0:09:50 \n","Iter:   2250,  Train Loss:  0.48,  Train Acc: 75.00%,  Val Loss:  0.88,  Val Acc: 56.77%,  Time: 0:10:03 \n","Iter:   2300,  Train Loss:  0.57,  Train Acc: 81.25%,  Val Loss:  0.88,  Val Acc: 57.02%,  Time: 0:10:16 \n","No optimization for a long time, auto-stopping...\n","Loading data...\n","Vocab size: 11694\n","Time usage: 0:00:02\n","Epoch [1/20]\n","Iter:      0,  Train Loss:   1.1,  Train Acc: 37.50%,  Val Loss:   1.1,  Val Acc: 42.21%,  Time: 0:00:00 *\n","Iter:     50,  Train Loss:  0.84,  Train Acc: 75.00%,  Val Loss:  0.97,  Val Acc: 50.04%,  Time: 0:00:14 *\n","Iter:    100,  Train Loss:  0.91,  Train Acc: 50.00%,  Val Loss:  0.96,  Val Acc: 51.49%,  Time: 0:00:27 *\n","Iter:    150,  Train Loss:   1.0,  Train Acc: 31.25%,  Val Loss:  0.94,  Val Acc: 54.21%,  Time: 0:00:40 *\n","Iter:    200,  Train Loss:  0.91,  Train Acc: 43.75%,  Val Loss:  0.93,  Val Acc: 54.98%,  Time: 0:00:54 *\n","Iter:    250,  Train Loss:  0.81,  Train Acc: 62.50%,  Val Loss:  0.92,  Val Acc: 52.09%,  Time: 0:01:07 *\n","Epoch [2/20]\n","Iter:    300,  Train Loss:   1.0,  Train Acc: 25.00%,  Val Loss:  0.91,  Val Acc: 53.96%,  Time: 0:01:21 *\n","Iter:    350,  Train Loss:  0.67,  Train Acc: 75.00%,  Val Loss:  0.91,  Val Acc: 54.64%,  Time: 0:01:34 \n","Iter:    400,  Train Loss:   1.0,  Train Acc: 56.25%,  Val Loss:   0.9,  Val Acc: 54.81%,  Time: 0:01:48 *\n","Iter:    450,  Train Loss:   1.0,  Train Acc: 37.50%,  Val Loss:   0.9,  Val Acc: 56.09%,  Time: 0:02:01 *\n","Iter:    500,  Train Loss:  0.84,  Train Acc: 50.00%,  Val Loss:  0.89,  Val Acc: 55.23%,  Time: 0:02:14 *\n","Iter:    550,  Train Loss:  0.93,  Train Acc: 56.25%,  Val Loss:  0.89,  Val Acc: 54.21%,  Time: 0:02:28 \n","Epoch [3/20]\n","Iter:    600,  Train Loss:  0.61,  Train Acc: 93.75%,  Val Loss:  0.88,  Val Acc: 56.34%,  Time: 0:02:41 *\n","Iter:    650,  Train Loss:   1.4,  Train Acc: 31.25%,  Val Loss:  0.88,  Val Acc: 55.57%,  Time: 0:02:55 \n","Iter:    700,  Train Loss:  0.87,  Train Acc: 68.75%,  Val Loss:  0.88,  Val Acc: 55.91%,  Time: 0:03:08 *\n","Iter:    750,  Train Loss:  0.92,  Train Acc: 37.50%,  Val Loss:  0.87,  Val Acc: 56.68%,  Time: 0:03:21 *\n","Iter:    800,  Train Loss:   0.7,  Train Acc: 68.75%,  Val Loss:  0.87,  Val Acc: 57.79%,  Time: 0:03:35 *\n","Iter:    850,  Train Loss:  0.75,  Train Acc: 50.00%,  Val Loss:  0.87,  Val Acc: 57.36%,  Time: 0:03:48 \n","Epoch [4/20]\n","Iter:    900,  Train Loss:  0.93,  Train Acc: 37.50%,  Val Loss:  0.87,  Val Acc: 57.70%,  Time: 0:04:02 \n","Iter:    950,  Train Loss:  0.65,  Train Acc: 81.25%,  Val Loss:  0.87,  Val Acc: 56.94%,  Time: 0:04:15 *\n","Iter:   1000,  Train Loss:  0.69,  Train Acc: 81.25%,  Val Loss:  0.87,  Val Acc: 57.62%,  Time: 0:04:28 *\n","Iter:   1050,  Train Loss:  0.75,  Train Acc: 68.75%,  Val Loss:  0.86,  Val Acc: 57.96%,  Time: 0:04:42 *\n","Iter:   1100,  Train Loss:  0.66,  Train Acc: 75.00%,  Val Loss:  0.86,  Val Acc: 58.98%,  Time: 0:04:55 *\n","Iter:   1150,  Train Loss:  0.66,  Train Acc: 87.50%,  Val Loss:  0.86,  Val Acc: 58.21%,  Time: 0:05:08 \n","Epoch [5/20]\n","Iter:   1200,  Train Loss:  0.67,  Train Acc: 75.00%,  Val Loss:  0.86,  Val Acc: 58.55%,  Time: 0:05:22 \n","Iter:   1250,  Train Loss:  0.79,  Train Acc: 75.00%,  Val Loss:  0.86,  Val Acc: 58.30%,  Time: 0:05:36 \n","Iter:   1300,  Train Loss:   1.0,  Train Acc: 37.50%,  Val Loss:  0.86,  Val Acc: 58.21%,  Time: 0:05:49 \n","Iter:   1350,  Train Loss:  0.69,  Train Acc: 68.75%,  Val Loss:  0.86,  Val Acc: 57.87%,  Time: 0:06:02 \n","Iter:   1400,  Train Loss:  0.69,  Train Acc: 75.00%,  Val Loss:  0.85,  Val Acc: 59.15%,  Time: 0:06:15 *\n","Iter:   1450,  Train Loss:  0.52,  Train Acc: 93.75%,  Val Loss:  0.85,  Val Acc: 58.47%,  Time: 0:06:29 \n","Epoch [6/20]\n","Iter:   1500,  Train Loss:  0.87,  Train Acc: 50.00%,  Val Loss:  0.86,  Val Acc: 58.21%,  Time: 0:06:43 \n","Iter:   1550,  Train Loss:  0.77,  Train Acc: 56.25%,  Val Loss:  0.85,  Val Acc: 57.79%,  Time: 0:06:56 \n","Iter:   1600,  Train Loss:  0.63,  Train Acc: 81.25%,  Val Loss:  0.86,  Val Acc: 57.70%,  Time: 0:07:09 \n","Iter:   1650,  Train Loss:  0.82,  Train Acc: 56.25%,  Val Loss:  0.85,  Val Acc: 58.04%,  Time: 0:07:22 \n","Iter:   1700,  Train Loss:  0.71,  Train Acc: 68.75%,  Val Loss:  0.85,  Val Acc: 59.15%,  Time: 0:07:36 *\n","Iter:   1750,  Train Loss:  0.74,  Train Acc: 68.75%,  Val Loss:  0.85,  Val Acc: 58.72%,  Time: 0:07:49 \n","Epoch [7/20]\n","Iter:   1800,  Train Loss:  0.53,  Train Acc: 87.50%,  Val Loss:  0.85,  Val Acc: 58.21%,  Time: 0:08:03 \n","Iter:   1850,  Train Loss:  0.74,  Train Acc: 68.75%,  Val Loss:  0.86,  Val Acc: 58.81%,  Time: 0:08:16 \n","Iter:   1900,  Train Loss:   0.5,  Train Acc: 81.25%,  Val Loss:  0.85,  Val Acc: 58.89%,  Time: 0:08:30 \n","Iter:   1950,  Train Loss:   0.6,  Train Acc: 68.75%,  Val Loss:  0.85,  Val Acc: 58.21%,  Time: 0:08:43 \n","Iter:   2000,  Train Loss:  0.55,  Train Acc: 81.25%,  Val Loss:  0.85,  Val Acc: 59.49%,  Time: 0:08:56 \n","Iter:   2050,  Train Loss:  0.75,  Train Acc: 75.00%,  Val Loss:  0.85,  Val Acc: 59.23%,  Time: 0:09:09 \n","Epoch [8/20]\n","Iter:   2100,  Train Loss:  0.49,  Train Acc: 93.75%,  Val Loss:  0.85,  Val Acc: 58.64%,  Time: 0:09:23 \n","Iter:   2150,  Train Loss:   0.8,  Train Acc: 56.25%,  Val Loss:  0.86,  Val Acc: 60.17%,  Time: 0:09:37 \n","Iter:   2200,  Train Loss:  0.75,  Train Acc: 75.00%,  Val Loss:  0.85,  Val Acc: 58.89%,  Time: 0:09:50 \n","No optimization for a long time, auto-stopping...\n","Loading data...\n","Vocab size: 11694\n","Time usage: 0:00:02\n","Epoch [1/20]\n","Iter:      0,  Train Loss:   1.1,  Train Acc: 25.00%,  Val Loss:   1.1,  Val Acc: 45.53%,  Time: 0:00:00 *\n","Iter:     50,  Train Loss:  0.91,  Train Acc: 56.25%,  Val Loss:   1.0,  Val Acc: 49.53%,  Time: 0:00:14 *\n","Iter:    100,  Train Loss:  0.88,  Train Acc: 56.25%,  Val Loss:  0.97,  Val Acc: 52.09%,  Time: 0:00:27 *\n","Iter:    150,  Train Loss:   1.1,  Train Acc: 50.00%,  Val Loss:  0.96,  Val Acc: 52.77%,  Time: 0:00:40 *\n","Iter:    200,  Train Loss:  0.95,  Train Acc: 50.00%,  Val Loss:  0.94,  Val Acc: 52.26%,  Time: 0:00:54 *\n","Iter:    250,  Train Loss:  0.87,  Train Acc: 68.75%,  Val Loss:  0.94,  Val Acc: 53.70%,  Time: 0:01:07 *\n","Epoch [2/20]\n","Iter:    300,  Train Loss:  0.81,  Train Acc: 62.50%,  Val Loss:  0.93,  Val Acc: 53.28%,  Time: 0:01:21 *\n","Iter:    350,  Train Loss:   0.8,  Train Acc: 68.75%,  Val Loss:  0.93,  Val Acc: 53.19%,  Time: 0:01:34 *\n","Iter:    400,  Train Loss:  0.83,  Train Acc: 56.25%,  Val Loss:  0.92,  Val Acc: 53.70%,  Time: 0:01:48 *\n","Iter:    450,  Train Loss:  0.83,  Train Acc: 68.75%,  Val Loss:  0.92,  Val Acc: 53.45%,  Time: 0:02:01 *\n","Iter:    500,  Train Loss:  0.72,  Train Acc: 75.00%,  Val Loss:  0.91,  Val Acc: 54.30%,  Time: 0:02:14 *\n","Iter:    550,  Train Loss:  0.86,  Train Acc: 50.00%,  Val Loss:  0.91,  Val Acc: 54.47%,  Time: 0:02:28 *\n","Epoch [3/20]\n","Iter:    600,  Train Loss:  0.84,  Train Acc: 56.25%,  Val Loss:   0.9,  Val Acc: 54.55%,  Time: 0:02:41 *\n","Iter:    650,  Train Loss:  0.74,  Train Acc: 81.25%,  Val Loss:   0.9,  Val Acc: 54.13%,  Time: 0:02:55 *\n","Iter:    700,  Train Loss:  0.84,  Train Acc: 62.50%,  Val Loss:   0.9,  Val Acc: 54.89%,  Time: 0:03:08 \n","Iter:    750,  Train Loss:  0.83,  Train Acc: 68.75%,  Val Loss:   0.9,  Val Acc: 54.89%,  Time: 0:03:21 *\n","Iter:    800,  Train Loss:  0.97,  Train Acc: 43.75%,  Val Loss:   0.9,  Val Acc: 54.55%,  Time: 0:03:35 *\n","Iter:    850,  Train Loss:   1.0,  Train Acc: 43.75%,  Val Loss:   0.9,  Val Acc: 55.66%,  Time: 0:03:48 \n","Epoch [4/20]\n","Iter:    900,  Train Loss:  0.77,  Train Acc: 62.50%,  Val Loss:  0.89,  Val Acc: 55.74%,  Time: 0:04:02 *\n","Iter:    950,  Train Loss:  0.68,  Train Acc: 75.00%,  Val Loss:  0.89,  Val Acc: 54.98%,  Time: 0:04:15 \n","Iter:   1000,  Train Loss:  0.87,  Train Acc: 56.25%,  Val Loss:  0.89,  Val Acc: 55.49%,  Time: 0:04:28 \n","Iter:   1050,  Train Loss:  0.78,  Train Acc: 62.50%,  Val Loss:  0.89,  Val Acc: 54.64%,  Time: 0:04:42 \n","Iter:   1100,  Train Loss:  0.63,  Train Acc: 75.00%,  Val Loss:  0.89,  Val Acc: 54.89%,  Time: 0:04:55 *\n","Iter:   1150,  Train Loss:  0.83,  Train Acc: 62.50%,  Val Loss:  0.89,  Val Acc: 55.06%,  Time: 0:05:08 \n","Epoch [5/20]\n","Iter:   1200,  Train Loss:  0.93,  Train Acc: 56.25%,  Val Loss:  0.88,  Val Acc: 55.15%,  Time: 0:05:22 *\n","Iter:   1250,  Train Loss:  0.62,  Train Acc: 62.50%,  Val Loss:  0.89,  Val Acc: 55.57%,  Time: 0:05:36 \n","Iter:   1300,  Train Loss:  0.63,  Train Acc: 81.25%,  Val Loss:  0.89,  Val Acc: 55.32%,  Time: 0:05:49 \n","Iter:   1350,  Train Loss:  0.88,  Train Acc: 56.25%,  Val Loss:  0.89,  Val Acc: 55.74%,  Time: 0:06:02 \n","Iter:   1400,  Train Loss:  0.76,  Train Acc: 81.25%,  Val Loss:  0.88,  Val Acc: 55.15%,  Time: 0:06:15 \n","Iter:   1450,  Train Loss:  0.82,  Train Acc: 68.75%,  Val Loss:  0.88,  Val Acc: 56.17%,  Time: 0:06:29 *\n","Epoch [6/20]\n","Iter:   1500,  Train Loss:  0.66,  Train Acc: 81.25%,  Val Loss:  0.88,  Val Acc: 57.02%,  Time: 0:06:43 *\n","Iter:   1550,  Train Loss:  0.71,  Train Acc: 62.50%,  Val Loss:  0.88,  Val Acc: 57.02%,  Time: 0:06:56 \n","Iter:   1600,  Train Loss:  0.97,  Train Acc: 68.75%,  Val Loss:  0.88,  Val Acc: 56.43%,  Time: 0:07:09 \n","Iter:   1650,  Train Loss:  0.91,  Train Acc: 62.50%,  Val Loss:  0.89,  Val Acc: 55.74%,  Time: 0:07:22 \n","Iter:   1700,  Train Loss:   0.7,  Train Acc: 56.25%,  Val Loss:  0.88,  Val Acc: 55.40%,  Time: 0:07:36 \n","Iter:   1750,  Train Loss:  0.91,  Train Acc: 68.75%,  Val Loss:  0.88,  Val Acc: 56.85%,  Time: 0:07:49 \n","Epoch [7/20]\n","Iter:   1800,  Train Loss:  0.57,  Train Acc: 75.00%,  Val Loss:  0.88,  Val Acc: 57.02%,  Time: 0:08:03 \n","Iter:   1850,  Train Loss:  0.44,  Train Acc: 93.75%,  Val Loss:  0.88,  Val Acc: 57.02%,  Time: 0:08:16 \n","Iter:   1900,  Train Loss:  0.73,  Train Acc: 68.75%,  Val Loss:  0.88,  Val Acc: 57.11%,  Time: 0:08:29 \n","Iter:   1950,  Train Loss:  0.73,  Train Acc: 81.25%,  Val Loss:  0.89,  Val Acc: 57.19%,  Time: 0:08:43 \n","Iter:   2000,  Train Loss:  0.61,  Train Acc: 75.00%,  Val Loss:  0.89,  Val Acc: 55.40%,  Time: 0:08:56 \n","No optimization for a long time, auto-stopping...\n","Loading data...\n","Vocab size: 11694\n","Time usage: 0:00:02\n","Epoch [1/20]\n","Iter:      0,  Train Loss:   1.1,  Train Acc: 68.75%,  Val Loss:   1.1,  Val Acc: 39.15%,  Time: 0:00:00 *\n","Iter:     50,  Train Loss:   1.0,  Train Acc: 43.75%,  Val Loss:  0.97,  Val Acc: 51.57%,  Time: 0:00:14 *\n","Iter:    100,  Train Loss:  0.98,  Train Acc: 43.75%,  Val Loss:  0.95,  Val Acc: 52.60%,  Time: 0:00:27 *\n","Iter:    150,  Train Loss:  0.89,  Train Acc: 62.50%,  Val Loss:  0.93,  Val Acc: 53.45%,  Time: 0:00:40 *\n","Iter:    200,  Train Loss:  0.91,  Train Acc: 62.50%,  Val Loss:  0.94,  Val Acc: 53.02%,  Time: 0:00:54 \n","Iter:    250,  Train Loss:  0.92,  Train Acc: 43.75%,  Val Loss:  0.92,  Val Acc: 53.53%,  Time: 0:01:07 *\n","Epoch [2/20]\n","Iter:    300,  Train Loss:  0.99,  Train Acc: 50.00%,  Val Loss:  0.91,  Val Acc: 53.28%,  Time: 0:01:21 *\n","Iter:    350,  Train Loss:  0.85,  Train Acc: 50.00%,  Val Loss:   0.9,  Val Acc: 55.32%,  Time: 0:01:34 *\n","Iter:    400,  Train Loss:  0.87,  Train Acc: 62.50%,  Val Loss:  0.89,  Val Acc: 55.91%,  Time: 0:01:48 *\n","Iter:    450,  Train Loss:  0.98,  Train Acc: 50.00%,  Val Loss:   0.9,  Val Acc: 55.49%,  Time: 0:02:01 \n","Iter:    500,  Train Loss:  0.91,  Train Acc: 56.25%,  Val Loss:  0.91,  Val Acc: 53.62%,  Time: 0:02:14 \n","Iter:    550,  Train Loss:  0.83,  Train Acc: 56.25%,  Val Loss:  0.89,  Val Acc: 54.81%,  Time: 0:02:27 *\n","Epoch [3/20]\n","Iter:    600,  Train Loss:   0.7,  Train Acc: 75.00%,  Val Loss:  0.88,  Val Acc: 55.66%,  Time: 0:02:41 *\n","Iter:    650,  Train Loss:  0.72,  Train Acc: 81.25%,  Val Loss:  0.88,  Val Acc: 56.00%,  Time: 0:02:55 *\n","Iter:    700,  Train Loss:  0.78,  Train Acc: 68.75%,  Val Loss:  0.87,  Val Acc: 57.19%,  Time: 0:03:08 *\n","Iter:    750,  Train Loss:  0.73,  Train Acc: 62.50%,  Val Loss:  0.88,  Val Acc: 56.00%,  Time: 0:03:21 \n","Iter:    800,  Train Loss:  0.95,  Train Acc: 62.50%,  Val Loss:  0.88,  Val Acc: 55.57%,  Time: 0:03:35 \n","Iter:    850,  Train Loss:  0.78,  Train Acc: 68.75%,  Val Loss:  0.87,  Val Acc: 56.43%,  Time: 0:03:48 *\n","Epoch [4/20]\n","Iter:    900,  Train Loss:  0.72,  Train Acc: 68.75%,  Val Loss:  0.87,  Val Acc: 57.79%,  Time: 0:04:02 *\n","Iter:    950,  Train Loss:   0.9,  Train Acc: 50.00%,  Val Loss:  0.87,  Val Acc: 56.43%,  Time: 0:04:15 *\n","Iter:   1000,  Train Loss:  0.82,  Train Acc: 62.50%,  Val Loss:  0.86,  Val Acc: 57.79%,  Time: 0:04:28 *\n","Iter:   1050,  Train Loss:   1.0,  Train Acc: 50.00%,  Val Loss:  0.87,  Val Acc: 56.85%,  Time: 0:04:42 \n","Iter:   1100,  Train Loss:   1.1,  Train Acc: 31.25%,  Val Loss:  0.87,  Val Acc: 57.53%,  Time: 0:04:55 \n","Iter:   1150,  Train Loss:  0.78,  Train Acc: 56.25%,  Val Loss:  0.86,  Val Acc: 58.38%,  Time: 0:05:08 *\n","Epoch [5/20]\n","Iter:   1200,  Train Loss:  0.68,  Train Acc: 62.50%,  Val Loss:  0.86,  Val Acc: 57.79%,  Time: 0:05:22 \n","Iter:   1250,  Train Loss:   1.1,  Train Acc: 31.25%,  Val Loss:  0.86,  Val Acc: 58.38%,  Time: 0:05:35 \n","Iter:   1300,  Train Loss:  0.73,  Train Acc: 75.00%,  Val Loss:  0.85,  Val Acc: 59.83%,  Time: 0:05:49 *\n","Iter:   1350,  Train Loss:  0.67,  Train Acc: 68.75%,  Val Loss:  0.86,  Val Acc: 58.89%,  Time: 0:06:02 \n","Iter:   1400,  Train Loss:  0.76,  Train Acc: 56.25%,  Val Loss:  0.86,  Val Acc: 57.45%,  Time: 0:06:15 \n","Iter:   1450,  Train Loss:  0.85,  Train Acc: 50.00%,  Val Loss:  0.85,  Val Acc: 59.40%,  Time: 0:06:29 *\n","Epoch [6/20]\n","Iter:   1500,  Train Loss:  0.72,  Train Acc: 68.75%,  Val Loss:  0.85,  Val Acc: 59.23%,  Time: 0:06:43 \n","Iter:   1550,  Train Loss:  0.52,  Train Acc: 75.00%,  Val Loss:  0.85,  Val Acc: 59.15%,  Time: 0:06:56 \n","Iter:   1600,  Train Loss:  0.53,  Train Acc: 75.00%,  Val Loss:  0.85,  Val Acc: 58.30%,  Time: 0:07:09 \n","Iter:   1650,  Train Loss:  0.52,  Train Acc: 81.25%,  Val Loss:  0.85,  Val Acc: 58.38%,  Time: 0:07:22 \n","Iter:   1700,  Train Loss:  0.67,  Train Acc: 75.00%,  Val Loss:  0.86,  Val Acc: 58.30%,  Time: 0:07:36 \n","Iter:   1750,  Train Loss:  0.59,  Train Acc: 75.00%,  Val Loss:  0.85,  Val Acc: 59.06%,  Time: 0:07:49 *\n","Epoch [7/20]\n","Iter:   1800,  Train Loss:  0.63,  Train Acc: 68.75%,  Val Loss:  0.85,  Val Acc: 58.38%,  Time: 0:08:03 *\n","Iter:   1850,  Train Loss:  0.67,  Train Acc: 75.00%,  Val Loss:  0.85,  Val Acc: 59.57%,  Time: 0:08:16 *\n","Iter:   1900,  Train Loss:  0.73,  Train Acc: 81.25%,  Val Loss:  0.85,  Val Acc: 58.13%,  Time: 0:08:30 \n","Iter:   1950,  Train Loss:  0.38,  Train Acc: 93.75%,  Val Loss:  0.86,  Val Acc: 58.30%,  Time: 0:08:43 \n","Iter:   2000,  Train Loss:  0.62,  Train Acc: 75.00%,  Val Loss:  0.86,  Val Acc: 58.64%,  Time: 0:08:56 \n","Iter:   2050,  Train Loss:   0.6,  Train Acc: 68.75%,  Val Loss:  0.85,  Val Acc: 58.72%,  Time: 0:09:09 \n","Epoch [8/20]\n","Iter:   2100,  Train Loss:  0.54,  Train Acc: 87.50%,  Val Loss:  0.85,  Val Acc: 58.47%,  Time: 0:09:23 \n","Iter:   2150,  Train Loss:  0.63,  Train Acc: 75.00%,  Val Loss:  0.85,  Val Acc: 60.00%,  Time: 0:09:36 \n","Iter:   2200,  Train Loss:  0.61,  Train Acc: 75.00%,  Val Loss:  0.85,  Val Acc: 58.89%,  Time: 0:09:50 \n","Iter:   2250,  Train Loss:  0.57,  Train Acc: 87.50%,  Val Loss:  0.86,  Val Acc: 57.79%,  Time: 0:10:03 \n","Iter:   2300,  Train Loss:  0.59,  Train Acc: 75.00%,  Val Loss:  0.87,  Val Acc: 58.89%,  Time: 0:10:16 \n","Iter:   2350,  Train Loss:  0.76,  Train Acc: 68.75%,  Val Loss:  0.85,  Val Acc: 58.04%,  Time: 0:10:30 \n","No optimization for a long time, auto-stopping...\n","f: 0.551249809894705+0.017954750401265297\n","f1: 0.5686131199972595+0.02774402956290237\n","f2: 0.542303445159721+0.024229320204618342\n","f3: 0.4573893158752182+0.029917674601198308\n"]}],"source":["import time\n","import torch\n","import numpy as np\n","\n","dataset='/content/drive/MyDrive/Prediction' # 数据集\n","embedding = 'embedding_CantoStance.npz'\n","model_name = 'Tan' \n","\n","f=list()\n","f1=list()\n","f2=list()\n","f3=list()\n","\n","for i in range(5):\n","    dataset='/content/drive/MyDrive/Prediction'\n","    df = pd.concat([df2,df_info])\n","    df = df.sample(frac=1, random_state=i)   \n","    cut_idx1 = int(round(0.8 * df.shape[0]))\n","    df_train,df_test = df[:cut_idx1], df[cut_idx1:]\n","\n","    config = CNN_Config(dataset, embedding,df_train,df_test)\n","    np.random.seed(1)\n","    torch.manual_seed(1)\n","    torch.cuda.manual_seed_all(1)\n","    torch.backends.cudnn.deterministic = True  # 保证每次结果一样\n","\n","    start_time = time.time()\n","    print(\"Loading data...\")\n","    vocab, train_data, test_data = build_dataset(config)\n","    train_iter = build_iterator(train_data, config,)\n","    dev_iter = build_iterator(test_data, config,)\n","    test_iter = build_iterator(test_data, config)\n","    time_dif = get_time_dif(start_time)\n","    print(\"Time usage:\", time_dif)\n","\n","    config.n_vocab = len(vocab)\n","    model = TextCNN(config).to(config.device)\n","    init_network(model)\n","    train(config, model, train_iter, dev_iter, test_iter)\n","    df_test,f_score = test(config, model, test_iter, df_test)\n","    f.append(f_score)\n","    f1.append(get_depth_f1(df_test,1))\n","    f2.append(get_depth_f1(df_test,2))\n","    f3.append(get_depth_f1(df_test,3))\n","\n","\n","print(\"f: \"+str(np.mean(f))+\"+\"+str(np.std(f, ddof = 1)))\n","print(\"f1: \"+str(np.mean(f1))+\"+\"+str(np.std(f1, ddof = 1)))\n","print(\"f2: \"+str(np.mean(f2))+\"+\"+str(np.std(f2, ddof = 1)))\n","print(\"f3: \"+str(np.mean(f3))+\"+\"+str(np.std(f3, ddof = 1)))"]},{"cell_type":"markdown","metadata":{"id":"fxWxrOJWah0Q"},"source":["# Kill Process"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2286,"status":"ok","timestamp":1648889260392,"user":{"displayName":"刘耘果","userId":"11745284739417236882"},"user_tz":-480},"id":"fqi23lxMTYnY","outputId":"372b3e83-4238-4754-ff14-cc6a9b07fe79"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","psmisc is already the newest version (23.1-1ubuntu0.1).\n","0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n","/dev/nvidia0:        33539m\n","/dev/nvidiactl:      33539m\n","/dev/nvidia-uvm:     33539m\n"]}],"source":["!apt install psmisc\n","!sudo fuser /dev/nvidia*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ibiqMTu-TdoU"},"outputs":[],"source":["!kill -9 33539"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":915,"status":"ok","timestamp":1648721648285,"user":{"displayName":"刘耘果","userId":"11745284739417236882"},"user_tz":-480},"id":"sTipKlJPO8pJ","outputId":"a03d4a1b-4803-4c85-e87d-547acbb864e7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Thu Mar 31 10:14:08 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   39C    P0    34W / 250W |   1371MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":5798,"status":"ok","timestamp":1648958951095,"user":{"displayName":"刘耘果","userId":"11745284739417236882"},"user_tz":-480},"id":"044G1nxVQw0V","outputId":"08cba26e-7fda-47ea-a6e8-a41f3370b359"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Tesla P100-PCIE-16GB'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","torch.cuda.get_device_name(0)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["20xHeJLjaS3N","Rjuffl54JfA7","ju43j9EvfnRb"],"machine_shape":"hm","name":"Global_Pooling_BranchBert.ipynb","provenance":[{"file_id":"1ctG7GwT-vlbjiDp6isHaXTjFdpAX1I5E","timestamp":1634993162676}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"d35c5e198ab340c59b6df7962333c0cf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cffc7ab6456d44079aca76c3bf7a8a54","IPY_MODEL_b215df49c8274f7c999886ae1d0f2a67","IPY_MODEL_0d0be826c7a84839af6b8bea6ad58886"],"layout":"IPY_MODEL_4b6b68186b924e5da4f387e5e8aa88ab"}},"cffc7ab6456d44079aca76c3bf7a8a54":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe94bdb132d84b68a0f07ce6f1d5d533","placeholder":"​","style":"IPY_MODEL_74f09aeaef414f72adcfc9da0101f24a","value":"Downloading: 100%"}},"b215df49c8274f7c999886ae1d0f2a67":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dbb92e31c0ed4b5f9d1d501bf14903f4","max":128441,"min":0,"orientation":"horizontal","style":"IPY_MODEL_63295c5bced44cfea8912fdad7970b6d","value":128441}},"0d0be826c7a84839af6b8bea6ad58886":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_562b632249184d118d1cbedad28e9906","placeholder":"​","style":"IPY_MODEL_c5a3d1b59f6c4a5093a972af19b771cd","value":" 125k/125k [00:00&lt;00:00, 672kB/s]"}},"4b6b68186b924e5da4f387e5e8aa88ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe94bdb132d84b68a0f07ce6f1d5d533":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74f09aeaef414f72adcfc9da0101f24a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dbb92e31c0ed4b5f9d1d501bf14903f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63295c5bced44cfea8912fdad7970b6d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"562b632249184d118d1cbedad28e9906":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5a3d1b59f6c4a5093a972af19b771cd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"40899b2b7ec9499db3eebb6f91382410":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fce9e6de750f40de80e7924072a4c701","IPY_MODEL_8a16107863ff4e678d1f09d52428a5b2","IPY_MODEL_2fe248d6678f49d1b960f20853ddf24e"],"layout":"IPY_MODEL_70eaac4cc699458d9ae9ad7a303afa19"}},"fce9e6de750f40de80e7924072a4c701":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b36e90de30d549e0800e4fa622303a05","placeholder":"​","style":"IPY_MODEL_f515a7928ef5421598261a34135f675e","value":"Downloading: 100%"}},"8a16107863ff4e678d1f09d52428a5b2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ec0848f71b341bcb37ecec59064dda6","max":585,"min":0,"orientation":"horizontal","style":"IPY_MODEL_932656006afc418091148cfd06f56321","value":585}},"2fe248d6678f49d1b960f20853ddf24e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e8f7caf3bd54a6eabae710d12ab032d","placeholder":"​","style":"IPY_MODEL_f3d3c990201b4fab8d40acd4c4b9bbdb","value":" 585/585 [00:00&lt;00:00, 21.9kB/s]"}},"70eaac4cc699458d9ae9ad7a303afa19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b36e90de30d549e0800e4fa622303a05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f515a7928ef5421598261a34135f675e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3ec0848f71b341bcb37ecec59064dda6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"932656006afc418091148cfd06f56321":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1e8f7caf3bd54a6eabae710d12ab032d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3d3c990201b4fab8d40acd4c4b9bbdb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"632a0533a6ba4abf90c018b7b5037125":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f7531eb574ac4a2db3be5cc9ff23cd6a","IPY_MODEL_6b97bc5c87af4f8cab6c20398bb223db","IPY_MODEL_5a4698d5a8974d97a261ed005f6cec0b"],"layout":"IPY_MODEL_a3c3e4d26c2044a0bebabcede63862eb"}},"f7531eb574ac4a2db3be5cc9ff23cd6a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db3366d76ab441ecb64efcfffa1bf3e0","placeholder":"​","style":"IPY_MODEL_73445e2a7c874d20b00c240da624c795","value":"Downloading: 100%"}},"6b97bc5c87af4f8cab6c20398bb223db":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3bdc219ba22346eeab25b71f0a89ee25","max":436377630,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6801197e17274ebf8410169977a39ea0","value":436377630}},"5a4698d5a8974d97a261ed005f6cec0b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_875e214302724e0188b214e22449305c","placeholder":"​","style":"IPY_MODEL_08658c1a73894a77bbfc4c7541e21bb3","value":" 416M/416M [00:09&lt;00:00, 41.4MB/s]"}},"a3c3e4d26c2044a0bebabcede63862eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db3366d76ab441ecb64efcfffa1bf3e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73445e2a7c874d20b00c240da624c795":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3bdc219ba22346eeab25b71f0a89ee25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6801197e17274ebf8410169977a39ea0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"875e214302724e0188b214e22449305c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08658c1a73894a77bbfc4c7541e21bb3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}