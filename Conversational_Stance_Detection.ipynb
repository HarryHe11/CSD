{"cells":[{"cell_type":"markdown","metadata":{"id":"Zu8bU7EbaEf4"},"source":["# Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":58861,"status":"ok","timestamp":1649723869079,"user":{"displayName":"刘耘果","userId":"11745284739417236882"},"user_tz":-480},"id":"gtIFGEWQhMjy","outputId":"67dd75a0-450c-42fe-ac98-57c859f8ed4b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16872,"status":"ok","timestamp":1649723885947,"user":{"displayName":"刘耘果","userId":"11745284739417236882"},"user_tz":-480},"id":"omeDVC8Hzy6G","outputId":"a06a4fdf-eae4-4b26-ecd2-1b3a53f57b55"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting opencc-python-reimplemented\n","  Downloading opencc-python-reimplemented-0.1.6.tar.gz (484 kB)\n","\u001b[?25l\r\u001b[K     |▊                               | 10 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██                              | 30 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 40 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 51 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |████                            | 61 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 71 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 81 kB 3.5 MB/s eta 0:00:01\r\u001b[K     |██████                          | 92 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 102 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 112 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 122 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 133 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 143 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 153 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 163 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 174 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 184 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 194 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 204 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 215 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 225 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 235 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 245 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 256 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 266 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 276 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 286 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 296 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 307 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 317 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 327 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 337 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 348 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 358 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 368 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 378 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 389 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 399 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 409 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 419 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 430 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 440 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 450 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 460 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 471 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 481 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 484 kB 4.3 MB/s \n","\u001b[?25hBuilding wheels for collected packages: opencc-python-reimplemented\n","  Building wheel for opencc-python-reimplemented (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for opencc-python-reimplemented: filename=opencc_python_reimplemented-0.1.6-py2.py3-none-any.whl size=486152 sha256=7f126bd312ab13ae6319e1286c93c585b187bf0d6cc6d02e30a77df05af8f60d\n","  Stored in directory: /root/.cache/pip/wheels/4e/e2/60/d062d260be08788bb389521544a8fc173de9a9a78d6a593344\n","Successfully built opencc-python-reimplemented\n","Installing collected packages: opencc-python-reimplemented\n","Successfully installed opencc-python-reimplemented-0.1.6\n","Collecting pycantonese\n","  Downloading pycantonese-3.4.0-py3-none-any.whl (3.9 MB)\n","\u001b[K     |████████████████████████████████| 3.9 MB 4.2 MB/s \n","\u001b[?25hCollecting wordseg==0.0.2\n","  Downloading wordseg-0.0.2-py3-none-any.whl (9.5 kB)\n","Collecting pylangacq<0.17.0,>=0.16.0\n","  Downloading pylangacq-0.16.2-py3-none-any.whl (64 kB)\n","\u001b[K     |████████████████████████████████| 64 kB 3.5 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata>=1.0 in /usr/local/lib/python3.7/dist-packages (from pycantonese) (4.11.3)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.0->pycantonese) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.0->pycantonese) (3.7.0)\n","Requirement already satisfied: requests<=3.0.0,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from pylangacq<0.17.0,>=0.16.0->pycantonese) (2.23.0)\n","Requirement already satisfied: python-dateutil<=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from pylangacq<0.17.0,>=0.16.0->pycantonese) (2.8.2)\n","Requirement already satisfied: tabulate[widechars]<=0.9.0,>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from pylangacq<0.17.0,>=0.16.0->pycantonese) (0.8.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<=3.0.0,>=2.0.0->pylangacq<0.17.0,>=0.16.0->pycantonese) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<=3.0.0,>=2.18.0->pylangacq<0.17.0,>=0.16.0->pycantonese) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<=3.0.0,>=2.18.0->pylangacq<0.17.0,>=0.16.0->pycantonese) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<=3.0.0,>=2.18.0->pylangacq<0.17.0,>=0.16.0->pycantonese) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<=3.0.0,>=2.18.0->pylangacq<0.17.0,>=0.16.0->pycantonese) (2.10)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from tabulate[widechars]<=0.9.0,>=0.8.9->pylangacq<0.17.0,>=0.16.0->pycantonese) (0.2.5)\n","Installing collected packages: wordseg, pylangacq, pycantonese\n","Successfully installed pycantonese-3.4.0 pylangacq-0.16.2 wordseg-0.0.2\n","Collecting transformers\n","  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n","\u001b[K     |████████████████████████████████| 4.0 MB 4.1 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 7.6 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 92.3 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 69.8 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 61.3 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.18.0\n"]}],"source":["!pip install opencc-python-reimplemented\n","!pip install pycantonese"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_xQkGSnyhSYr"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import re,string\n","import math\n","import opencc\n","from sklearn import preprocessing\n","import torch\n","import torch.nn as nn\n","import pkg_resources\n","import warnings\n","pkg_resources.get_distribution(\"xlrd\").version\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QmuJRLtrO5jc"},"outputs":[],"source":["#Data pre-processing\n","def simplify_punctuation_and_whitespace(sentence):\n","    #remove urls\n","    sent = _replace_urls(sentence)\n","    #remove redundant punctuation\n","    sent = _simplify_punctuation(sent)\n","    #normilize whitespaces\n","    sent = _normalize_whitespace(sent)\n","    return sent\n","\n","def _replace_urls(text):\n","    url_regex = r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n","    text = text.replace('�','.')\n","    text = re.sub(r'^[\\x00-\\x7F]+|[\\x00-\\x7F]+$', '', text)#special case\n","    text = re.sub(url_regex, \"<URL>\", text)\n","    return text\n","\n","def _simplify_punctuation(text):\n","    \"\"\"\n","    This function simplifies doubled or more complex punctuation. The exception is '...'.\n","    \"\"\"\n","    corrected = str(text)\n","    corrected = re.sub(r'([!?,;？❓！.])\\1+', r'\\1', corrected)\n","    corrected = re.sub(r'\\.{2,}', r'...', corrected)\n","    return corrected\n","\n","def _normalize_whitespace(text):\n","    \"\"\"\n","    This function normalizes whitespaces, removing duplicates.\n","    \"\"\"\n","    corrected = str(text)\n","    corrected = re.sub(r\"//t\",r\"\\t\", corrected)\n","    corrected = re.sub(r\"( )\\1+\",r\"\\1\", corrected)\n","    corrected = re.sub(r\"(\\n)\\1+\",r\"\\1\", corrected)\n","    corrected = re.sub(r\"(\\r)\\1+\",r\"\\1\", corrected)\n","    corrected = re.sub(r\"(\\t)\\1+\",r\"\\1\", corrected)\n","    return corrected.strip(\" \")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zOg1LkYw9pFj"},"outputs":[],"source":["#get data\n","data=pd.read_excel(r'/content/drive/MyDrive/DATA/dataset_feature.xlsx')\n","df=pd.DataFrame(data)\n","cc = opencc.OpenCC('s2hk')\n","for i in range(len(df)):\n","  text=cc.convert(df['text'].iloc[i]) #convert every Cantonese character into HK standard.\n","  text_new=simplify_punctuation_and_whitespace(text) #data pre-process.\n","  if len(text_new)==0:\n","    df.drop([i],inplace = True)\n","  else:\n","    df['text'].iloc[i]=text_new\n","print(len(df))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fKP53b1NRJvI"},"outputs":[],"source":["mask=(df['is_post']==1)\n","df_info=df.loc[mask] # dataframe for posts\n","df_info['branch_text']=df_info['text']\n","df_info['depth']=1\n","columns=['label', 'is_post', 'info_id', 'cmt_id', 'parent', 'text','branch_text','depth']\n","df_info=df_info[columns]\n","df_info['label']=df_info['label']-1\n","\n","mask=(df['is_post']==0)\n","df2=df.loc[mask] # dataframe for comments\n","df2['last_sep']=0\n","df2['depth']=1 # iniatialize the depth for comments as 1\n","df2['branch_text']=''\n","columns=['label', 'info_id', 'cmt_id', 'parent','last_sep', 'text','branch_text','depth']\n","df2=df2[columns]\n","df2['label']=df2['label']-1\n","\n","for i in range(len(df2)):\n","  if math.isnan(df2['parent'].iloc[i]):\n","    df2['parent'].iloc[i]=df2['info_id'].iloc[i] #for those comments dont have a \"parent\", the post nodes are their parents\n","  else:\n","    df2['parent'].iloc[i] = str(int(df2['parent'].iloc[i]))\n","df2.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z0qa0cOHSM9J"},"outputs":[],"source":["Sep = '[SEP]'\n","lenths=[]#over-long lenths\n","emptys=[]#error nodes\n","\n","def build_branch(dataframe, i, text_list):\n","  '''\n","  build sub-branches for branch BERT\n","  :param dataframe: dataframe \n","  :param i: current pointer\n","  ;param text_list: current built subbranch\n","  :return: built sub-branch, which is the input of Branch-BERT\n","  '''\n","  if dataframe['parent'].iloc[i] == dataframe['info_id'].iloc[i]:\n","    '''if parent node is a post'''\n","    info_id = dataframe['info_id'].iloc[i]\n","    df_parent = df_info.loc[df_info['info_id']==info_id]\n","    if len(df_parent) == 1:\n","      text_list.append(dataframe['text'].iloc[i])\n","      text_list.append(df_parent['text'].iloc[0])\n","    else:\n","      print(\"INFO EMPTY: \", info_id)\n","  else:\n","    '''if parent node is a comment'''\n","    text_list.append(dataframe['text'].iloc[i])\n","    parent_id = str(dataframe['parent'].iloc[i])\n","    df_parent = df2.loc[df2['cmt_id'] == parent_id]\n","    if len(df_parent) == 1:\n","      '''recursion for sub-branch building'''\n","      build_branch(df_parent, 0, text_list)\n","    else:\n","      emptys.append(parent_id)#error\n","  text_list = list(reversed(text_list))#reverse the found branch: [post,cmt1,cmt2...cmtn]\n","  Str = Sep.join(text_list) #Separate each two consecutive instances with [SEP]\n","  return Str\n","\n","'''iterate over the comments dataframe to build sub-branches'''\n","for i in range(len(df2)):\n","  text=[]\n","  text = build_branch(df2, i, text)  \n","  df2['branch_text'].iloc[i]=text\n","  df2['depth'].iloc[i]=len(text) # depth is the lenth of sub-branch\n","df2=df2[~df2['parent'].isin(emptys)]\n"]},{"cell_type":"markdown","metadata":{"id":"20xHeJLjaS3N"},"source":["# Branch Model\n","(Branch-BERT & Branch-BERT w/o CFE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_AMhXeyd97T2"},"outputs":[],"source":["# coding:utf-8\n","import pycantonese\n","import nltk\n","import numpy\n","import jieba\n","import codecs\n","import os\n","class SummaryTxt:#abstarct generater\n","    def __init__(self):\n","        #character number n\n","        self.N = 100\n","        #cluster threshold\n","        self.CLUSTER_THRESHOLD = 5\n","        #top n sentences\n","        self.TOP_SENTENCES = 5\n","        #load stopwords\n","        self.stopwords = {}.fromkeys(pycantonese.stop_words())\n","    def _split_sentences(self,texts):\n","        '''\n","        split text to single sentences.（.!?。！？）are signals for splitting.\n","        :param texts: texts\n","        :return:\n","        '''\n","        splitstr = '.!?。！？'.encode('utf8').decode('utf8')\n","        start = 0\n","        index = 0  # position for each characters\n","        sentences = []\n","        for text in texts:\n","            if text in splitstr:  # check whether their is a spliter\n","                sentences.append(texts[start:index + 1]) \n","                start = index + 1  \n","            index += 1\n","        if start < len(texts):\n","            sentences.append(texts[start:])  # in case that their is no spliter in the ending of the text\n","        return sentences\n","\n","    def _score_sentences(self,sentences, topn_words):\n","        '''\n","        score stentences with top n key words\n","        :param sentences: sentence list\n","        :param topn_words: key words list\n","        :return:\n","        '''\n","        scores = []\n","        sentence_idx = -1\n","        for s in [pycantonese.segment(s) for s in sentences]:\n","            sentence_idx += 1\n","            word_idx = []\n","            for w in topn_words:\n","                try:\n","                    word_idx.append(s.index(w))  # keywords'index\n","                except ValueError:  # w is not in the sentence \n","                    pass\n","            word_idx.sort()\n","            if len(word_idx) == 0:\n","                continue\n","            # for two consecutive words, comupute cluster\n","            clusters = []\n","            cluster = [word_idx[0]]\n","            i = 1\n","            while i < len(word_idx):\n","                if word_idx[i] - word_idx[i - 1] < self.CLUSTER_THRESHOLD:\n","                    cluster.append(word_idx[i])\n","                else:\n","                    clusters.append(cluster[:])\n","                    cluster = [word_idx[i]]\n","                i += 1\n","            clusters.append(cluster)\n","            # score each cluster, the maximum score of each cluster is the score for the sentence\n","            max_cluster_score = 0\n","            for c in clusters:\n","                significant_words_in_cluster = len(c)\n","                total_words_in_cluster = c[-1] - c[0] + 1\n","                score = 1.0 * significant_words_in_cluster * significant_words_in_cluster / total_words_in_cluster\n","                if score > max_cluster_score:\n","                    max_cluster_score = score\n","            scores.append((sentence_idx, max_cluster_score))\n","        return scores\n","\n","    def summaryScoredtxt(self,text):\n","        # split text into sentences\n","        sentences = self._split_sentences(text)\n","        # split words\n","        words = [w for sentence in sentences for w in pycantonese.segment(sentence) if w not in self.stopwords if\n","                 len(w) > 1 and w != '\\t']\n","        # get word frequency\n","        wordfre = nltk.FreqDist(words)\n","        # get words with top n frequency\n","        topn_words = [w[0] for w in sorted(wordfre.items(), key=lambda d: d[1], reverse=True)][:self.N]\n","        # score each sentences with the top n key words\n","        scored_sentences = self._score_sentences(sentences, topn_words)\n","        # use average number and std to filter non-trival sentences\n","        avg = numpy.mean([s[1] for s in scored_sentences])  # avg\n","        std = numpy.std([s[1] for s in scored_sentences])  # std\n","        summarySentences = []\n","        for (sent_idx, score) in scored_sentences:\n","            if score > (avg + 0.5 * std):\n","                summarySentences.append(sentences[sent_idx])\n","        return summarySentences\n","\n","    def summaryTopNtxt(self,text):\n","        # split text into sentences\n","        sentences = self._split_sentences(text)\n","        # split words\n","        words = [w for sentence in sentences for w in pycantonese.segment(sentence) if w not in self.stopwrods if\n","                 len(w) > 1 and w != '\\t']\n","        # get word frequency\n","        wordfre = nltk.FreqDist(words)\n","        # get words with top n frequency\n","        topn_words = [w[0] for w in sorted(wordfre.items(), key=lambda d: d[1], reverse=True)][:self.N]\n","        # score each sentences with the top n key words\n","        scored_sentences = self._score_sentences(sentences, topn_words)\n","        top_n_scored = sorted(scored_sentences, key=lambda s: s[1])[-self.TOP_SENTENCES:]\n","        top_n_scored = sorted(top_n_scored, key=lambda s: s[0])\n","        summarySentences = []\n","        for (idx, score) in top_n_scored:\n","            summarySentences.append(sentences[idx])\n","        return sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WVln7AGMKas8"},"outputs":[],"source":["import numpy as np\n","import torch\n","from tqdm import tqdm\n","import time\n","from datetime import timedelta\n","import pandas as pd\n","import numpy as np\n","from sklearn import preprocessing\n","from transformers import BertTokenizer\n","PAD, CLS = '[PAD]', '[CLS]'  # padding token \n","\n","def build_dataset(config, mode= 'branch'):\n","    def load_dataset(df, pad_size=config.pad_size):\n","        contents = []\n","        summarizer = SummaryTxt()\n","        tokenizer=BertTokenizer.from_pretrained(config.bert_path)\n","        tokenizer.add_special_tokens({\"additional_special_tokens\": ['[PAD]','[CLS]']})\n","        if mode =='branch':\n","          text_col='branch_text'\n","        else:\n","          text_col='text'\n","        #build dataset\n","        print('mode: ', mode)\n","        for i in range(len(df)):\n","            content=df[text_col].iloc[i]\n","            label=df['label'].iloc[i]\n","            token = tokenizer.tokenize(content)\n","            seq_len = len(token)\n","            if mode =='branch':#branch mode\n","              if seq_len > pad_size: #if\n","                #if text is to long, generate the post's abstract to replace the post\n","                first_index=content.find(Sep)\n","                post=content[:first_index]\n","                post=summarizer.summaryScoredtxt(post)\n","                post = '。'.join(post)\n","                content=post + content[first_index:]\n","                #tokenizer the abstract\n","                token = tokenizer.tokenize(content)\n","                seq_len = len(token)\n","                if seq_len >pad_size:\n","                  #if the text is still to long, simply cutting\n","                  token = token[-pad_size:]\n","                  seq_len = len(token)\n","              pos = [i for i, x in enumerate(token) if x ==Sep]    \n","              if len(pos)>0:#if there is a [SEP], it is a subbranch for comment\n","                last_sep=pos[-1]#record the position of the last [SEP] for futuring vector cutting\n","              else:\n","                #if it is a subbranch for post\n","                last_sep=0\n","              mask = []\n","              token_ids = tokenizer.convert_tokens_to_ids(token)\n","              if pad_size:\n","                  if len(token) < pad_size:\n","                      mask = [1] * len(token_ids) + [0] * (pad_size - len(token))\n","                      token_ids += ([0] * (pad_size - len(token)))\n","                  else:\n","                      mask = [1] * pad_size\n","                      token_ids = token_ids[:pad_size]\n","                      seq_len = pad_size\n","              contents.append((token_ids, int(label), seq_len, mask ,last_sep))\n","            else:#non-branch mode\n","              mask = []\n","              token = [CLS] + token\n","              token_ids = tokenizer.convert_tokens_to_ids(token)\n","              pad_size = config.cmt_max_len\n","              if pad_size:\n","                  if len(token) < pad_size:\n","                      mask = [1] * len(token_ids) + [0] * (pad_size - len(token))\n","                      token_ids += ([0] * (pad_size - len(token)))\n","                  else:\n","                      mask = [1] * pad_size\n","                      token_ids = token_ids[:pad_size]\n","                      seq_len = pad_size\n","              contents.append((token_ids, int(label), seq_len, mask))\n","        return contents\n","    train = load_dataset(config.train_df, config.pad_size)\n","    test = load_dataset(config.test_df, config.pad_size)\n","    return train, test\n","\n","class DatasetIterater(object):\n","    def __init__(self, batches, batch_size, device, mode):\n","        self.batch_size = batch_size\n","        self.batches = batches\n","        self.n_batches = len(batches) // batch_size\n","        self.residue = False  # if n_batch is an interger\n","        if len(batches) % self.n_batches != 0:\n","            self.residue = True\n","        self.index = 0\n","        self.device = device   \n","        self.mode = mode   \n","          \n","    def _to_tensor(self, datas):\n","        x = torch.LongTensor([_[0] for _ in datas]).to(self.device)\n","        y = torch.LongTensor([_[1] for _ in datas]).to(self.device)\n","        seq_len = torch.LongTensor([_[2] for _ in datas]).to(self.device)\n","        mask = torch.LongTensor([_[3] for _ in datas]).to(self.device)\n","        if self.mode==\"branch\":\n","          last_sep = torch.LongTensor([_[4] for _ in datas]).to(self.device)\n","          return (x, seq_len, mask, last_sep), y\n","        else:\n","          return (x, seq_len, mask), y\n","\n","    def __next__(self):\n","        if self.residue and self.index == self.n_batches:\n","            batches = self.batches[self.index * self.batch_size: len(self.batches)]\n","            self.index += 1\n","            batches = self._to_tensor(batches)\n","            return batches\n","        elif self.index >= self.n_batches:\n","            self.index = 0\n","            raise StopIteration\n","        else:\n","            batches = self.batches[self.index * self.batch_size: (self.index + 1) * self.batch_size]\n","            self.index += 1\n","            batches = self._to_tensor(batches)\n","            return batches\n","\n","    def __iter__(self):\n","        return self\n","\n","    def __len__(self):\n","        if self.residue:\n","            return self.n_batches + 1\n","        else:\n","            return self.n_batches\n","\n","def build_iterator(dataset, config, mode='branch'):\n","    iter = DatasetIterater(dataset, config.batch_size, config.device, mode)\n","    return iter\n","\n","\n","def get_time_dif(start_time):\n","    \"\"\"get used time\"\"\"\n","    end_time = time.time()\n","    time_dif = end_time - start_time\n","    return timedelta(seconds=int(round(time_dif)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UHxRUoTWMM4k"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","from sklearn import metrics\n","import time\n","\n","def train(config, model, train_iter, dev_iter, test_iter):\n","    start_time = time.time()\n","    model.train()\n","    bert_params = list(map(id, model.bert.parameters()))\n","    other_params = filter(lambda p: id(p) not in bert_params, model.parameters())\n","    optimizer = torch.optim.AdamW([\n","             {'params': other_params, \"lr\": 1e-4},\n","             {'params': model.bert.parameters(), 'lr':config.learning_rate}])\n","    total_batch = 0  # record batch number\n","    dev_best_loss = float('inf')\n","    last_improve = 0  # record the batch number of last improvement\n","    flag = False  # if improved\n","    model.train()\n","    train_score=[]\n","    test_score=[]\n","    for epoch in range(config.num_epochs):\n","        print('Epoch [{}/{}]'.format(epoch + 1, config.num_epochs))\n","        for i, (trains, labels) in enumerate(train_iter):\n","            outputs = model(trains)\n","            model.zero_grad()\n","            loss = F.cross_entropy(outputs,labels)\n","            loss.backward()\n","            optimizer.step()\n","            if total_batch % 50 == 0:\n","                # print performance\n","                true = labels.data.cpu()\n","                predic = torch.max(outputs.data, 1)[1].cpu()\n","                train_acc = metrics.accuracy_score(true, predic)\n","                dev_acc, dev_loss = evaluate(config, model, dev_iter)\n","                if dev_loss < dev_best_loss:\n","                    dev_best_loss = dev_loss\n","                    torch.save(model.state_dict(), config.save_path)\n","                    improve = '*'\n","                    last_improve = total_batch\n","                else:\n","                    improve = ''\n","                time_dif = get_time_dif(start_time)\n","                msg = 'Iter: {0:>6},  Train Loss: {1:>5.2},  Train Acc: {2:>6.2%},  Val Loss: {3:>5.2},  Val Acc: {4:>6.2%},  Time: {5} {6}'\n","                print(msg.format(total_batch, loss.item(), train_acc, dev_loss, dev_acc, time_dif, improve))\n","                train_score.append(loss.item())\n","                test_score.append(dev_loss)\n","                model.train()\n","            total_batch += 1\n","            if total_batch - last_improve > config.require_improvement:\n","              # early stopping\n","              print(\"No optimization for a long time, auto-stopping...\")\n","              flag = True\n","              break\n","        if flag:\n","          break\n","    return None\n","\n","def evaluate(config, model, data_iter, test=False):\n","    model.eval()\n","    loss_total = 0\n","    predict_all = np.array([], dtype=int)\n","    labels_all = np.array([], dtype=int)\n","    with torch.no_grad():\n","        for texts, labels in data_iter:\n","            outputs = model(texts)\n","            loss = F.cross_entropy(outputs, labels)\n","            loss_total += loss\n","            labels = labels.data.cpu().numpy()\n","            predic = torch.max(outputs.data, 1)[1].cpu().numpy()\n","            labels_all = np.append(labels_all, labels)\n","            predict_all = np.append(predict_all, predic)\n","    acc = metrics.accuracy_score(labels_all, predict_all)\n","    if test:\n","        f1 = metrics.f1_score(labels_all, predict_all,average='macro')\n","        return predict_all, f1\n","    return acc, loss_total / len(data_iter)\n","\n","def test(config, model, test_iter, df_test):\n","# test\n","  model.load_state_dict(torch.load(config.save_path))\n","  start_time = time.time()\n","  predict_all,  f1 = evaluate(config, model, test_iter, test=True)\n","  df_test['predicted'] = list(predict_all)\n","  return df_test, f1\n","\n","def get_depth_f1(df,depth):\n","  if depth < 5:\n","    df_depth=df.loc[df['depth']==depth]\n","  else:\n","    df_depth=df.loc[df['depth']>=depth]\n","  f1 = metrics.f1_score(df_depth['label'].to_list(),df_depth['predicted'].to_list(),average='macro')\n","  return f1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q-J0mHD-HtXo"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from transformers import BertModel, BertTokenizer\n","\n","class BERT_CNN_Config(object):\n","    def __init__(self, dataset, df_train, df_test):\n","        self.model_name = 'Branch_BERT'\n","        self.train_df = df_train  # train set\n","        self.dev_df = df_test  # vad set\n","        self.test_df = df_test  # test set\n","        self.class_list = [0, 1, 2]  #  class list\n","        self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'    \n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # device\n","        self.require_improvement = 500      # require improvement batches for early stopping\n","        self.num_classes = len(self.class_list)                   \n","        self.num_epochs = 100                            \n","        self.batch_size = 16      \n","        self.pad_size = 500 #padding size for subbranches\n","        self.cmt_max_len = 200   # the max size of vecter for each instance after SR\n","        self.learning_rate = 1e-5  \n","        self.bert_path = '/content/drive/MyDrive/Prediction/pretrained_BERT'\n","        self.tokenizer = BertTokenizer.from_pretrained(self.bert_path)\n","        self.hidden_size = 768 \n","        self.filter_sizes = (2, 3, 4) # filter size\n","        self.num_filters = 32# filter number\n","        self.dropout = 0.5# droptout rate\n","\n","class Global_Pooling_Config(object):\n","    def __init__(self, dataset, df_train, df_test):\n","        self.model_name = 'Branch_BERT w/o CFE'\n","        self.train_df = df_train  \n","        self.dev_df = df_test  \n","        self.test_df = df_test  \n","        self.class_list = [0, 1, 2]  \n","        self.num_classes = len(self.class_list)       \n","        self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'        \n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n","        self.require_improvement = 500                              \n","        self.num_epochs = 100                                       \n","        self.batch_size = 16                              \n","        self.pad_size = 500\n","        self.cmt_max_len = 200                                            \n","        self.learning_rate = 1e-5                                       \n","        self.bert_path = '/content/drive/MyDrive/Prediction/pretrained_BERT'\n","        self.tokenizer = BertTokenizer.from_pretrained(self.bert_path)\n","        self.hidden_size = 768\n","        self.dropout = 0.5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fsbWRk50YWMl"},"outputs":[],"source":["class BranchBert_CNN(nn.Module):\n","    '''Branch-BERT'''\n","    def __init__(self, config, mode='branch'):\n","        super(BranchBert_CNN, self).__init__()\n","        self.bert = BertModel.from_pretrained(config.bert_path)\n","        self.mode = mode\n","        for param in self.bert.parameters():\n","            param.requires_grad = True\n","        self.convs = nn.ModuleList(\n","            [nn.Conv2d(in_channels=1, out_channels=config.num_filters, kernel_size=(k, config.hidden_size)) for k in config.filter_sizes]\n","        )\n","        self.droptout = nn.Dropout(config.dropout)\n","        self.fc = nn.Linear(config.num_filters * len(config.filter_sizes), config.num_classes)\n","    def conv_and_pool(self, x, conv):#convulutional feature extraction layer\n","        x = conv(x)\n","        x = F.relu(x)\n","        x = x.squeeze(3)\n","        size = x.size(2)\n","        x = F.max_pool1d(x, size)\n","        x = x.squeeze(2)\n","        return x\n","    def forward(self, x):\n","        context = x[0]  # input sentences\n","        seq_len = x[1]\n","        mask = x[2]  \n","        cut_idx = x[3] # the position of the last [SEP]\n","        output = self.bert(context, attention_mask=mask)#SR mudule\n","        last_hidden_state = output.last_hidden_state #(batch_size,max_lenth, hidden)\n","        ts_list=[]\n","        for i in range(cut_idx.size()[0]):  \n","          temp_tensor=last_hidden_state[i,cut_idx[i]:seq_len[i],:].unsqueeze(0)#(batch_size, *real_lenth, hidden_size)   \n","          diff= int(config.cmt_max_len - (seq_len[i] - (cut_idx[i])))\n","          if diff>0:\n","            zero_pad = torch.zeros(1, diff , config.hidden_size).to(config.device)\n","            a = torch.cat([temp_tensor,zero_pad],dim=1) #zero-padding\n","          else:\n","            a=last_hidden_state[i,cut_idx[i]:cut_idx[i]+config.cmt_max_len,:].unsqueeze(0) #cutting \n","          ts_list.append(a)\n","        last_hidden_state= torch.cat(ts_list,dim=0)\n","        out = last_hidden_state.unsqueeze(1) # ([batch_size, 1, padding_size, hidden_size])\n","        out = torch.cat([self.conv_and_pool(out, conv)for conv in self.convs], 1) #CFE module\n","        out = self.droptout(out)\n","        out = self.fc(out)#classification\n","        return out\n","\n","class Branch_Averaging_Model(nn.Module):\n","    '''Branch-BERT w/o CFE'''  \n","    def __init__(self, config, mode='branch'):\n","        super(Branch_Averaging_Model, self).__init__()\n","        self.config = config\n","        self.mode = mode\n","        self.bert = BertModel.from_pretrained(config.bert_path)\n","        for param in self.bert.parameters():\n","            param.requires_grad = True\n","        self.droptout = nn.Dropout(config.dropout)\n","        self.fc = nn.Linear(config.hidden_size, config.num_classes)\n","        self.global_pooling=nn.AdaptiveAvgPool1d(1)\n","\n","    def forward(self, x):\n","        context = x[0]\n","        seq_len = x[1]\n","        mask = x[2]  \n","        cut_idx = x[3]\n","        output = self.bert(context, attention_mask=mask)\n","        last_hidden_state = output.last_hidden_state #(batch_size,max_lenth, hidden)\n","        ts_list=[]\n","        for i in range(cut_idx.size()[0]):\n","          temp_tensor=last_hidden_state[i,cut_idx[i]:seq_len[i],:].unsqueeze(0)  #(batch_size, *real_lenth, hidden_size)\n","          diff= int(self.config.cmt_max_len - (seq_len[i] - (cut_idx[i])))\n","          if diff>0: #need zero padding\n","            zero_pad = torch.zeros(1, diff , self.config.hidden_size).to(self.config.device)\n","            a = torch.cat([temp_tensor,zero_pad],dim=1)\n","          else:# cut\n","            a=last_hidden_state[i,cut_idx[i]:cut_idx[i]+self.config.cmt_max_len,:].unsqueeze(0) \n","          ts_list.append(a)\n","        last_hidden_state= torch.cat(ts_list,dim=0)# ([batch_size, seq_len, hidden_size])\n","        input = last_hidden_state.permute(0, 2, 1)\n","        output = self.global_pooling(input)\n","        output=output.permute(0, 2, 1)\n","        output=output.squeeze(1)# (batch_size,hidden_size)\n","        out = self.droptout(output)\n","        out = self.fc(out)\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mBRgXYuEsKzz"},"outputs":[],"source":["'''Branch-BERT'''\n","f=list()#overall F1\n","f1=list()#F1 depth=1\n","f2=list()#F1 depth=2\n","f3=list()#F1 depth=3\n","f4=list()#F1 depth=4\n","f5=list()#F1 depth>=5\n","\n","for i in range(10):\n","    dataset='/content/drive/MyDrive/Prediction'\n","    df = pd.concat([df2,df_info])\n","    df = df.sample(frac=1, random_state=i)   #random shuffle\n","    cut_idx1 = int(round(0.8 * df.shape[0]))\n","    df_train,df_test = df[:cut_idx1], df[cut_idx1:]\n","    config = BERT_CNN_Config(dataset,df_train,df_test)\n","    np.random.seed(1)\n","    torch.manual_seed(1)\n","    torch.cuda.manual_seed_all(1)\n","    torch.backends.cudnn.deterministic = True  # ensure the results can be re-implemented\n","    start_time = time.time()\n","    print(\"Loading data...\")\n","    train_data, test_data = build_dataset(config)\n","    train_iter = build_iterator(train_data, config)\n","    dev_iter = build_iterator(test_data, config)\n","    test_iter = build_iterator(test_data, config)\n","    time_dif = get_time_dif(start_time)\n","    print(\"Time usage:\", time_dif)\n","    model = BranchBert_CNN(config).to(config.device)  #Branch-BERT\n","    train(config, model, train_iter, dev_iter, test_iter)\n","    df_test,f_score = test(config, model, test_iter, df_test)\n","    filename=\"/content/drive/MyDrive/DATA/test_\"+config.model_name+str(i)+'.csv'\n","    df_test.to_csv(filename)\n","    f.append(f_score)\n","    f1.append(get_depth_f1(df_test,1))\n","    f2.append(get_depth_f1(df_test,2))\n","    f3.append(get_depth_f1(df_test,3))\n","    f4.append(get_depth_f1(df_test,4))\n","    f5.append(get_depth_f1(df_test,5))\n","\n","print(\"f: \"+str(np.mean(f))+\"+\"+str(np.std(f, ddof = 1)))\n","print(\"f1: \"+str(np.mean(f1))+\"+\"+str(np.std(f1, ddof = 1)))\n","print(\"f2: \"+str(np.mean(f2))+\"+\"+str(np.std(f2, ddof = 1)))\n","print(\"f3: \"+str(np.mean(f3))+\"+\"+str(np.std(f3, ddof = 1)))\n","print(\"f4: \"+str(np.mean(f4))+\"+\"+str(np.std(f4, ddof = 1)))\n","print(\"f5: \"+str(np.mean(f5))+\"+\"+str(np.std(f5, ddof = 1)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AqYQOyf9MgdD"},"outputs":[],"source":["'''Branch-BERT w/o CFE'''\n","f=list()\n","f1=list()\n","f2=list()\n","f3=list()\n","f4=list()\n","f5=list()\n","\n","for i in range(10):\n","    dataset='/content/drive/MyDrive/Prediction'\n","    df = pd.concat([df2,df_info])\n","    df = df.sample(frac=1, random_state=i)   \n","    cut_idx1 = int(round(0.8 * df.shape[0]))\n","    df_train,df_test = df[:cut_idx1], df[cut_idx1:]\n","    config = Global_Pooling_Config(dataset,df_train,df_test)\n","    np.random.seed(1)\n","    torch.manual_seed(1)\n","    torch.cuda.manual_seed_all(1)\n","    torch.backends.cudnn.deterministic = True \n","    start_time = time.time()\n","    print(\"Loading data...\")\n","    train_data, test_data = build_dataset(config)\n","    train_iter = build_iterator(train_data, config)\n","    dev_iter = build_iterator(test_data, config)\n","    test_iter = build_iterator(test_data, config)\n","    time_dif = get_time_dif(start_time)\n","    print(\"Time usage:\", time_dif)\n","    model = Branch_Averaging_Model(config).to(config.device) #Branch-BERT w/o CFE\n","    train(config, model, train_iter, dev_iter, test_iter)\n","    df_test,f_score = test(config, model, test_iter, df_test)\n","    filename=\"/content/drive/MyDrive/DATA/test_\"+config.model_name+str(i)+'.csv'\n","    df_test.to_csv(filename)\n","    f.append(f_score)\n","    f1.append(get_depth_f1(df_test,1))\n","    f2.append(get_depth_f1(df_test,2))\n","    f3.append(get_depth_f1(df_test,3))\n","    f4.append(get_depth_f1(df_test,4))\n","    f5.append(get_depth_f1(df_test,5))\n","\n","print(\"f: \"+str(np.mean(f))+\"+\"+str(np.std(f, ddof = 1)))\n","print(\"f1: \"+str(np.mean(f1))+\"+\"+str(np.std(f1, ddof = 1)))\n","print(\"f2: \"+str(np.mean(f2))+\"+\"+str(np.std(f2, ddof = 1)))\n","print(\"f3: \"+str(np.mean(f3))+\"+\"+str(np.std(f3, ddof = 1)))\n","print(\"f4: \"+str(np.mean(f4))+\"+\"+str(np.std(f4, ddof = 1)))\n","print(\"f5: \"+str(np.mean(f5))+\"+\"+str(np.std(f5, ddof = 1)))"]},{"cell_type":"markdown","metadata":{"id":"Rjuffl54JfA7"},"source":["# Non-Branch Model\n","(BERT & Branch-BERT w/o SR)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4BlJ-bjJJtjU"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from transformers import BertModel, BertTokenizer\n","from transformers import ElectraModel, ElectraTokenizer\n","\n","class BERT_CNN_Config(object):\n","    def __init__(self, dataset, df_train, df_test):\n","        self.model_name = 'BERT_CNN'\n","        self.train_df = df_train  \n","        self.dev_df = df_test  \n","        self.test_df = df_test  \n","        self.class_list = [0, 1, 2]  \n","        self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'      \n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   \n","        self.require_improvement = 500                               \n","        self.num_classes = len(self.class_list)                         \n","        self.num_epochs = 100                                            \n","        self.batch_size = 16                                   \n","        self.pad_size = 200\n","        self.learning_rate = 1e-5                                      \n","        self.bert_path = '/content/drive/MyDrive/Prediction/pretrained_BERT/'\n","        self.tokenizer = BertTokenizer.from_pretrained(self.bert_path)\n","        self.hidden_size = 768\n","        self.filter_sizes = (2, 3, 4) \n","        self.num_filters = 32\n","        self.dropout = 0.5\n","\n","class BERT_Config(object):\n","    def __init__(self, dataset, df_train, df_test):\n","        self.model_name = 'BERT'\n","        self.train_df = df_train  \n","        self.dev_df = df_test  \n","        self.test_df = df_test  \n","        self.class_list = [0, 1, 2]  \n","        self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'        \n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n","        self.require_improvement = 300                              \n","        self.num_classes = len(self.class_list)                        \n","        self.num_epochs = 100                                      \n","        self.batch_size = 16                                  \n","        self.pad_size = 200\n","        self.learning_rate = 1e-5                                      \n","        self.bert_path = '/content/drive/MyDrive/Prediction/pretrained_BERT/'\n","        self.tokenizer = BertTokenizer.from_pretrained(self.bert_path)\n","        self.hidden_size = 768\n","        self.dropout = 0.5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wbrL6bVvJtjU"},"outputs":[],"source":["class BERT_Model(nn.Module):\n","    '''BERT'''\n","    def __init__(self, config):\n","        super(BERT_Model, self).__init__()\n","        self.config = config\n","        self.bert = BertModel.from_pretrained(config.bert_path)\n","        for param in self.bert.parameters():\n","            param.requires_grad = True\n","        self.dropout = nn.Dropout(config.dropout)\n","        self.fc = nn.Linear(config.hidden_size, config.num_classes)\n","\n","    def forward(self, x):\n","        context = x[0]  # 输入的句子\n","        seq_len = x[1]\n","        mask = x[2]  # 对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]\n","        output = self.bert(context, attention_mask=mask)\n","        out = self.dropout(output.pooler_output)\n","        out = self.fc(out)\n","        return out\n","\n","class BERT_CNN(nn.Module):\n","    '''Branch-BERT w/o SR'''\n","    def __init__(self, config):\n","        super(BERT_CNN, self).__init__()\n","        self.bert = BertModel.from_pretrained(config.bert_path)\n","        for param in self.bert.parameters():\n","            param.requires_grad = True\n","        self.convs = nn.ModuleList(\n","            [nn.Conv2d(1, config.num_filters, (k, config.hidden_size)) for k in config.filter_sizes])\n","        self.dropout = nn.Dropout(config.dropout)\n","        self.fc_cnn = nn.Linear(config.num_filters * len(config.filter_sizes), config.num_classes)\n","\n","    def conv_and_pool(self, x, conv):\n","        x = F.relu(conv(x)).squeeze(3)\n","        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n","        return x\n","\n","    def forward(self, x):\n","        context = x[0]  # 输入的句子\n","        mask = x[2]  # 对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]\n","        output = self.bert(context, attention_mask=mask)\n","        last_hidden_state = output.last_hidden_state #(batch_size,max_lenth, hidden)\n","        out = last_hidden_state.unsqueeze(1)\n","        out = torch.cat([self.conv_and_pool(out, conv) for conv in self.convs], 1)\n","        out = self.dropout(out)\n","        out = self.fc_cnn(out)\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"72H9n_dFJtjV"},"outputs":[],"source":["'''Branch-BERT w/o SR'''\n","f=list()\n","f1=list()\n","f2=list()\n","f3=list()\n","f4=list()\n","f5=list()\n","\n","\n","for i in range(10):\n","    dataset='/content/drive/MyDrive/Prediction'\n","    df = pd.concat([df2,df_info])\n","    df = df.sample(frac=1, random_state=i)   \n","    cut_idx1 = int(round(0.8 * df.shape[0]))\n","    df_train,df_test = df[:cut_idx1], df[cut_idx1:]\n","    config = BERT_CNN_Config(dataset,df_train,df_test)\n","    np.random.seed(1)\n","    torch.manual_seed(1)\n","    torch.cuda.manual_seed_all(1)\n","    torch.backends.cudnn.deterministic = True  \n","    start_time = time.time()\n","    print(\"Loading data...\")\n","    train_data, test_data = build_dataset(config,mode='non-branch'))\n","    train_iter = build_iterator(train_data, config,mode='non-branch'))\n","    dev_iter = build_iterator(test_data, config,mode='non-branch'))\n","    test_iter = build_iterator(test_data, config,mode='non-branch'))\n","    time_dif = get_time_dif(start_time)\n","    print(\"Time usage:\", time_dif)\n","    model = BERT_CNN(config).to(config.device) '''Branch-BERT w/o SR'''\n","    train(config, model, train_iter, dev_iter, test_iter)\n","    df_test,f_score = test(config, model, test_iter, df_test)\n","    filename=\"/content/drive/MyDrive/DATA/test_\"+config.model_name+str(i)+'.csv'\n","    df_test.to_csv(filename)\n","    f.append(f_score)\n","    f1.append(get_depth_f1(df_test,1))\n","    f2.append(get_depth_f1(df_test,2))\n","    f3.append(get_depth_f1(df_test,3))\n","    f4.append(get_depth_f1(df_test,4))\n","    f5.append(get_depth_f1(df_test,5))\n","\n","print(\"f: \"+str(np.mean(f))+\"+\"+str(np.std(f, ddof = 1)))\n","print(\"f1: \"+str(np.mean(f1))+\"+\"+str(np.std(f1, ddof = 1)))\n","print(\"f2: \"+str(np.mean(f2))+\"+\"+str(np.std(f2, ddof = 1)))\n","print(\"f3: \"+str(np.mean(f3))+\"+\"+str(np.std(f3, ddof = 1)))\n","print(\"f4: \"+str(np.mean(f4))+\"+\"+str(np.std(f4, ddof = 1)))\n","print(\"f5: \"+str(np.mean(f5))+\"+\"+str(np.std(f5, ddof = 1)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":867997,"status":"ok","timestamp":1649724851770,"user":{"displayName":"刘耘果","userId":"11745284739417236882"},"user_tz":-480},"id":"hEFd-t_qJXYV","outputId":"f47f663b-f593-4ab5-8172-fe982a227326"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading data...\n","Time usage: 0:00:02\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/drive/MyDrive/Prediction/pretrained_BERT/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/100]\n","Iter:      0,  Train Loss:  0.96,  Train Acc: 50.00%,  Val Loss:   1.0,  Val Acc: 38.64%,  Time: 0:00:08 *\n","Iter:     50,  Train Loss:   1.0,  Train Acc: 50.00%,  Val Loss:  0.94,  Val Acc: 52.34%,  Time: 0:00:18 *\n","Iter:    100,  Train Loss:  0.88,  Train Acc: 62.50%,  Val Loss:  0.91,  Val Acc: 53.28%,  Time: 0:00:27 *\n","Iter:    150,  Train Loss:   1.0,  Train Acc: 18.75%,  Val Loss:  0.88,  Val Acc: 55.32%,  Time: 0:00:37 *\n","Iter:    200,  Train Loss:  0.82,  Train Acc: 43.75%,  Val Loss:  0.85,  Val Acc: 56.60%,  Time: 0:00:46 *\n","Iter:    250,  Train Loss:  0.98,  Train Acc: 50.00%,  Val Loss:  0.89,  Val Acc: 55.74%,  Time: 0:00:55 \n","Epoch [2/100]\n","Iter:    300,  Train Loss:  0.96,  Train Acc: 50.00%,  Val Loss:  0.83,  Val Acc: 59.83%,  Time: 0:01:04 *\n","Iter:    350,  Train Loss:  0.87,  Train Acc: 62.50%,  Val Loss:  0.86,  Val Acc: 57.28%,  Time: 0:01:13 \n","Iter:    400,  Train Loss:  0.76,  Train Acc: 56.25%,  Val Loss:  0.83,  Val Acc: 58.13%,  Time: 0:01:21 \n","Iter:    450,  Train Loss:  0.65,  Train Acc: 75.00%,  Val Loss:  0.89,  Val Acc: 58.98%,  Time: 0:01:29 \n","Iter:    500,  Train Loss:  0.67,  Train Acc: 62.50%,  Val Loss:  0.88,  Val Acc: 57.53%,  Time: 0:01:38 \n","Iter:    550,  Train Loss:  0.75,  Train Acc: 75.00%,  Val Loss:  0.84,  Val Acc: 60.60%,  Time: 0:01:46 \n","Epoch [3/100]\n","Iter:    600,  Train Loss:  0.76,  Train Acc: 50.00%,  Val Loss:  0.83,  Val Acc: 62.38%,  Time: 0:01:55 \n","No optimization for a long time, auto-stopping...\n","Loading data...\n","Time usage: 0:00:02\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/drive/MyDrive/Prediction/pretrained_BERT/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/100]\n","Iter:      0,  Train Loss:   1.1,  Train Acc: 31.25%,  Val Loss:   1.0,  Val Acc: 37.70%,  Time: 0:00:04 *\n","Iter:     50,  Train Loss:  0.98,  Train Acc: 43.75%,  Val Loss:  0.94,  Val Acc: 50.98%,  Time: 0:00:13 *\n","Iter:    100,  Train Loss:  0.91,  Train Acc: 43.75%,  Val Loss:  0.92,  Val Acc: 51.66%,  Time: 0:00:23 *\n","Iter:    150,  Train Loss:   1.0,  Train Acc: 25.00%,  Val Loss:  0.89,  Val Acc: 55.74%,  Time: 0:00:32 *\n","Iter:    200,  Train Loss:   1.0,  Train Acc: 43.75%,  Val Loss:  0.89,  Val Acc: 54.21%,  Time: 0:00:42 *\n","Iter:    250,  Train Loss:  0.77,  Train Acc: 43.75%,  Val Loss:  0.86,  Val Acc: 60.00%,  Time: 0:00:51 *\n","Epoch [2/100]\n","Iter:    300,  Train Loss:  0.87,  Train Acc: 68.75%,  Val Loss:  0.84,  Val Acc: 58.98%,  Time: 0:01:01 *\n","Iter:    350,  Train Loss:  0.86,  Train Acc: 50.00%,  Val Loss:  0.83,  Val Acc: 60.77%,  Time: 0:01:10 *\n","Iter:    400,  Train Loss:   1.1,  Train Acc: 50.00%,  Val Loss:  0.81,  Val Acc: 60.51%,  Time: 0:01:20 *\n","Iter:    450,  Train Loss:  0.77,  Train Acc: 50.00%,  Val Loss:  0.82,  Val Acc: 60.60%,  Time: 0:01:28 \n","Iter:    500,  Train Loss:   0.8,  Train Acc: 75.00%,  Val Loss:  0.83,  Val Acc: 62.72%,  Time: 0:01:37 \n","Iter:    550,  Train Loss:  0.82,  Train Acc: 50.00%,  Val Loss:   0.8,  Val Acc: 61.11%,  Time: 0:01:46 *\n","Epoch [3/100]\n","Iter:    600,  Train Loss:  0.88,  Train Acc: 62.50%,  Val Loss:  0.81,  Val Acc: 61.96%,  Time: 0:01:54 \n","Iter:    650,  Train Loss:  0.73,  Train Acc: 62.50%,  Val Loss:   0.8,  Val Acc: 62.47%,  Time: 0:02:03 \n","Iter:    700,  Train Loss:  0.54,  Train Acc: 68.75%,  Val Loss:  0.79,  Val Acc: 63.49%,  Time: 0:02:12 *\n","Iter:    750,  Train Loss:  0.55,  Train Acc: 62.50%,  Val Loss:  0.81,  Val Acc: 61.87%,  Time: 0:02:21 \n","Iter:    800,  Train Loss:  0.76,  Train Acc: 68.75%,  Val Loss:  0.82,  Val Acc: 61.62%,  Time: 0:02:29 \n","Iter:    850,  Train Loss:  0.76,  Train Acc: 56.25%,  Val Loss:  0.84,  Val Acc: 61.36%,  Time: 0:02:38 \n","Epoch [4/100]\n","Iter:    900,  Train Loss:  0.67,  Train Acc: 62.50%,  Val Loss:  0.84,  Val Acc: 61.19%,  Time: 0:02:46 \n","Iter:    950,  Train Loss:  0.87,  Train Acc: 37.50%,  Val Loss:  0.82,  Val Acc: 62.64%,  Time: 0:02:54 \n","Iter:   1000,  Train Loss:  0.55,  Train Acc: 75.00%,  Val Loss:  0.88,  Val Acc: 61.28%,  Time: 0:03:03 \n","No optimization for a long time, auto-stopping...\n","Loading data...\n","Time usage: 0:00:02\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/drive/MyDrive/Prediction/pretrained_BERT/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/100]\n","Iter:      0,  Train Loss:   1.1,  Train Acc: 37.50%,  Val Loss:   1.0,  Val Acc: 45.53%,  Time: 0:00:04 *\n","Iter:     50,  Train Loss:  0.96,  Train Acc: 50.00%,  Val Loss:  0.95,  Val Acc: 51.15%,  Time: 0:00:13 *\n","Iter:    100,  Train Loss:  0.92,  Train Acc: 43.75%,  Val Loss:  0.92,  Val Acc: 54.04%,  Time: 0:00:23 *\n","Iter:    150,  Train Loss:   1.1,  Train Acc: 43.75%,  Val Loss:  0.97,  Val Acc: 51.15%,  Time: 0:00:31 \n","Iter:    200,  Train Loss:  0.76,  Train Acc: 81.25%,  Val Loss:   0.9,  Val Acc: 55.06%,  Time: 0:00:41 *\n","Iter:    250,  Train Loss:   1.2,  Train Acc: 43.75%,  Val Loss:  0.87,  Val Acc: 58.47%,  Time: 0:00:50 *\n","Epoch [2/100]\n","Iter:    300,  Train Loss:  0.75,  Train Acc: 68.75%,  Val Loss:  0.84,  Val Acc: 59.66%,  Time: 0:01:00 *\n","Iter:    350,  Train Loss:  0.69,  Train Acc: 81.25%,  Val Loss:  0.85,  Val Acc: 60.51%,  Time: 0:01:08 \n","Iter:    400,  Train Loss:   0.7,  Train Acc: 68.75%,  Val Loss:  0.83,  Val Acc: 60.51%,  Time: 0:01:18 *\n","Iter:    450,  Train Loss:  0.81,  Train Acc: 56.25%,  Val Loss:  0.81,  Val Acc: 61.62%,  Time: 0:01:27 *\n","Iter:    500,  Train Loss:  0.56,  Train Acc: 87.50%,  Val Loss:  0.82,  Val Acc: 62.30%,  Time: 0:01:36 \n","Iter:    550,  Train Loss:  0.59,  Train Acc: 75.00%,  Val Loss:  0.85,  Val Acc: 58.64%,  Time: 0:01:44 \n","Epoch [3/100]\n","Iter:    600,  Train Loss:  0.57,  Train Acc: 75.00%,  Val Loss:  0.88,  Val Acc: 59.66%,  Time: 0:01:52 \n","Iter:    650,  Train Loss:  0.55,  Train Acc: 75.00%,  Val Loss:  0.85,  Val Acc: 62.04%,  Time: 0:02:01 \n","Iter:    700,  Train Loss:  0.56,  Train Acc: 68.75%,  Val Loss:  0.82,  Val Acc: 63.15%,  Time: 0:02:09 \n","Iter:    750,  Train Loss:  0.73,  Train Acc: 68.75%,  Val Loss:  0.84,  Val Acc: 60.17%,  Time: 0:02:17 \n","No optimization for a long time, auto-stopping...\n","Loading data...\n","Time usage: 0:00:02\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/drive/MyDrive/Prediction/pretrained_BERT/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/100]\n","Iter:      0,  Train Loss:   1.1,  Train Acc: 31.25%,  Val Loss:  0.99,  Val Acc: 42.55%,  Time: 0:00:04 *\n","Iter:     50,  Train Loss:  0.82,  Train Acc: 62.50%,  Val Loss:  0.95,  Val Acc: 48.34%,  Time: 0:00:13 *\n","Iter:    100,  Train Loss:  0.92,  Train Acc: 56.25%,  Val Loss:  0.91,  Val Acc: 51.91%,  Time: 0:00:23 *\n","Iter:    150,  Train Loss:   1.0,  Train Acc: 50.00%,  Val Loss:  0.85,  Val Acc: 57.11%,  Time: 0:00:32 *\n","Iter:    200,  Train Loss:   1.0,  Train Acc: 56.25%,  Val Loss:  0.86,  Val Acc: 52.68%,  Time: 0:00:41 \n","Iter:    250,  Train Loss:  0.67,  Train Acc: 81.25%,  Val Loss:  0.81,  Val Acc: 61.11%,  Time: 0:00:50 *\n","Epoch [2/100]\n","Iter:    300,  Train Loss:  0.87,  Train Acc: 43.75%,  Val Loss:   0.8,  Val Acc: 60.85%,  Time: 0:01:00 *\n","Iter:    350,  Train Loss:  0.92,  Train Acc: 56.25%,  Val Loss:  0.81,  Val Acc: 59.57%,  Time: 0:01:08 \n","Iter:    400,  Train Loss:  0.76,  Train Acc: 56.25%,  Val Loss:  0.79,  Val Acc: 61.87%,  Time: 0:01:18 *\n","Iter:    450,  Train Loss:  0.95,  Train Acc: 43.75%,  Val Loss:  0.85,  Val Acc: 58.72%,  Time: 0:01:26 \n","Iter:    500,  Train Loss:   1.0,  Train Acc: 50.00%,  Val Loss:   0.8,  Val Acc: 61.96%,  Time: 0:01:34 \n","Iter:    550,  Train Loss:   1.2,  Train Acc: 31.25%,  Val Loss:  0.88,  Val Acc: 57.62%,  Time: 0:01:43 \n","Epoch [3/100]\n","Iter:    600,  Train Loss:  0.57,  Train Acc: 81.25%,  Val Loss:  0.78,  Val Acc: 62.38%,  Time: 0:01:52 *\n","Iter:    650,  Train Loss:  0.69,  Train Acc: 68.75%,  Val Loss:  0.77,  Val Acc: 64.43%,  Time: 0:02:02 *\n","Iter:    700,  Train Loss:   0.5,  Train Acc: 81.25%,  Val Loss:  0.77,  Val Acc: 63.57%,  Time: 0:02:10 \n","Iter:    750,  Train Loss:  0.66,  Train Acc: 56.25%,  Val Loss:  0.78,  Val Acc: 62.81%,  Time: 0:02:19 \n","Iter:    800,  Train Loss:  0.74,  Train Acc: 68.75%,  Val Loss:   0.8,  Val Acc: 62.98%,  Time: 0:02:27 \n","Iter:    850,  Train Loss:  0.85,  Train Acc: 56.25%,  Val Loss:  0.79,  Val Acc: 63.40%,  Time: 0:02:35 \n","Epoch [4/100]\n","Iter:    900,  Train Loss:  0.96,  Train Acc: 50.00%,  Val Loss:  0.76,  Val Acc: 64.85%,  Time: 0:02:45 *\n","Iter:    950,  Train Loss:  0.47,  Train Acc: 81.25%,  Val Loss:  0.82,  Val Acc: 63.06%,  Time: 0:02:53 \n","Iter:   1000,  Train Loss:  0.51,  Train Acc: 81.25%,  Val Loss:  0.79,  Val Acc: 65.02%,  Time: 0:03:02 \n","Iter:   1050,  Train Loss:  0.56,  Train Acc: 75.00%,  Val Loss:  0.81,  Val Acc: 64.09%,  Time: 0:03:10 \n","Iter:   1100,  Train Loss:  0.95,  Train Acc: 62.50%,  Val Loss:  0.79,  Val Acc: 64.43%,  Time: 0:03:18 \n","Iter:   1150,  Train Loss:  0.67,  Train Acc: 62.50%,  Val Loss:  0.77,  Val Acc: 65.45%,  Time: 0:03:27 \n","Epoch [5/100]\n","Iter:   1200,  Train Loss:  0.45,  Train Acc: 87.50%,  Val Loss:  0.79,  Val Acc: 64.77%,  Time: 0:03:35 \n","No optimization for a long time, auto-stopping...\n","Loading data...\n","Time usage: 0:00:02\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/drive/MyDrive/Prediction/pretrained_BERT/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/100]\n","Iter:      0,  Train Loss:   1.3,  Train Acc: 25.00%,  Val Loss:   1.0,  Val Acc: 43.57%,  Time: 0:00:05 *\n","Iter:     50,  Train Loss:  0.69,  Train Acc: 56.25%,  Val Loss:   1.0,  Val Acc: 49.62%,  Time: 0:00:13 \n","Iter:    100,  Train Loss:  0.96,  Train Acc: 37.50%,  Val Loss:  0.95,  Val Acc: 53.28%,  Time: 0:00:23 *\n","Iter:    150,  Train Loss:  0.79,  Train Acc: 56.25%,  Val Loss:  0.91,  Val Acc: 53.28%,  Time: 0:00:33 *\n","Iter:    200,  Train Loss:  0.79,  Train Acc: 62.50%,  Val Loss:  0.89,  Val Acc: 55.40%,  Time: 0:00:43 *\n","Iter:    250,  Train Loss:  0.84,  Train Acc: 43.75%,  Val Loss:  0.86,  Val Acc: 60.00%,  Time: 0:00:53 *\n","Epoch [2/100]\n","Iter:    300,  Train Loss:   1.0,  Train Acc: 43.75%,  Val Loss:  0.89,  Val Acc: 56.60%,  Time: 0:01:01 \n","Iter:    350,  Train Loss:  0.78,  Train Acc: 75.00%,  Val Loss:  0.84,  Val Acc: 58.64%,  Time: 0:01:11 *\n","Iter:    400,  Train Loss:  0.72,  Train Acc: 56.25%,  Val Loss:  0.83,  Val Acc: 58.04%,  Time: 0:01:20 *\n","Iter:    450,  Train Loss:  0.76,  Train Acc: 68.75%,  Val Loss:  0.85,  Val Acc: 60.34%,  Time: 0:01:29 \n","Iter:    500,  Train Loss:  0.85,  Train Acc: 37.50%,  Val Loss:  0.84,  Val Acc: 58.13%,  Time: 0:01:37 \n","Iter:    550,  Train Loss:  0.74,  Train Acc: 56.25%,  Val Loss:  0.81,  Val Acc: 63.49%,  Time: 0:01:46 *\n","Epoch [3/100]\n","Iter:    600,  Train Loss:  0.71,  Train Acc: 50.00%,  Val Loss:  0.85,  Val Acc: 60.51%,  Time: 0:01:55 \n","Iter:    650,  Train Loss:   0.6,  Train Acc: 62.50%,  Val Loss:  0.83,  Val Acc: 59.57%,  Time: 0:02:03 \n","Iter:    700,  Train Loss:  0.84,  Train Acc: 50.00%,  Val Loss:  0.81,  Val Acc: 61.87%,  Time: 0:02:12 \n","Iter:    750,  Train Loss:  0.74,  Train Acc: 62.50%,  Val Loss:  0.83,  Val Acc: 62.89%,  Time: 0:02:20 \n","Iter:    800,  Train Loss:  0.53,  Train Acc: 81.25%,  Val Loss:  0.84,  Val Acc: 60.17%,  Time: 0:02:28 \n","Iter:    850,  Train Loss:  0.56,  Train Acc: 75.00%,  Val Loss:  0.82,  Val Acc: 62.04%,  Time: 0:02:37 \n","No optimization for a long time, auto-stopping...\n","f: 0.6054150333071904+0.02021335442089282\n","f1: 0.5860845009200774+0.08223245800363785\n","f2: 0.6037529308529365+0.022854425672146874\n","f3: 0.4980756934753029+0.04146250876969134\n","f4: 0.4965951688886733+0.08212687342424436\n","f5: 0.4023778400248988+0.021863592592796716\n","[0.58430173511643, 0.6234195067072931, 0.59050206695815, 0.6299113627037437, 0.5989404950503349]\n","[0.5090030742204655, 0.5103365624687907, 0.5669585157390036, 0.6825332675423139, 0.6615910846298135]\n","[0.5860038119931168, 0.6356890767721447, 0.58909740492615, 0.6204105823239219, 0.5875637782493489]\n","[0.5020051650555126, 0.5328286544257507, 0.4900312639788396, 0.43216374269005847, 0.5333496412263535]\n","[0.6098484848484849, 0.43629343629343625, 0.40648148148148144, 0.5443797581728615, 0.4859726836471023]\n","[0.37609427609427604, 0.38888888888888884, 0.4329004329004329, 0.4013071895424836, 0.41269841269841273]\n"]}],"source":["'''BERT'''\n","f=list()\n","f1=list()\n","f2=list()\n","f3=list()\n","f4=list()\n","f5=li\n","for i in range(10):\n","    dataset='/content/drive/MyDrive/Prediction'\n","    df = pd.concat([df2,df_info])\n","    df = df.sample(frac=1, random_state=i)   \n","    cut_idx1 = int(round(0.8 * df.shape[0]))\n","    df_train,df_test = df[:cut_idx1], df[cut_idx1:]\n","    config = BERT_Config(dataset,df_train,df_test)\n","    np.random.seed(1)\n","    torch.manual_seed(1)\n","    torch.cuda.manual_seed_all(1)\n","    torch.backends.cudnn.deterministic = True  \n","    start_time = time.time()\n","    print(\"Loading data...\")\n","    train_data, test_data = build_dataset(config,mode='non-branch'))\n","    train_iter = build_iterator(train_data, config,mode='non-branch')\n","    dev_iter = build_iterator(test_data, config,mode='non-branch')\n","    test_iter = build_iterator(test_data, config,mode='non-branch')\n","    time_dif = get_time_dif(start_time)\n","    print(\"Time usage:\", time_dif)\n","    model = BERT_Model(config).to(config.device)\n","    train(config, model, train_iter, dev_iter, test_iter)\n","    df_test,f_score = test(config, model, test_iter, df_test)\n","    filename=\"/content/drive/MyDrive/DATA/test_\"+config.model_name+str(i)+'.csv'\n","    df_test.to_csv(filename)\n","    f.append(f_score)\n","    f1.append(get_depth_f1(df_test,1))\n","    f2.append(get_depth_f1(df_test,2))\n","    f3.append(get_depth_f1(df_test,3))\n","    f4.append(get_depth_f1(df_test,4))\n","    f5.append(get_depth_f1(df_test,5))\n","\n","print(\"f: \"+str(np.mean(f))+\"+\"+str(np.std(f, ddof = 1)))\n","print(\"f1: \"+str(np.mean(f1))+\"+\"+str(np.std(f1, ddof = 1)))\n","print(\"f2: \"+str(np.mean(f2))+\"+\"+str(np.std(f2, ddof = 1)))\n","print(\"f3: \"+str(np.mean(f3))+\"+\"+str(np.std(f3, ddof = 1)))\n","print(\"f4: \"+str(np.mean(f4))+\"+\"+str(np.std(f4, ddof = 1)))\n","print(\"f5: \"+str(np.mean(f5))+\"+\"+str(np.std(f5, ddof = 1)))"]},{"cell_type":"markdown","metadata":{"id":"a_2XFAhokmEP"},"source":["# Deep Learning Based Model\n","(TextCNN & Tan)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lb_KnGDNLB_5"},"outputs":[],"source":["\n","'''Load pre-trained word embedding'''\n","emb_dim = 300\n","vocab_dir = \"/content/drive/MyDrive/Prediction/data/vocab.pkl\"\n","filename_trimmed_dir = \"/content/drive/MyDrive/Prediction/data/embedding_CantoStance\"\n","\n","if os.path.exists(vocab_dir):\n","    word_to_id = pkl.load(open(vocab_dir, 'rb'))\n","else:\n","    tokenizer = fenci\n","    #tokenizer = lambda x: [y for y in x]  # 以字为单位构建词表\n","    word_to_id = build_vocab(tokenizer=tokenizer, max_size=MAX_VOCAB_SIZE, min_freq=1,df=df)\n","    pkl.dump(word_to_id, open(vocab_dir, 'wb'))\n","\n","count=0\n","embeddings = np.random.rand(len(word_to_id), emb_dim)\n","print(len(word_to_id))\n","emb_model = gensim.models.KeyedVectors.load_word2vec_format(r'/content/drive/MyDrive/DATA/sgns.wiki.word.bz2')\n","for i in word_to_id.keys():\n","  idx = word_to_id[i]\n","  if i not in emb_model.vocab:\n","    count+=1\n","    emb = [0]* emb_dim\n","  else:\n","    emb = emb_model[i]\n","  embeddings[idx] = np.asarray(emb, dtype='float32')\n","print(count)\n","np.savez_compressed(filename_trimmed_dir, embeddings=embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jARI2e4vo7iu"},"outputs":[],"source":["# coding: UTF-8\n","import os\n","import torch\n","import numpy as np\n","import gensim\n","import numpy as np\n","import pickle as pkl\n","from tqdm import tqdm\n","import time\n","from datetime import timedelta\n","import pycantonese\n","import jieba\n","MAX_VOCAB_SIZE = 20000  # 词表长度限制\n","UNK, PAD = '<UNK>', '<PAD>'  # 未知字，padding符号\n","\n","fenci=jieba.lcut# tonkenizer\n","def build_vocab(tokenizer, max_size, min_freq, df):\n","    vocab_dic = {}\n","    for i in range(len(df)):\n","      content = df['text'].iloc[i]\n","      for word in tokenizer(content):\n","        vocab_dic[word] = vocab_dic.get(word, 0) + 1\n","    vocab_list = sorted([_ for _ in vocab_dic.items() if _[1] >= min_freq], key=lambda x: x[1], reverse=True)[:max_size-2]\n","    vocab_dic = {word_count[0]: idx for idx, word_count in enumerate(vocab_list)}\n","    vocab_dic.update({UNK: len(vocab_dic), PAD: len(vocab_dic) + 1})\n","    return vocab_dic\n","\n","def build_dataset(config):\n","    tokenizer = fenci\n","    if os.path.exists(config.vocab_path):\n","        vocab = pkl.load(open(config.vocab_path, 'rb'))\n","    else:\n","        vocab = build_vocab(config.train_path, tokenizer=tokenizer, max_size=MAX_VOCAB_SIZE, min_freq=1)\n","        pkl.dump(vocab, open(config.vocab_path, 'wb'))\n","    print(f\"Vocab size: {len(vocab)}\")\n","    def load_dataset(data, pad_size=200):\n","        contents = []\n","        for i in range(len(data)):\n","                content = data['text'].iloc[i]\n","                label = data['label'].iloc[i]\n","                words_line = []\n","                token = tokenizer(content)\n","                seq_len = len(token)\n","                if pad_size:\n","                    if len(token) < pad_size:\n","                        token.extend([PAD] * (pad_size - len(token)))\n","                    else:\n","                        token = token[:pad_size]\n","                        seq_len = pad_size\n","                # word to id\n","                for word in token:\n","                    words_line.append(vocab.get(word, vocab.get(UNK)))\n","                contents.append((words_line, int(label), seq_len))\n","        return contents \n","    train = load_dataset(config.train_df, config.pad_size)\n","    test = load_dataset(config.test_df, config.pad_size)\n","    return vocab, train, test\n","\n","\n","class DatasetIterater(object):\n","    def __init__(self, batches, batch_size, device, pad_size, is_Tan):\n","        self.batch_size = batch_size\n","        self.batches = batches\n","        self.n_batches = len(batches) // batch_size\n","        self.residue = False  # 记录batch数量是否为整数\n","        if len(batches) % self.n_batches != 0:\n","            self.residue = True\n","        self.index = 0\n","        self.device = device\n","        self.is_Tan = is_Tan\n","        self.target = '接种新冠疫苗'\n","        tokenizer=fenci\n","        vocab = pkl.load(open(config.vocab_path, 'rb'))\n","        self.target_ids= [vocab.get(word, vocab.get(UNK)) for word in tokenizer(self.target)]#target embedding ids\n","        if len(self.target_ids) < pad_size:\n","            self.target_ids.extend([vocab.get(PAD)] * (pad_size - len(self.target_ids)))\n","        \n","    def _to_tensor(self, datas):\n","        x = torch.LongTensor([_[0] for _ in datas]).to(self.device)\n","        y = torch.LongTensor([_[1] for _ in datas]).to(self.device)\n","        # pad前的长度(超过pad_size的设为pad_size)\n","        seq_len = torch.LongTensor([_[2] for _ in datas]).to(self.device)\n","        if self.is_Tan:\n","          t = torch.LongTensor([self.target_ids for _ in range(len(datas))]).to(self.device)\n","          return (x, t, seq_len), y\n","        else:\n","          return (x, seq_len), y\n","\n","    def __next__(self):\n","        if self.residue and self.index == self.n_batches:\n","            batches = self.batches[self.index * self.batch_size: len(self.batches)]\n","            self.index += 1\n","            batches = self._to_tensor(batches)\n","            return batches\n","\n","        elif self.index >= self.n_batches:\n","            self.index = 0\n","            raise StopIteration\n","        else:\n","            batches = self.batches[self.index * self.batch_size: (self.index + 1) * self.batch_size]\n","            self.index += 1\n","            batches = self._to_tensor(batches)\n","            return batches\n","\n","    def __iter__(self):\n","        return self\n","\n","    def __len__(self):\n","        if self.residue:\n","            return self.n_batches + 1\n","        else:\n","            return self.n_batches\n","\n","\n","def build_iterator(dataset, config ,is_Tan=True):\n","    iter = DatasetIterater(dataset, config.batch_size, config.device, config.pad_size, is_Tan)\n","    return iter\n","\n","\n","def get_time_dif(start_time):\n","    \"\"\"获取已使用时间\"\"\"\n","    end_time = time.time()\n","    time_dif = end_time - start_time\n","    return timedelta(seconds=int(round(time_dif)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tE5bYBf4ztyg"},"outputs":[],"source":["# coding: UTF-8\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from sklearn import metrics\n","import time\n","\n","def init_network(model, method='xavier', exclude='embedding', seed=123):#initialize network\n","    for name, w in model.named_parameters():\n","        if exclude not in name:\n","            if 'weight' in name:\n","                if method == 'xavier':\n","                    nn.init.xavier_normal_(w)\n","                elif method == 'kaiming':\n","                    nn.init.kaiming_normal_(w)\n","                else:\n","                    nn.init.normal_(w)\n","            elif 'bias' in name:\n","                nn.init.constant_(w, 0)\n","            else:\n","                pass\n","\n","\n","def train(config, model, train_iter, dev_iter, test_iter):\n","    start_time = time.time()\n","    model.train()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate,weight_decay=1e-6)\n","    total_batch = 0  \n","    dev_best_loss = float('inf')\n","    last_improve = 0  \n","    flag = False  \n","    for epoch in range(config.num_epochs):\n","        print('Epoch [{}/{}]'.format(epoch + 1, config.num_epochs))\n","        for i, (trains, labels) in enumerate(train_iter):\n","            outputs = model(trains)\n","            model.zero_grad()\n","            loss = F.cross_entropy(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            if total_batch % 50 == 0:\n","                true = labels.data.cpu()\n","                predic = torch.max(outputs.data, 1)[1].cpu()\n","                train_acc = metrics.accuracy_score(true, predic)\n","                dev_acc, dev_loss = evaluate(config, model, dev_iter)\n","                if dev_loss < dev_best_loss:\n","                    dev_best_loss = dev_loss\n","                    torch.save(model.state_dict(), config.save_path)\n","                    improve = '*'\n","                    last_improve = total_batch\n","                else:\n","                    improve = ''\n","                time_dif = get_time_dif(start_time)\n","                msg = 'Iter: {0:>6},  Train Loss: {1:>5.2},  Train Acc: {2:>6.2%},  Val Loss: {3:>5.2},  Val Acc: {4:>6.2%},  Time: {5} {6}'\n","                print(msg.format(total_batch, loss.item(), train_acc, dev_loss, dev_acc, time_dif, improve))\n","                model.train()\n","            total_batch += 1\n","            if total_batch - last_improve > config.require_improvement:\n","                print(\"No optimization for a long time, auto-stopping...\")\n","                flag = True\n","                break\n","        if flag:\n","            break\n","    return None\n","    \n","def evaluate(config, model, data_iter, test=False):\n","    model.eval()\n","    loss_total = 0\n","    predict_all = np.array([], dtype=int)\n","    labels_all = np.array([], dtype=int)\n","    with torch.no_grad():\n","        for texts, labels in data_iter:\n","            outputs = model(texts)\n","            loss = F.cross_entropy(outputs, labels)\n","            loss_total += loss\n","            labels = labels.data.cpu().numpy()\n","            predic = torch.max(outputs.data, 1)[1].cpu().numpy()\n","            labels_all = np.append(labels_all, labels)\n","            predict_all = np.append(predict_all, predic)\n","    acc = metrics.accuracy_score(labels_all, predict_all)\n","    if test:\n","        f1 = metrics.f1_score(labels_all, predict_all,average='macro')\n","        return predict_all, f1\n","    return acc, loss_total / len(data_iter)\n","\n","def get_depth_f1(df,depth):\n","  if depth < 5:\n","    df_depth=df.loc[df['depth']==depth]\n","  else:\n","    df_depth=df.loc[df['depth']>=depth]\n","  f1 = metrics.f1_score(df_depth['label'].to_list(),df_depth['predicted'].to_list(),average='macro')\n","  return f1\n","\n","def test(config, model, test_iter, df):\n","# test\n","  model.load_state_dict(torch.load(config.save_path))\n","  model.eval()\n","  start_time = time.time()\n","  predict_all,  f1 = evaluate(config, model, test_iter, test=True)\n","  df['predicted'] = list(predict_all)\n","  return df, f1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rzrCHn9s1JqF"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","class CNN_Config(object):\n","    def __init__(self, dataset, embedding, df_train, df_test):\n","        self.model_name = 'Text_CNN'\n","        self.train_df = df_train  \n","        self.dev_df = df_test  \n","        self.test_df = df_test  \n","        self.class_list = [0, 1, 2]  \n","        self.num_classes = len(self.class_list)\n","        self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'        \n","        self.vocab_path = dataset + '/data/vocab.pkl' \n","        self.embedding_pretrained = torch.tensor(np.load(dataset + '/data/' + embedding)[\"embeddings\"].astype('float32')) # pre-trained word-embeddings\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        self.require_improvement = 500 \n","        self.n_vocab = 0    #number of vocabulary, will be initiated when running\n","        self.num_epochs = 100   \n","        self.batch_size = 16   \n","        self.pad_size = 200    \n","        self.learning_rate = 2e-4\n","        self.embed = self.embedding_pretrained.size(1)if self.embedding_pretrained is not None else 300  # word-embedding dimention\n","        self.filter_sizes = (2, 3, 4) \n","        self.num_filters = 32 \n","        self.dropout = 0.5  \n","\n","class Bi_LSTM_Att_Config(object):\n","    def __init__(self, dataset, embedding, df_train, df_test):\n","      self.model_name = 'TAN'\n","      self.train_df = df_train  \n","      self.dev_df = df_test  \n","      self.test_df = df_test  \n","      self.class_list = [0, 1, 2]  \n","      self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'   \n","      self.vocab_path = dataset + '/data/vocab.pkl' \n","      self.embedding_pretrained = torch.tensor(np.load(dataset + '/data/' + embedding)[\"embeddings\"].astype('float32')) \n","      self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","      self.require_improvement = 500 \n","      self.num_classes = len(self.class_list) \n","      self.n_vocab = 0  \n","      self.num_epochs = 20     \n","      self.batch_size = 16     \n","      self.pad_size = 64      \n","      self.learning_rate = 2e-4 \n","      self.embed = self.embedding_pretrained.size(1)if self.embedding_pretrained is not None else 300  \n","      self.hidden_size = 256 # lstm hidden size\n","      self.num_layers = 2  # lstm layer number\n","      self.hidden_size2 = 64\n","      self.dropout = 0.5  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XqiN_ces6xVi"},"outputs":[],"source":["class TextCNN(nn.Module):\n","    def __init__(self, config):\n","        super(TextCNN, self).__init__()\n","        if config.embedding_pretrained is not None:\n","            self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=False)\n","        else:\n","            self.embedding = nn.Embedding(config.n_vocab, config.embed, padding_idx=config.n_vocab - 1)\n","        self.convs = nn.ModuleList(\n","            [nn.Conv2d(1, config.num_filters, (k, config.embed)) for k in config.filter_sizes])\n","        self.dropout = nn.Dropout(config.dropout)\n","        self.fc = nn.Linear(config.num_filters * len(config.filter_sizes), config.num_classes)\n","\n","    def conv_and_pool(self, x, conv):\n","        x = F.relu(conv(x)).squeeze(3)\n","        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n","        return x\n","\n","    def forward(self, x):\n","        out = self.embedding(x[0])\n","        out = out.unsqueeze(1)\n","        out = torch.cat([self.conv_and_pool(out, conv) for conv in self.convs], 1)\n","        out = self.dropout(out)\n","        out = self.fc(out)\n","        return out\n","\n","class TAN(nn.Module):\n","    def __init__(self,config):\n","        super(TAN, self).__init__()\n","        self.embedding_dim = config.embed\n","        if config.embedding_pretrained is not None:\n","            self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=False)\n","        else:\n","            self.embedding = nn.Embedding(config.n_vocab, config.embed, padding_idx=config.n_vocab - 1)\n","        self.attention = nn.Linear(2*config.embed,1)\n","        self.lstm = nn.LSTM(config.embed, config.hidden_size, bidirectional=True)\n","        self.dropout = nn.Dropout(config.dropout)\n","        # self.fc = nn.Linear(2*config.hidden_size, config.num_classes)\n","        self.fc1 = nn.Linear(config.hidden_size * 2, config.hidden_size2)\n","        self.fc = nn.Linear(config.hidden_size2, config.num_classes)\n","        self.w = nn.Parameter(torch.zeros(config.embed * 2))\n","\n","    def forward(self,x):\n","        sentence,target, _ = x\n","        x_emb = self.embedding(sentence)# [batch_size, seq_len, embeding]\n","        t_emb = self.embedding(target)# [batch_size, seq_len, embeding]\n","        z = torch.sum(t_emb, dim=1) \n","        z = torch.div(z, x_emb.size()[1]) #[batch_size,  embeding]\n","        z = z.unsqueeze(1)    # (batch_size, 1, emb_dim)\n","        z = torch.tile(z, [1, x_emb.size()[1], 1]) # (batch_size, seq_len, embeding)      \n","        xt_emb = torch.cat((x_emb,z),dim=2)# (batch_size, seq_len, 2*embeding) \n","        # attention_layer\n","        a = F.softmax(torch.matmul(xt_emb, self.w), dim=1)\n","        a = a.unsqueeze(-1)  \n","        h, _ = self.lstm(x_emb)# (batch_size, seq_len, 2*hidden) \n","        out = h * a # (batch_size, seq_len, 2*hidden) \n","        # = torch.mean(out,dim=1)# (batch_size, 2*hidden)\n","        # out = self.fc(final_hidden_state)\n","        out = torch.sum(out, 1)  # [128, 256]\n","        out = F.relu(out)\n","        out = self.fc1(out)\n","        out = self.fc(out)  # [128, 64]\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JMOWAWYp0buU"},"outputs":[],"source":["# coding: UTF-8\n","import time\n","import torch\n","import numpy as np\n","\n","embedding = 'embedding_CantoStance.npz'\n","\n","f=list()\n","f1=list()\n","f2=list()\n","f3=list()\n","f4=list()\n","f5=list()\n","\n","for i in range(10):\n","    dataset='/content/drive/MyDrive/Prediction'\n","    df = pd.concat([df2,df_info])\n","    df = df.sample(frac=1, random_state=i)   \n","    cut_idx1 = int(round(0.8 * df.shape[0]))\n","    df_train,df_test = df[:cut_idx1], df[cut_idx1:]\n","    config = Bi_LSTM_Att_Config(dataset, embedding,df_train,df_test)\n","    np.random.seed(1)\n","    torch.manual_seed(1)\n","    torch.cuda.manual_seed_all(1)\n","    torch.backends.cudnn.deterministic = True  \n","    start_time = time.time()\n","    print(\"Loading data...\")\n","    vocab, train_data, test_data = build_dataset(config)\n","    train_iter = build_iterator(train_data, config,)\n","    dev_iter = build_iterator(test_data, config,)\n","    test_iter = build_iterator(test_data, config)\n","    time_dif = get_time_dif(start_time)\n","    print(\"Time usage:\", time_dif)\n","    config.n_vocab = len(vocab)\n","    model = TAN(config).to(config.device)\n","    init_network(model)\n","    train(config, model, train_iter, dev_iter, test_iter)\n","    df_test,f_score = test(config, model, test_iter, df_test)\n","    filename=\"/content/drive/MyDrive/DATA/test_\"+config.model_name+str(i)+'.csv'\n","    df_test.to_csv(filename)\n","    f.append(f_score)\n","    f1.append(get_depth_f1(df_test,1))\n","    f2.append(get_depth_f1(df_test,2))\n","    f3.append(get_depth_f1(df_test,3))\n","    f4.append(get_depth_f1(df_test,4))\n","    f5.append(get_depth_f1(df_test,5))\n","\n","print(\"f: \"+str(np.mean(f))+\"+\"+str(np.std(f, ddof = 1)))\n","print(\"f1: \"+str(np.mean(f1))+\"+\"+str(np.std(f1, ddof = 1)))\n","print(\"f2: \"+str(np.mean(f2))+\"+\"+str(np.std(f2, ddof = 1)))\n","print(\"f3: \"+str(np.mean(f3))+\"+\"+str(np.std(f3, ddof = 1)))\n","print(\"f4: \"+str(np.mean(f4))+\"+\"+str(np.std(f4, ddof = 1)))\n","print(\"f5: \"+str(np.mean(f5))+\"+\"+str(np.std(f5, ddof = 1)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rjYn67LyiUgy"},"outputs":[],"source":["import time\n","import torch\n","import numpy as np\n","\n","dataset='/content/drive/MyDrive/Prediction' # 数据集\n","embedding = 'embedding_CantoStance.npz'\n","\n","\n","f=list()\n","f1=list()\n","f2=list()\n","f3=list()\n","f4=list()\n","f5=list()\n","\n","for i in range(10):\n","    dataset='/content/drive/MyDrive/Prediction'\n","    df = pd.concat([df2,df_info])\n","    df = df.sample(frac=1, random_state=i)   \n","    cut_idx1 = int(round(0.8 * df.shape[0]))\n","    df_train,df_test = df[:cut_idx1], df[cut_idx1:]\n","    config = CNN_Config(dataset, embedding,df_train,df_test)\n","    np.random.seed(1)\n","    torch.manual_seed(1)\n","    torch.cuda.manual_seed_all(1)\n","    torch.backends.cudnn.deterministic = True  # 保证每次结果一样\n","    start_time = time.time()\n","    print(\"Loading data...\")\n","    vocab, train_data, test_data = build_dataset(config)\n","    train_iter = build_iterator(train_data, config,)\n","    dev_iter = build_iterator(test_data, config,)\n","    test_iter = build_iterator(test_data, config)\n","    time_dif = get_time_dif(start_time)\n","    print(\"Time usage:\", time_dif)\n","    config.n_vocab = len(vocab)\n","    model = TextCNN(config).to(config.device)\n","    init_network(model)\n","    train(config, model, train_iter, dev_iter, test_iter)\n","    df_test,f_score = test(config, model, test_iter, df_test)\n","    filename=\"/content/drive/MyDrive/DATA/test_\"+config.model_name+str(i)+'.csv'\n","    df_test.to_csv(filename)\n","    f.append(f_score)\n","    f1.append(get_depth_f1(df_test,1))\n","    f2.append(get_depth_f1(df_test,2))\n","    f3.append(get_depth_f1(df_test,3))\n","    f4.append(get_depth_f1(df_test,4))\n","    f5.append(get_depth_f1(df_test,5))\n","\n","print(\"f: \"+str(np.mean(f))+\"+\"+str(np.std(f, ddof = 1)))\n","print(\"f1: \"+str(np.mean(f1))+\"+\"+str(np.std(f1, ddof = 1)))\n","print(\"f2: \"+str(np.mean(f2))+\"+\"+str(np.std(f2, ddof = 1)))\n","print(\"f3: \"+str(np.mean(f3))+\"+\"+str(np.std(f3, ddof = 1)))\n","print(\"f4: \"+str(np.mean(f4))+\"+\"+str(np.std(f4, ddof = 1)))\n","print(\"f5: \"+str(np.mean(f5))+\"+\"+str(np.std(f5, ddof = 1)))"]},{"cell_type":"markdown","metadata":{"id":"ju43j9EvfnRb"},"source":["# SVM-ngram Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3vyMRlVKftb1"},"outputs":[],"source":["import pycantonese\n","\n","stop_words = pycantonese.stop_words()\n","def create_sentences(df):#create input of SVM\n","  sentences=[]\n","  for i in range(len(df)):\n","      text_raw=df['text'].iloc[i]\n","      segs = pycantonese.segment(text_raw)\n","      segs = filter(lambda x:x not in stop_words, segs)\n","      sentences.append((\" \".join(segs),df['label'].iloc[i]))\n","  return sentences\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":68930,"status":"ok","timestamp":1649670413171,"user":{"displayName":"刘耘果","userId":"11745284739417236882"},"user_tz":-480},"id":"JpTpioBpyvcN","outputId":"cb2f6631-bdbe-4674-b69b-ea549a0592af"},"outputs":[{"output_type":"stream","name":"stdout","text":["f: 0.5814944220161355+0.014710859049724941\n","f1: 0.6006518201108779+0.06745646440246371\n","f2: 0.5746718842473272+0.01750741325619133\n","f3: 0.49993026665811346+0.05111860555236531\n","f4: 0.5155522237209406+0.10012384602799053\n","f5: 0.472909064720317+0.1130151559665336\n","[0.5856178438014196, 0.5765555839364268, 0.578176449075225, 0.5775772212228832, 0.5757272820843918, 0.5909993888770427, 0.5475099163915668, 0.6013348791727348, 0.586397632823067, 0.5950480227765976]\n","[0.6214896214896215, 0.6170989517110411, 0.5040364216652586, 0.6761408692893931, 0.5576814136651865, 0.7334512953132797, 0.5341880341880342, 0.6091726889491117, 0.5699521531100479, 0.5833067517278043]\n","[0.5684665122351539, 0.5612756954603473, 0.5823193765335689, 0.5608017976884305, 0.5761988676366431, 0.5695096581000945, 0.5454402290221138, 0.5925250724434244, 0.5838283600417511, 0.6063532733117446]\n","[0.570084262880873, 0.4579737732656515, 0.5534064522503829, 0.48089777516416743, 0.4695340501792115, 0.4748032495557248, 0.47202202001576676, 0.5540552455568012, 0.42083353347102803, 0.545692304241527]\n","[0.5974842767295597, 0.5283703224879696, 0.4332399626517273, 0.6461538461538461, 0.45945945945945943, 0.6164454852979443, 0.4435483870967742, 0.47483204134366924, 0.6131313131313131, 0.3428571428571428]\n","[0.3605283605283605, 0.6140350877192983, 0.5087542087542088, 0.3518518518518518, 0.5301587301587302, 0.5984848484848485, 0.3702359346642468, 0.5066045066045065, 0.5747863247863249, 0.3136507936507937]\n"]}],"source":["import random\n","from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC,LinearSVC\n","from sklearn import svm\n","from sklearn.metrics import *\n","import tensorflow as tf\n","import scipy.sparse as sp\n","from scipy.sparse import hstack\n","\n","class TextClassifier():\n","    def __init__(self, classifier=SVC(kernel='linear')):\n","        self.classifier = LinearSVC(C=0.2, class_weight=None, dual=True, fit_intercept=True,\n","                       intercept_scaling=1, loss='squared_hinge', max_iter=300,\n","                       multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n","                       verbose=0)\n","        self.vectorizer1 = CountVectorizer(analyzer='word', ngram_range=(1,3), stop_words=stop_words)\n","        self.vectorizer2 = CountVectorizer(analyzer='char', ngram_range=(2,5), stop_words=stop_words)\n","    def features(self, X):\n","        word=self.vectorizer1.transform(X)\n","        character=self.vectorizer2.transform(X)\n","        feature = hstack([word,character])\n","        return feature\n","    def fit(self, X, y):\n","        self.vectorizer1.fit(X) #word-level ngram\n","        self.vectorizer2.fit(X) #charactor ngram\n","        self.classifier.fit(self.features(X), y) \n","    def predict(self, x):\n","        return self.classifier.predict(self.features(x))\n","\n","\n","f=list()\n","f1=list()\n","f2=list()\n","f3=list()\n","f4=list()\n","f5=list()\n","\n","\n","def get_depth_f1(df,depth):\n","    if depth < 5:\n","      df_depth=df.loc[df['depth']==depth]\n","    else:\n","      df_depth=df.loc[df['depth']>=depth]\n","    f1 = f1_score(df_depth['label'].to_list(),df_depth['predicted'].to_list(),average='macro')\n","    return f1\n","for i in range(10):\n","    dataset='/content/drive/MyDrive/Prediction'\n","    df = pd.concat([df2,df_info])\n","    df = df.sample(frac=1, random_state=i)   \n","    cut_idx1 = int(round(0.8 * df.shape[0]))\n","    df_train,df_test = df[:cut_idx1], df[cut_idx1:]\n","    sentences_train=create_sentences(df_train)\n","    sentences_test=create_sentences(df_test)\n","    x_train, y_train = zip(*sentences_train)\n","    x_test, y_test = zip(*sentences_test)\n","    text_classifier=TextClassifier()\n","    text_classifier.fit(x_train, y_train)\n","    y_pred = text_classifier.predict(x_test)\n","    df_test['predicted']=y_pred\n","    filename=\"/content/drive/MyDrive/DATA/SVM\"+str(i)+'.csv'\n","    df_test.to_csv(filename)\n","    f.append(f1_score(y_test, y_pred,average='macro'))\n","    f1.append(get_depth_f1(df_test,1))\n","    f2.append(get_depth_f1(df_test,2))\n","    f3.append(get_depth_f1(df_test,3))\n","    f4.append(get_depth_f1(df_test,4))\n","    f5.append(get_depth_f1(df_test,5))\n","\n","print(\"f: \"+str(np.mean(f))+\"+\"+str(np.std(f, ddof = 1)))\n","print(\"f1: \"+str(np.mean(f1))+\"+\"+str(np.std(f1, ddof = 1)))\n","print(\"f2: \"+str(np.mean(f2))+\"+\"+str(np.std(f2, ddof = 1)))\n","print(\"f3: \"+str(np.mean(f3))+\"+\"+str(np.std(f3, ddof = 1)))\n","print(\"f4: \"+str(np.mean(f4))+\"+\"+str(np.std(f4, ddof = 1)))\n","print(\"f5: \"+str(np.mean(f5))+\"+\"+str(np.std(f5, ddof = 1)))"]},{"cell_type":"markdown","metadata":{"id":"fxWxrOJWah0Q"},"source":["# Kill Process"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2467,"status":"ok","timestamp":1649609204304,"user":{"displayName":"刘耘果","userId":"11745284739417236882"},"user_tz":-480},"id":"fqi23lxMTYnY","outputId":"f2a6731f-939a-4f8e-d496-b3dd0141012d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","psmisc is already the newest version (23.1-1ubuntu0.1).\n","0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n","/dev/nvidia0:        19745m\n","/dev/nvidiactl:      19745m\n","/dev/nvidia-uvm:     19745m\n"]}],"source":["!apt install psmisc\n","!sudo fuser /dev/nvidia*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ibiqMTu-TdoU"},"outputs":[],"source":["!kill -9 19745"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":915,"status":"ok","timestamp":1648721648285,"user":{"displayName":"刘耘果","userId":"11745284739417236882"},"user_tz":-480},"id":"sTipKlJPO8pJ","outputId":"a03d4a1b-4803-4c85-e87d-547acbb864e7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Thu Mar 31 10:14:08 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   39C    P0    34W / 250W |   1371MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":377,"status":"ok","timestamp":1649609218424,"user":{"displayName":"刘耘果","userId":"11745284739417236882"},"user_tz":-480},"id":"044G1nxVQw0V","outputId":"c3a5e48d-bafc-445c-b6e8-2cf1ceee4dcc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Tesla P100-PCIE-16GB'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}],"source":["import torch\n","torch.cuda.get_device_name(0)"]}],"metadata":{"colab":{"collapsed_sections":["Zu8bU7EbaEf4","20xHeJLjaS3N","Rjuffl54JfA7","a_2XFAhokmEP","ju43j9EvfnRb","fxWxrOJWah0Q"],"machine_shape":"hm","name":"Conversational_Stance_Detection.ipynb","provenance":[{"file_id":"1ctG7GwT-vlbjiDp6isHaXTjFdpAX1I5E","timestamp":1634993162676}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}